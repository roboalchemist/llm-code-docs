# Aimlapi Documentation

Source: https://docs.aimlapi.com/llms-full.txt

---

# Documentation Map

Learn how to get started with the AI/ML API

This documentation portal is designed to help you choose and configure the AI **model** that best suits your needs‚Äîor one of our **solutions** (ready-to-use tools for specific practical tasks) from our available options and correctly integrate it into your code.

Have suggestions for improvement? [**Let us know!**](https://forms.aimlapi.com/doc)

***

**Trending Models**

<table data-column-title-hidden data-view="cards"><thead><tr><th align="center"></th><th data-hidden data-card-cover data-type="image">Cover image</th><th data-hidden data-card-target data-type="content-ref"></th></tr></thead><tbody><tr><td align="center">Top Video Generator</td><td><a href="https://3927338786-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FROMd1X5PuqtikJ48n2N9%2Fuploads%2Fgit-blob-daefe57d7e1fb595691c1b5d21945be789e0963a%2Fphoto_2025-11-10_18-53-24.jpg?alt=media">photo_2025-11-10_18-53-24.jpg</a></td><td><a href="../api-references/video-models/openai/sora-2-t2v">sora-2-t2v</a></td></tr><tr><td align="center">Best Image Model</td><td><a href="https://3927338786-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FROMd1X5PuqtikJ48n2N9%2Fuploads%2Fgit-blob-51e574c4f1f787b168f22aa826444fdd8eae69c3%2Fphoto_2025-11-10_18-44-22.jpg?alt=media">photo_2025-11-10_18-44-22.jpg</a></td><td><a href="../api-references/image-models/google/gemini-2.5-flash-image">gemini-2.5-flash-image</a></td></tr><tr><td align="center">Advanced Chat AI</td><td><a href="https://3927338786-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FROMd1X5PuqtikJ48n2N9%2Fuploads%2Fgit-blob-3d142fff5b8d74dc860ad01f9fa9bf34db54ada0%2Fphoto_2025-11-10_18-53-28.jpg?alt=media">photo_2025-11-10_18-53-28.jpg</a></td><td><a href="../api-references/text-models-llm/openai/gpt-5">gpt-5</a></td></tr></tbody></table>

***

<table data-header-hidden data-full-width="false"><thead><tr><th width="281.09991455078125" valign="top"></th><th valign="top"></th></tr></thead><tbody><tr><td valign="top"><p><strong>Start with this code block</strong><br><br><span data-gb-custom-inline data-tag="emoji" data-code="1fa81">ü™Å</span> Step-by-step example:</p><p><a href="setting-up">Setting Up</a><br><br><span data-gb-custom-inline data-tag="emoji" data-code="1fa81">ü™Å</span> Choose the SDK to use:</p><p><a href="supported-sdks">Supported SDKs</a></p></td><td valign="top"><pre class="language-python" data-overflow="wrap"><code class="lang-python">from openai import OpenAI
client = OpenAI(
base_url="https://api.aimlapi.com/v1",
api_key="&#x3C;YOUR_AIMLAPI_KEY>",
)
response = client.chat.completions.create(
model="gpt-4o",
messages=[{"role": "user", "content": "Write a one-sentence story about numbers."}]
)
print(response.choices[0].message.content)
</code></pre></td></tr></tbody></table>

***

## Browse Models

Popular | [View all 200+ models >](https://docs.aimlapi.com/api-references/model-database)

<table data-view="cards"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type="content-ref"></th></tr></thead><tbody><tr><td><a href="../api-references/text-models-llm/openai">ChatGPT</a></td><td></td><td></td><td><a href="../api-references/text-models-llm/openai">openai</a></td></tr><tr><td><a href="../api-references/text-models-llm/deepseek">DeepSeek</a></td><td></td><td></td><td><a href="../api-references/text-models-llm/deepseek">deepseek</a></td></tr><tr><td><a href="../api-references/image-models/flux">Flux</a></td><td></td><td></td><td><a href="../api-references/image-models/flux">flux</a></td></tr></tbody></table>

Select the model by its **Task**, by its **Developer** or by the supported **Capabilities**:

{% hint style="info" %}
If you've already made your choice and know the model ID, use the [Search panel](https://docs.aimlapi.com/?q=) on your right.
{% endhint %}

{% tabs %}
{% tab title="Models by TASK" %}
{% content-ref url="../api-references/text-models-llm" %}
[text-models-llm](https://docs.aimlapi.com/api-references/text-models-llm)
{% endcontent-ref %}

{% content-ref url="../api-references/image-models" %}
[image-models](https://docs.aimlapi.com/api-references/image-models)
{% endcontent-ref %}

{% content-ref url="../api-references/video-models" %}
[video-models](https://docs.aimlapi.com/api-references/video-models)
{% endcontent-ref %}

{% content-ref url="../api-references/music-models" %}
[music-models](https://docs.aimlapi.com/api-references/music-models)
{% endcontent-ref %}

{% content-ref url="../api-references/speech-models" %}
[speech-models](https://docs.aimlapi.com/api-references/speech-models)
{% endcontent-ref %}

{% content-ref url="../api-references/moderation-safety-models" %}
[moderation-safety-models](https://docs.aimlapi.com/api-references/moderation-safety-models)
{% endcontent-ref %}

{% content-ref url="../api-references/3d-generating-models" %}
[3d-generating-models](https://docs.aimlapi.com/api-references/3d-generating-models)
{% endcontent-ref %}

{% content-ref url="../api-references/vision-models" %}
[vision-models](https://docs.aimlapi.com/api-references/vision-models)
{% endcontent-ref %}

{% content-ref url="../api-references/embedding-models" %}
[embedding-models](https://docs.aimlapi.com/api-references/embedding-models)
{% endcontent-ref %}
{% endtab %}

{% tab title="Models by DEVELOPER" %}
**Alibaba Cloud**: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/alibaba-cloud) [Image](https://docs.aimlapi.com/api-references/video-models/alibaba-cloud) [Video](https://docs.aimlapi.com/api-references/image-models/alibaba-cloud) [Text-to-Speech](https://docs.aimlapi.com/api-references/speech-models/text-to-speech/alibaba-cloud)

**Anthracite**: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/anthracite)

<mark style="background-color:green;">**Anthropic**</mark>: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/anthropic) [Embedding](https://docs.aimlapi.com/api-references/embedding-models/anthropic)

**Assembly AI:** [Speech-To-Text](https://docs.aimlapi.com/api-references/speech-models/speech-to-text/assembly-ai)

**BAAI**: [Embedding](https://docs.aimlapi.com/api-references/embedding-models/baai)

**ByteDance**: [Image](https://docs.aimlapi.com/api-references/video-models/bytedance) [Video](https://docs.aimlapi.com/api-references/image-models/bytedance)

**Cohere**: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/cohere)

<mark style="background-color:green;">**DeepSeek**</mark>: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/deepseek)

**Deepgram**: [Speech-To-Text](https://docs.aimlapi.com/api-references/speech-models/speech-to-text/deepgram) [Text-to-Speech](https://docs.aimlapi.com/api-references/speech-models/text-to-speech/deepgram)

<mark style="background-color:green;">**ElevenLabs**</mark>**:** [Text-to-Speech](https://docs.aimlapi.com/api-references/speech-models/text-to-speech/elevenlabs) [Voice Chat](https://docs.aimlapi.com/api-references/speech-models/voice-chat/elevenlabs) [Music](https://docs.aimlapi.com/api-references/music-models/elevenlabs)

<mark style="background-color:green;">**Flux**</mark>: [Image](https://docs.aimlapi.com/api-references/image-models/flux)

**Google**: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/google) [Image](https://docs.aimlapi.com/api-references/image-models/google) [Video](https://docs.aimlapi.com/api-references/video-models/google) [Music](https://docs.aimlapi.com/api-references/vision-models/ocr-optical-character-recognition/google) [Vision(OCR)](https://docs.aimlapi.com/api-references/music-models/google) [Embedding](https://docs.aimlapi.com/api-references/embedding-models/google)

<mark style="background-color:green;">**Kling AI**</mark>: [Video](https://docs.aimlapi.com/api-references/video-models/kling-ai)

**Meta**: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/meta)

**Microsoft**: [Text-to-Speech](https://docs.aimlapi.com/api-references/speech-models/text-to-speech/microsoft)

<mark style="background-color:green;">**MiniMax**</mark>: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/minimax) [Video](https://docs.aimlapi.com/api-references/video-models/minimax) [Music](https://docs.aimlapi.com/api-references/music-models/minimax) [Voice-Chat](https://docs.aimlapi.com/api-references/speech-models/voice-chat)

**Mistral AI**: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/mistral-ai) [Vision(OCR)](https://docs.aimlapi.com/api-references/vision-models/ocr-optical-character-recognition/mistral-ai)

**Moonshot**: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/moonshot)

**NousResearch**: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/nousresearch)

**NVIDIA**: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/nvidia)

<mark style="background-color:green;">**OpenAI**</mark>: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/openai) [Image](https://docs.aimlapi.com/api-references/image-models/openai) [Speech-To-Text](https://docs.aimlapi.com/api-references/speech-models/speech-to-text/openai) [Embedding](https://docs.aimlapi.com/api-references/embedding-models/openai)

**Perplexity**: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/perplexity)

**PixVerse:** [Video](https://docs.aimlapi.com/api-references/video-models/pixverse)

**RecraftAI**: [Image](https://docs.aimlapi.com/api-references/image-models/recraftai)

**Reve**: [Image](https://docs.aimlapi.com/api-references/image-models/reve)

**Runway**: [Video](https://docs.aimlapi.com/api-references/video-models/runway)

<mark style="background-color:green;">**Stability AI**</mark>: [Image](https://docs.aimlapi.com/api-references/image-models/stability-ai) [Music](https://docs.aimlapi.com/api-references/music-models/stability-ai) [3D-Generation](https://docs.aimlapi.com/api-references/3d-generating-models/stability-ai)

**Sber AI**: [Video](https://docs.aimlapi.com/api-references/video-models/sber-ai)

**Tencent**: [Image](https://docs.aimlapi.com/api-references/image-models/tencent)

**Together AI**: [Embedding](https://docs.aimlapi.com/api-references/embedding-models/together-ai)

**xAI**: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/xai) [Image](https://docs.aimlapi.com/api-references/image-models/xai)

**Zhipu**: [Text/Chat](https://docs.aimlapi.com/api-references/text-models-llm/zhipu)
{% endtab %}

{% tab title="Text Models by CAPABILITY" %}
{% content-ref url="../capabilities/completion-or-chat-models" %}
[completion-or-chat-models](https://docs.aimlapi.com/capabilities/completion-or-chat-models)
{% endcontent-ref %}

{% content-ref url="../capabilities/streaming-mode" %}
[streaming-mode](https://docs.aimlapi.com/capabilities/streaming-mode)
{% endcontent-ref %}

{% content-ref url="../capabilities/code-generation" %}
[code-generation](https://docs.aimlapi.com/capabilities/code-generation)
{% endcontent-ref %}

{% content-ref url="../capabilities/thinking-reasoning" %}
[thinking-reasoning](https://docs.aimlapi.com/capabilities/thinking-reasoning)
{% endcontent-ref %}

{% content-ref url="../capabilities/function-calling" %}
[function-calling](https://docs.aimlapi.com/capabilities/function-calling)
{% endcontent-ref %}

{% content-ref url="../capabilities/image-to-text-vision" %}
[image-to-text-vision](https://docs.aimlapi.com/capabilities/image-to-text-vision)
{% endcontent-ref %}

{% content-ref url="../capabilities/web-search" %}
[web-search](https://docs.aimlapi.com/capabilities/web-search)
{% endcontent-ref %}
{% endtab %}
{% endtabs %}

## Browse Solutions

* [AI Search Engine](https://docs.aimlapi.com/solutions/bagoodex/ai-search-engine) ‚Äì if you need to create a project where information must be found on the internet and then presented to you in a structured format, use this solution.
* [OpenAI Assistants](https://docs.aimlapi.com/solutions/openai/assistants) ‚Äì if you need to create tailored AI Assistants capable of handling customer support, data analysis, content generation, and more.

***

## Going Deeper

<table data-header-hidden data-full-width="false"><thead><tr><th width="409.4000244140625"></th><th valign="top"></th></tr></thead><tbody><tr><td><p><strong>Use more text model capabilities in your project:</strong><br><br><span data-gb-custom-inline data-tag="emoji" data-code="1f4d6">üìñ</span> <a href="../capabilities/completion-or-chat-models">‚ÄãCompletion and Chat Completion</a></p><p><span data-gb-custom-inline data-tag="emoji" data-code="1f4d6">üìñ</span> <a href="../capabilities/function-calling">Function Calling</a></p><p><span data-gb-custom-inline data-tag="emoji" data-code="1f4d6">üìñ</span> <a href="../capabilities/streaming-mode">Streaming Mode</a></p><p><span data-gb-custom-inline data-tag="emoji" data-code="1f4d6">üìñ</span> <a href="../capabilities/image-to-text-vision">Vision in Text Models (Image-to-Text)</a></p><p><span data-gb-custom-inline data-tag="emoji" data-code="1f4d6">üìñ</span> <a href="../capabilities/code-generation">Code Generation</a></p><p><span data-gb-custom-inline data-tag="emoji" data-code="1f4d6">üìñ</span> <a href="../capabilities/thinking-reasoning">Thinking / Reasoning</a></p><p><span data-gb-custom-inline data-tag="emoji" data-code="1f4d6">üìñ</span> <a href="../capabilities/web-search">Web Search</a><br><br></p></td><td valign="top"><p><strong>Miscellaneous</strong>:<br><br><span data-gb-custom-inline data-tag="emoji" data-code="1f517">üîó</span> <a href="https://github.com/aimlapi/api-docs/blob/main/docs/broken-reference/README.md">Integrations</a></p><p><span data-gb-custom-inline data-tag="emoji" data-code="1f4d7">üìó</span> <a href="https://github.com/aimlapi/api-docs/blob/main/docs/broken-reference/README.md">Glossary</a></p><p><span data-gb-custom-inline data-tag="emoji" data-code="26a0">‚ö†Ô∏è</span> <a href="https://github.com/aimlapi/api-docs/blob/main/docs/broken-reference/README.md">Errors and Messages</a></p><p><span data-gb-custom-inline data-tag="emoji" data-code="2753">‚ùì</span> <a href="https://github.com/aimlapi/api-docs/blob/main/docs/broken-reference/README.md">FAQ</a> ‚Äã</p><p><br></p></td></tr><tr><td><strong>Learn more about developer-specific features:</strong><br><br><span data-gb-custom-inline data-tag="emoji" data-code="1f4d6">üìñ</span> <a href="../capabilities/anthropic">Features of Anthropic Models</a><br></td><td valign="top"></td></tr></tbody></table>

## Have a Minute? Help Make the Docs Better!

We‚Äôre currently working on improving our documentation portal, and your feedback would be **incredibly** helpful! Take [**a quick 5-question survey**](https://tally.so/r/w4G9Er) (no personal info required!)

You can also rate each individual page using the built-in form on the right side of the screen:

<figure><img src="https://3927338786-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FROMd1X5PuqtikJ48n2N9%2Fuploads%2Fgit-blob-62017f43d426ea34ff2a6cb09df4076bd12628ee%2Frateform-5.webp?alt=media" alt=""><figcaption></figcaption></figure>


# Setting Up

A step-by-step guide to setting up and making a test call to the AI model, including generating an API key, configuring the Base URL, and running the first request.

Here, you'll learn how to start using our API in your code. The following steps must be completed regardless of whether you integrate one of the [models](https://docs.aimlapi.com/api-references/model-database) we offer or use our ready-made [solution](https://github.com/aimlapi/api-docs/blob/main/docs/quickstart/broken-reference/README.md):

* [generating an AIML API Key](#generating-an-aiml-api-key),
* [configuring the base URL](#configuring-base-url),
* [making an API call](#making-an-api-call).

Let's walk through an example of connecting to the [**gpt-4o**](https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4o) model via OpenAI SDK. This guide is suitable even for complete beginners.

## G**enerating an AIML API Key**

<details>

<summary><mark style="color:blue;">What is an API Key?</mark></summary>

You can find your AIML API key on the [account page](https://aimlapi.com/app/keys).

An AIML API Key is a credential that grants you access to our API from within your code. It is a sensitive string of characters that should be kept confidential. Do not share this API key with anyone else, as it could be misused without your knowledge.

‚ö†Ô∏è <mark style="color:orange;">Note that API keys from third-party organizations cannot be used with our API: you need an AIML API Key.</mark>

</details>

To use the AIML API, you need to create an account and generate an API key. Follow these steps:

1. [**Create an Account**](https://aimlapi.com/app/sign-up)**:** Visit the AI/ML API website and create an account.
2. [**Generate an API Key**](https://aimlapi.com/app/keys)**:** After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

<figure><img src="https://3927338786-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FROMd1X5PuqtikJ48n2N9%2Fuploads%2Fgit-blob-959a01897d2fc6857e5848a8810d22949ff35ce0%2Fimage%20(4).png?alt=media" alt=""><figcaption><p>Your API key</p></figcaption></figure>

## **Configuring Base URL**

<details>

<summary><mark style="color:blue;">What is a Base URL?</mark></summary>

The **Base URL** is the first part of the URL (including the protocol, domain, and pathname) that determines the server responsible for handling your request. It‚Äôs crucial to configure the correct Base URL in your application, especially if you are using SDKs from OpenAI, Azure, or other providers. By default, these SDKs are set to point to their servers, which are not compatible with our API keys and do not support many of the models we offer.

</details>

Depending on your environment and application, you will set the base URL differently. Below is a universal string that you can use to access our API. Copy it or return here later when you are ready with your environment or app.

```
https://api.aimlapi.com
```

The AI/ML API supports both versioned and non-versioned URLs, providing flexibility in your API requests. You can use either of the following formats:

* <kbd><https://api.aimlapi.com></kbd>
* <kbd><https://api.aimlapi.com/v1></kbd>

{% hint style="success" %}
Using versioned URLs can help ensure compatibility with future updates and changes to the API. It is recommended to use versioned URLs for long-term projects to maintain stability.
{% endhint %}

## Making an API Call

Based on your environment, you will call our API differently. Below are two common ways to call our API using two popular programming languages: **Python** and **NodeJS**.

{% hint style="info" %}
In the examples below, we use the [**OpenAI SDK**](https://docs.aimlapi.com/supported-sdks#openai). This is possible due to our compatibility with most OpenAI APIs, but this is just one approach. You can use our API without this SDK with raw HTTP queries.
{% endhint %}

If you don‚Äôt want lengthy explanations, here‚Äôs the code you can use right away in a Python or Node.js program. You only need to replace `<YOUR_AIMLAPI_KEY>` with your AIML API Key obtained from your account.\
However, below, we will still go through these examples step by step in both languages explaining every single line.

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
from openai import OpenAI

base_url = "https://api.aimlapi.com/v1"

# Insert your AIML API key in the quotation marks instead of <YOUR_AIMLAPI_KEY>:
api_key = "<YOUR_AIMLAPI_KEY>" 

system_prompt = "You are a travel agent. Be descriptive and helpful."
user_prompt = "Tell me about San Francisco"

api = OpenAI(api_key=api_key, base_url=base_url)


def main():
    completion = api.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.7,
        max_tokens=256,
    )

    response = completion.choices[0].message.content

    print("User:", user_prompt)
    print("AI:", response)


if __name__ == "__main__":
    main()
```

{% endcode %}
{% endtab %}

{% tab title="NodeJS" %}

<pre class="language-javascript" data-overflow="wrap"><code class="lang-javascript"><strong>const { OpenAI } = require("openai");
</strong>
const baseURL = "https://api.aimlapi.com/v1";

// Insert your AIML API Key in the quotation marks instead of my_key:
const apiKey = "&#x3C;YOUR_AIMLAPI_KEY>"; 

const systemPrompt = "You are a travel agent. Be descriptive and helpful";
const userPrompt = "Tell me about San Francisco";

const api = new OpenAI({
  apiKey,
  baseURL,
});

const main = async () => {
  const completion = await api.chat.completions.create({
    model: "mistralai/Mistral-7B-Instruct-v0.2",
    messages: [
      {
        role: "system",
        content: systemPrompt,
      },
      {
        role: "user",
        content: userPrompt,
      },
    ],
    temperature: 0.7,
    max_tokens: 256,
  });

  const response = completion.choices[0].message.content;

  console.log("User:", userPrompt);
  console.log("AI:", response);
};

main();
</code></pre>

{% endtab %}
{% endtabs %}

<details>

<summary>Step-by-step example in Python</summary>

Let's start from very beginning. We assume you already installed Python (with venv), if not, here a [guide for the beginners](https://docs.aimlapi.com/faq/can-i-use-api-in-python).

Create a new folder for test project, name it as `aimlapi-welcome` and change to it.

```bash
mkdir ./aimlapi-welcome
cd ./aimlapi-welcome
```

(Optional) If you use IDE then we recommend to open created folder as workspace. On example, in VSCode you can do it with:

```
code .
```

Run a terminal inside created folder and create virtual envorinment with a command

```shell
python3 -m venv ./.venv
```

Activate created virtual environment

```shell
# Linux / Mac
source ./.venv/bin/activate
# Windows
./.venv/bin/Activate.bat
```

Install requirement dependencies. In our case we need only OpenAI SDK

```shell
pip install openai
```

Create new file and name it as `travel.py`

```shell
touch travel.py
```

Paste following content inside this `travel.py` and replace `<YOUR_AIMLAPI_KEY>` with your API key you got on [first step](#generating-an-api-key).

```python
from openai import OpenAI

base_url = "https://api.aimlapi.com/v1"
api_key = "<YOUR_AIMLAPI_KEY>"
system_prompt = "You are a travel agent. Be descriptive and helpful."
user_prompt = "Tell me about San Francisco"

api = OpenAI(api_key=api_key, base_url=base_url)


def main():
    completion = api.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.7,
        max_tokens=256,
    )

    response = completion.choices[0].message.content

    print("User:", user_prompt)
    print("AI:", response)


if __name__ == "__main__":
    main()
```

Run the application

```shell
python3 ./travel.py
```

If you done all correct, you will see following output:

{% code overflow="wrap" %}

```json5
User: Tell me about San Francisco
AI:  San Francisco, located in northern California, USA, is a vibrant and culturally rich city known for its iconic landmarks, beautiful vistas, and diverse neighborhoods. It's a popular tourist destination famous for its iconic Golden Gate Bridge, which spans the entrance to the San Francisco Bay, and the iconic Alcatraz Island, home to the infamous federal prison.

The city's famous hills offer stunning views of the bay and the cityscape. Lombard Street, the "crookedest street in the world," is a must-see attraction, with its zigzagging pavement and colorful gardens. Ferry Building Marketplace is a great place to explore local food and artisanal products, and the Pier 39 area is home to sea lions, shops, and restaurants.

San Francisco's diverse neighborhoods each have their unique character. The historic Chinatown is the oldest in North America, while the colorful streets of the Mission District are known for their murals and Latin American culture. The Castro District is famous for its LGBTQ+ community and vibrant nightlife.
```

{% endcode %}

</details>

<details>

<summary>Step-by-step example in NodeJS</summary>

As in the example from Python, we start from the very beginning too. We assume you already have Node.js installed. If not, here is a [guide for beginners](https://docs.aimlapi.com/faq/can-i-use-api-in-nodejs).

We need to create a new folder for the example project:

```bash
mkdir ./aimlapi-welcome
cd ./aimlapi-welcome
```

(Optional) If you use IDE then we recommend to open created folder as workspace. On example, in VSCode you can do it with:

```shell
code .
```

Now create a project file:

```bash
npm init -y
```

Install the required dependencies:

```bash
npm i openai
```

Create a file with the source code:

```bash
touch ./index.js
```

And paste the following content:

<pre class="language-javascript"><code class="lang-javascript"><strong>const { OpenAI } = require("openai");
</strong>
const baseURL = "https://api.aimlapi.com/v1";
const apiKey = "&#x3C;YOUR_AIMLAPI_KEY>";
const systemPrompt = "You are a travel agent. Be descriptive and helpful";
const userPrompt = "Tell me about San Francisco";

const api = new OpenAI({
  apiKey,
  baseURL,
});

const main = async () => {
  const completion = await api.chat.completions.create({
    model: "gpt-4o",
    messages: [
      {
        role: "system",
        content: systemPrompt,
      },
      {
        role: "user",
        content: userPrompt,
      },
    ],
    temperature: 0.7,
    max_tokens: 256,
  });

  const response = completion.choices[0].message.content;

  console.log("User:", userPrompt);
  console.log("AI:", response);
};

main();
</code></pre>

You will see a response that looks like this:

{% code overflow="wrap" %}

```json5
User: Tell me about San Francisco
AI: San Francisco, located in the northern part of California, USA, is a vibrant and culturally rich city known for its iconic landmarks, beautiful scenery, and diverse neighborhoods.

The city is famous for its iconic Golden Gate Bridge, an engineering marvel and one of the most recognized structures in the world. Spanning the Golden Gate Strait, this red-orange suspension bridge connects San Francisco to Marin County and offers breathtaking views of the San Francisco Bay and the Pacific Ocean.
```

{% endcode %}

</details>

## Code Explanation

Both examples are written in different programming languages, but despite that, they look very similar. Let's break down the code step by step and see what's going on.

In the examples above, we are using the OpenAI SDK. The OpenAI SDK is a nice module that allows us to use the AI/ML API without dealing with repetitive boilerplate code for handling HTTP requests. Before we can use the OpenAI SDK, it needs to be imported. The import happens in the following places:

{% tabs %}
{% tab title="JavaScript" %}

```javascript
const { OpenAI } = require("openai");
```

{% endtab %}

{% tab title="Python" %}

```python
from openai import OpenAI
```

{% endtab %}
{% endtabs %}

Simple as it is. The next step is to initialize variables that our code will use. The two main ones are: the base URL and the API key. We already discussed them at the beginning of the article.

{% tabs %}
{% tab title="JavaScript" %}

```javascript
const baseURL = "https://api.aimlapi.com/v1";
const apiKey = "<YOUR_AIMLAPI_KEY>";
const systemPrompt = "You are a travel agent. Be descriptive and helpful";
const userPrompt = "Tell me about San Francisco";
```

{% endtab %}

{% tab title="Python" %}

```python
base_url = "https://api.aimlapi.com/v1"
api_key = "<YOUR_AIMLAPI_KEY>"
system_prompt = "You are a travel agent. Be descriptive and helpful."
user_prompt = "Tell me about San Francisco"
```

{% endtab %}
{% endtabs %}

To communicate with LLM models, users use texts. These texts are usually called "Prompts." Inside our code, we have prompts with two roles: the system and the user. The system prompt is designed to be the main source of instruction for LLM generation, while the user prompt is designed to be user input, the subject of the system prompt. Despite that many models can operate differently, this behavior usually applies to chat LLM models, currently one of the most useful and popular ones.

Inside the code, the prompts are called in variables `systemPrompt`, `userPrompt` in JS, and `system_prompt`, `user_prompt` in Python.

Before we use the API, we need to create an instance of the OpenAI SDK class. It allows us to use all their methods. The instance is created with our imported package, and here we forward two main parameters: the base URL and the API key.

{% tabs %}
{% tab title="JavaScript" %}

```javascript
const api = new OpenAI({
  apiKey,
  baseURL,
});
```

{% endtab %}

{% tab title="Python" %}

```python
api = OpenAI(api_key=api_key, base_url=base_url)
```

{% endtab %}
{% endtabs %}

Because of notation, these two parameters are called slightly differently in these different languages (camel case in JS and snake case in Python), but their functionality is the same.

All preparation steps are done. Now we need to write our functionality and create something great. In the examples above, we make the simplest travel agent. Let's break down the steps of how we send a request to the model.

The best practice is to split the code blocks into complete parts with their own logic and not place executable code inside global module code. This rule applies in both languages we discuss. So we create a main function with all our logic. In JS, this function needs to be async, due to Promises and simplicity. In Python, requests run synchronously.

The OpenAI SDK provides us with methods to communicate with chat models. It is placed inside the `chat.completions.create` function. This function accepts multiple parameters but requires only two: `model` and `messages`.

`model` is a string, the name of the model that you want to use. For the best results, use a model designed for chat, or you can get unpredictable results if the model is not fine-tuned for that purpose. A list of supported models can be found here.

`messages` is an array of objects with a `content` field as prompt and a `role` string that can be one of `system`, `user`, `tool`, `assistant`. With the role, the model can understand what to do with this prompt: Is this an instruction? Is this a user message? Is this an example of how to answer? Is this the result of code execution? The tool role is used for more complex behavior and will be discussed in another article.

In our example, we also use `max_tokens` and `temperature`.

With that knowledge, we can now send our request like the following:

{% tabs %}
{% tab title="JavaScript" %}

```javascript
const completion = await api.chat.completions.create({
  model: "gpt-4o",
  messages: [
    {
      role: "system",
      content: systemPrompt,
    },
    {
      role: "user",
      content: userPrompt,
    },
  ],
  temperature: 0.7,
  max_tokens: 256,
});
```

{% endtab %}

{% tab title="Python" %}

```python
completion = api.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ],
    temperature=0.7,
    max_tokens=256,
)
```

{% endtab %}
{% endtabs %}

The response from the function `chat.completions.create` contains a [completion](https://docs.aimlapi.com/capabilities/completion-or-chat-models). Completion is a fundamental part of LLM models' logic. Every LLM model is some sort of word autocomplete engine, trained by huge amounts of data. The chat models are designed to autocomplete large chunks of messages with prompts and certain roles, but other models can have their own custom logic without even roles.

Inside this completion, we are interested in the text of the generation. We can get it by getting the result from the completion variable:

{% tabs %}
{% tab title="JavaScript" %}

```javascript
const response = completion.choices[0].message.content;
```

{% endtab %}

{% tab title="Python" %}

```python
response = completion.choices[0].message.content
```

{% endtab %}
{% endtabs %}

In certain cases, completion can have multiple results. These results are called choices. Every choice has a message, the product of generation. The string content is placed inside the `content` variable, which we placed inside our response variable above.

In the next steps, we can finally see the results. In both examples, we print the user prompt and response like it was a conversation:

{% tabs %}
{% tab title="JavaScript" %}

```javascript
console.log("User:", userPrompt);
console.log("AI:", response);
```

{% endtab %}

{% tab title="Python" %}

```python
print("User:", user_prompt)
print("AI:", response)
```

{% endtab %}
{% endtabs %}

Voila! Using AI/ML API models is the simplest and most productive way to get into the world of Machine Learning and Artificial Intelligence.

## Future Steps

* [Know more about OpenAI SDK inside AI/ML API](https://docs.aimlapi.com/quickstart/supported-sdks)


# Supported SDKs

A description of the software development kits (SDKs) that can be used to interact with the AIML API.

This page describes the SDK[^1]s that can be used to call our API.

## OpenAI

In the [setting up article](https://docs.aimlapi.com/quickstart/setting-up), we showed an example of how to use the OpenAI SDK with the AI/ML API. We configured the environment from the very beginning and executed our request to the AI/ML API.

We fully support the OpenAI API structure, and you can seamlessly use the features that the OpenAI SDK provides out-of-the-box, including:

* Streaming
* Completions
* Chat Completions
* Audio
* Beta Assistants
* Beta Threads
* Embeddings
* Image Generation
* Uploads

This support provides easy integration into systems already using OpenAI's standards. For example, you can integrate our API into any product that supports LLM models by updating only two things in the configuration: the base URL and the API key.

{% hint style="info" %}
[How do I configure the base URL and API key?](https://docs.aimlapi.com/quickstart/setting-up)
{% endhint %}

## REST API

Because we support the OpenAI API structure, our API can be used with the same endpoints as OpenAI. You can call them from any environment.

### Authorization

AI/ML API authorization is based on a Bearer token. You need to include it in the `Authorization` HTTP header within the request, on example:

```http
Authorization: Bearer <YOUR_AIMLAPI_KEY>
```

### Request Example

When your token is ready you can call our API through HTTP.

{% tabs %}
{% tab title="JavaScript" %}

```javascript
fetch("https://api.aimlapi.com/chat/completions", {
  method: "POST",
  headers: {
    Authorization: "Bearer <YOUR_AIMLAPI_KEY>",
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    model: "gpt-4o",
    messages: [
      {
        role: "user",
        content: "What kind of model are you?",
      },
    ],
    max_tokens: 512,
    stream: false,
  }),
})
  .then((res) => res.json())
  .then(console.log);
```

{% endtab %}

{% tab title="Python" %}

```python
import requests
import json

response = requests.post(
    url="https://api.aimlapi.com/chat/completions",
    headers={
        "Authorization": "Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type": "application/json",
    },
    data=json.dumps(
        {
            "model": "gpt-4o",
            "messages": [
                {
                    "role": "user",
                    "content": "What kind of model are you?",
                },
            ],
            "max_tokens": 512,
            "stream": False,
        }
    ),
)

response.raise_for_status()
print(response.json())
```

{% endtab %}

{% tab title="Shell" %}

```ruby
curl --request POST \
  --url https://api.aimlapi.com/chat/completions \
  --header 'Authorization: Bearer <YOUR_AIMLAPI_KEY>' \
  --header 'Content-Type: application/json' \
  --data '{
    "model": "gpt-4o",
    "messages": [
        {
            "role": "user",
            "content": "What kind of model are you?"
        }
    ],
    "max_tokens": 512,
    "stream": false
}'
```

{% endtab %}
{% endtabs %}

## AI/ML API Python library

We have started developing our own SDK to simplify the use of our service. Currently, it supports only chat completion and embedding models.

{% hint style="success" %}
If you‚Äôd like to contribute to expanding its functionality, feel free to reach out to us on [**Discord**](https://discord.com/invite/hvaUsJpVJf)!
{% endhint %}

### Installation

After obtaining your AIML API key, create an .env file and copy the required contents into it.

```shell
touch .env
```

Copy the code below, paste it into your `.env` file, and set your API key in `AIML_API_KEY="<YOUR_AIMLAPI_KEY>"`, replacing `<YOUR_AIMLAPI_KEY>` with your actual key:

```shell
AIML_API_KEY = "<YOUR_AIMLAPI_KEY>"
AIML_API_URL = "https://api.aimlapi.com/v1"
```

Install `aiml_api` package:

```shell
# install from PyPI
pip install aiml_api
```

### Request Example (Python)

```python
from aiml_api import AIML_API

api = AIML_API()

completion = api.chat.completions.create(
    model="mistralai/Mistral-7B-Instruct-v0.2",
    messages=[
        {"role": "user", "content": "Explain the importance of low-latency LLMs"},
    ],
    temperature=0.7,
    max_tokens=256,
)

response = completion.choices[0].message.content
print("AI:", response)
```

To execute the script, use:

```shell
python3 <your_script_name>.py
```

## Next Steps

* [Check our full list of model IDs](https://docs.aimlapi.com/api-references/model-database)

[^1]: Software Development Kits


# All Model IDs

A full list of available models.

{% hint style="info" %}
If you need to select models based on specific parameters for your task, visit the [dedicated page on our official website](https://aimlapi.com/models/), which offers convenient filtering options. On the selected model‚Äôs page, you can find detailed technical and commercial information.
{% endhint %}

The section **Get Model List via API** contains API reference for the service endpoint, which lets you request the full model list.

The section **Model IDs** lists the identifiers of all available and deprecated models, grouped by category. These IDs are used to specify the exact models in your code, like this:

<figure><img src="https://3927338786-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FROMd1X5PuqtikJ48n2N9%2Fuploads%2Fgit-blob-c8f02b4738bc2873d0edfe070cf6d4c30e0d2bb5%2Fmodel_ID.png?alt=media" alt="" width="442"><figcaption></figcaption></figure>

If you already know the model ID, use the page search function (<kbd>Ctrl+F</kbd> for Win/Linux, <kbd>Command+F</kbd> for Mac) to locate it. The hyperlink will take you directly to the model's API Reference page.

{% hint style="success" %}
**New Model Request**

Can't find the model you need? Join our [Discord community](https://discord.gg/8CwhkUuCR6) to propose new models for integration into our API offerings. Your contributions help us grow and serve you better.
{% endhint %}

## Get Model List via API

## GET /models

>

```json
{"openapi":"3.0.0","info":{"title":"AI/ML Gateway","version":"1.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/models":{"get":{"operationId":"ModelsController_getModels","parameters":[],"responses":{"200":{"description":"","content":{"application/json":{"schema":{"$ref":"#/components/schemas/Model.v1.ModelsResponseDto"}}}}},"tags":["Models"]}}},"components":{"schemas":{"Model.v1.ModelsResponseDto":{"type":"object","properties":{"object":{"type":"string"},"data":{"type":"array","items":{"$ref":"#/components/schemas/Model.v1.ModelListItemDto"}}},"required":["object","data"]},"Model.v1.ModelListItemDto":{"type":"object","properties":{"id":{"type":"string","description":"name of model"},"type":{"type":"string"},"info":{"$ref":"#/components/schemas/Model.v1.ModelInfoDto"},"features":{"type":"array","items":{"type":"string"}}},"required":["id","type","info","features"]},"Model.v1.ModelInfoDto":{"type":"object","properties":{"name":{"type":"string"},"developer":{"type":"string"},"description":{"type":"string"},"contextLength":{"type":"number"},"url":{"type":"string"}},"required":["name","developer","description","contextLength","url"]}}}}
```

## Full List of Model IDs

### Text Models (LLM)

<table><thead><tr><th width="297.4000244140625">Model ID + API Reference link</th><th width="134.20001220703125">Developer</th><th width="105.79998779296875">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a href="text-models-llm/openai/gpt-3.5-turbo">gpt-3.5-turbo</a></td><td>Open AI</td><td>16,000</td><td><a href="https://aimlapi.com/models/chat-gpt-3-5">Chat GPT 3.5 Turbo</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-3.5-turbo">gpt-3.5-turbo-0125</a></td><td>Open AI</td><td>16,000</td><td><a href="https://aimlapi.com/models/chat-gpt-3-5-turbo-0125">Chat GPT-3.5 Turbo 0125</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-3.5-turbo">gpt-3.5-turbo-1106</a></td><td>Open AI</td><td>16,000</td><td><a href="https://aimlapi.com/models/chat-gpt-3-5-turbo-1106">Chat GPT-3.5 Turbo 1106</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o">gpt-4o</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/chat-gpt-4-omni">Chat GPT-4o</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o">gpt-4o-2024-08-06</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-4o-2024-08-06-api">GPT-4o-2024-08-06</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o">gpt-4o-2024-05-13</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-4o-2024-05-13-api">GPT-4o-2024-05-13</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o-mini">gpt-4o-mini</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/chat-gpt-4o-mini">Chat GPT 4o mini</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o-mini">gpt-4o-mini-2024-07-18</a></td><td>Open AI</td><td>128,000</td><td>-</td></tr><tr><td><a href="text-models-llm/openai/gpt-4o">chatgpt-4o-latest</a></td><td>Open AI</td><td>128,000</td><td>-</td></tr><tr><td><a href="text-models-llm/openai/gpt-4o-audio-preview">gpt-4o-audio-preview</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-4o-audio-preview-api">GPT-4o Audio Preview</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o-mini-audio-preview">gpt-4o-mini-audio-preview</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-4o-mini-audio-api">GPT-4o mini Audio</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o-search-preview">gpt-4o-search-preview</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-4o-search-preview-api">GPT-4o Search Preview</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o-mini-search-preview">gpt-4o-mini-search-preview</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-4o-mini-search-preview-api">GPT-4o Mini Search Preview</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4-turbo">gpt-4-turbo</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/chat-gpt-4-turbo">Chat GPT 4 Turbo</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4-turbo">gpt-4-turbo-2024-04-09</a></td><td>Open AI</td><td>128,000</td><td>-</td></tr><tr><td><a href="text-models-llm/openai/gpt-4">gpt-4</a></td><td>Open AI</td><td>8,000</td><td><a href="https://aimlapi.com/models/chat-gpt-4">Chat GPT 4</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4-preview">gpt-4-0125-preview</a></td><td>Open AI</td><td>8,000</td><td>-</td></tr><tr><td><a href="text-models-llm/openai/gpt-4-preview">gpt-4-1106-preview</a></td><td>Open AI</td><td>8,000</td><td>-</td></tr><tr><td><a href="text-models-llm/openai/o1-mini">o1-mini</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/openai-o1-mini-api">OpenAI o1-mini</a></td></tr><tr><td><a href="text-models-llm/openai/o1-mini">o1-mini-2024-09-12</a></td><td>Open AI</td><td>128,000</td><td>-</td></tr><tr><td><a href="text-models-llm/openai/o1">o1</a></td><td>Open AI</td><td>200,000</td><td><a href="https://aimlapi.com/models/openai-o1-api">OpenAI o1</a></td></tr><tr><td><a href="text-models-llm/openai/o3">openai/o3-2025-04-16</a></td><td>Open AI</td><td>200,000</td><td><a href="https://aimlapi.com/models/o3">o3</a></td></tr><tr><td><a href="text-models-llm/openai/o3-mini">o3-mini</a></td><td>Open AI</td><td>200,000</td><td><a href="https://aimlapi.com/models/openai-o3-mini-api">OpenAI o3 mini</a></td></tr><tr><td><a href="text-models-llm/openai/o3-pro">openai/o3-pro</a></td><td>Open AI</td><td>200,000</td><td><a href="https://aimlapi.com/models/o3-pro">o3-pro</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4.1">openai/gpt-4.1-2025-04-14</a></td><td>Open AI</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gpt-4-1">GPT-4.1</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4.1-mini">openai/gpt-4.1-mini-2025-04-14</a></td><td>Open AI</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gpt-4-1-mini-api">GPT-4.1 Mini</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4.1-nano">openai/gpt-4.1-nano-2025-04-14</a></td><td>Open AI</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gpt-4-1-nano-api">GPT-4.1 Nano</a></td></tr><tr><td><a href="text-models-llm/openai/o4-mini">openai/o4-mini-2025-04-16</a></td><td>Open AI</td><td>200,000</td><td><a href="https://aimlapi.com/models/gpt-o4-mini-2025-04-16">GPT-o4-mini-2025-04-16</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-oss-20b">openai/gpt-oss-20b</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-oss-20b">GPT OSS 20B</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-oss-120b">openai/gpt-oss-120b</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-oss-120b">GPT OSS 120B</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5">openai/gpt-5-2025-08-07</a></td><td>Open AI</td><td>400,000</td><td><a href="https://aimlapi.com/models/gpt-5">GPT-5</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-mini">openai/gpt-5-mini-2025-08-07</a></td><td>Open AI</td><td>400,000</td><td><a href="https://aimlapi.com/models/gpt-5-mini">GPT-5 Mini</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-nano">openai/gpt-5-nano-2025-08-07</a></td><td>Open AI</td><td>400,000</td><td><a href="https://aimlapi.com/models/gpt-5-nano">GPT-5 Nano</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-chat">openai/gpt-5-chat-latest</a></td><td>Open AI</td><td>400,000</td><td><a href="https://aimlapi.com/models/gpt-5-chat">GPT-5 Chat</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-1">openai/gpt-5-1</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-5-1">GPT-5.1</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-1-chat-latest">openai/gpt-5-1-chat-latest</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-5-1-chat-latest">GPT-5.1 Chat Latest</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-1-codex">openai/gpt-5-1-codex</a></td><td>Open AI</td><td>400,000</td><td><a href="https://aimlapi.com/models/gpt-5-1-codex">GPT-5.1 Codex</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-1-codex-mini">openai/gpt-5-1-codex-mini</a></td><td>Open AI</td><td>400,000</td><td><a href="https://aimlapi.com/models/gpt-5-1-codex-mini">GPT-5.1 Codex Mini</a></td></tr><tr><td><a href="text-models-llm/anthropic/claude-3-opus">claude-3-opus-20240229</a></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-3-opus">Claude 3 Opus</a></td></tr><tr><td><a href="text-models-llm/anthropic/claude-3-haiku">claude-3-haiku-20240307</a></td><td>Anthropic</td><td>200,000</td><td>-</td></tr><tr><td><a href="text-models-llm/anthropic/claude-3.5-haiku">claude-3-5-haiku-20241022</a></td><td>Anthropic</td><td>200,000</td><td>-</td></tr><tr><td><a href="text-models-llm/anthropic/claude-3.7-sonnet">claude-3-7-sonnet-20250219</a></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-3-7-sonnet-api">Claude 3.7 Sonnet</a></td></tr><tr><td><a href="text-models-llm/anthropic/claude-4-opus">anthropic/claude-opus-4</a></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-4-opus">Claude 4 Opus</a></td></tr><tr><td><a href="text-models-llm/anthropic/claude-opus-4.1">anthropic/claude-opus-4.1<br>claude-opus-4-1<br>claude-opus-4-1-20250805</a></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-opus-4-1">Claude Opus 4.1</a></td></tr><tr><td><a href="text-models-llm/anthropic/claude-4-sonnet">anthropic/claude-sonnet-4</a></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-4-sonnet">Claude 4 Sonnet</a></td></tr><tr><td><p><a href="text-models-llm/anthropic/claude-4-5-sonnet">claude-sonnet-4-5-20250929</a></p><p><a href="text-models-llm/anthropic/claude-4-5-sonnet">anthropic/claude-sonnet-4.5</a></p><p><a href="text-models-llm/anthropic/claude-4-5-sonnet">claude-sonnet-4-5</a></p></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-4-5-sonnet">Claude 4.5 Sonnet</a></td></tr><tr><td><p><a href="text-models-llm/anthropic/claude-4.5-haiku">anthropic/claude-haiku-4.5</a><br><a href="text-models-llm/anthropic/claude-4.5-haiku">claude-haiku-4-5</a></p><p><a href="text-models-llm/anthropic/claude-4.5-haiku">claude-haiku-4-5-20251001</a></p></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-4-5-haiku">Claude 4.5 Haiku</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen2.5-7b-instruct-turbo">Qwen/Qwen2.5-7B-Instruct-Turbo</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-2-5-7b-instruct-api">Qwen 2.5 7B Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen2.5-coder-32b-instruct">Qwen/Qwen2.5-Coder-32B-Instruct</a></td><td>Alibaba Cloud</td><td>131,000</td><td>-</td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen-max">qwen-max</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-max-api">Qwen Max</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen-max">qwen-max-2025-01-25</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-max-2025-01-25-api">Qwen Max 2025-01-25</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen-plus">qwen-plus</a></td><td>Alibaba Cloud</td><td>131,000</td><td><a href="https://aimlapi.com/models/qwen-plus-api">Qwen Plus</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen-turbo">qwen-turbo</a></td><td>Alibaba Cloud</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/qwen-turbo-api">Qwen Turbo</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen2.5-72b-instruct-turbo">Qwen/Qwen2.5-72B-Instruct-Turbo</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-2-5-72b-instruct-turbo">Qwen 2.5 72B Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen-qwq-32b">Qwen/QwQ-32B</a></td><td>Alibaba Cloud</td><td>131,000</td><td><a href="https://aimlapi.com/models/qwq-32b-api">QwQ-32B</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-235b-a22b">Qwen/Qwen3-235B-A22B-fp8-tput</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-3-235b-a22b-api">Qwen 3 235B A22B</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-32b">alibaba/qwen3-32b</a></td><td>Alibaba Cloud</td><td>131,000</td><td><a href="https://aimlapi.com/models/qwen3-32b">Qwen3-32B</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-coder-480b-a35b-instruct">alibaba/qwen3-coder-480b-a35b-instruct</a></td><td>Alibaba Cloud</td><td>262,000</td><td><a href="https://aimlapi.com/models/qwen3-coder-480b-a35b-instruct">Qwen3 Coder</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-235b-a22b-thinking-2507">alibaba/qwen3-235b-a22b-thinking-2507</a></td><td>Alibaba Cloud</td><td>262,000</td><td><a href="https://aimlapi.com/models/qwen3-235b-a22b">Qwen3 235B A22B Thinking</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-next-80b-a3b-instruct">alibaba/qwen3-next-80b-a3b-instruct</a></td><td>Alibaba Cloud</td><td>262,000</td><td><a href="https://aimlapi.com/models/qwen3-next-80b-a3b-instruct">Qwen3-Next-80B-A3B Instruct</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-next-80b-a3b-thinking">alibaba/qwen3-next-80b-a3b-thinking</a></td><td>Alibaba Cloud</td><td>262,000</td><td><a href="https://aimlapi.com/models/qwen3-next-80b-a3b-thinking">Qwen3-Next-80B-A3B Thinking</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-max-preview">alibaba/qwen3-max-preview</a></td><td>Alibaba Cloud</td><td>258,000</td><td><a href="text-models-llm/alibaba-cloud/qwen3-max-preview">Qwen3-Max Preview</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-max-instruct">alibaba/qwen3-max-instruct</a></td><td>Alibaba Cloud</td><td>262,000</td><td><a href="text-models-llm/alibaba-cloud/qwen3-max-instruct">Qwen3-Max Instruct</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-omni-30b-a3b-captioner">qwen3-omni-30b-a3b-captioner</a></td><td>Alibaba Cloud</td><td>65,000</td><td><a href="text-models-llm/alibaba-cloud/qwen3-omni-30b-a3b-captioner">qwen3-omni-30b-a3b-captioner</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-chat">deepseek-chat or<br>deepseek/deepseek-chat or<br>deepseek/deepseek-chat-v3-0324</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3">DeepSeek V3</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-r1">deepseek/deepseek-r1 or<br>deepseek-reasoner</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-r1-api">DeepSeek R1</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-prover-v2">deepseek/deepseek-prover-v2</a></td><td>DeepSeek</td><td>164,000</td><td><a href="https://aimlapi.com/models/deepseek-prover-v2-api">DeepSeek Prover V2</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-chat-v3.1">deepseek/deepseek-chat-v3.1</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3-1-chat">DeepSeek V3.1 Chat</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-reasoner-v3.1">deepseek/deepseek-reasoner-v3.1</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3-1-reasoner">DeepSeek V3.1 Reasoner</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-reasoner-v3.2-exp-thinking">deepseek/deepseek-thinking-v3.2-exp</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3-2-exp-thinking">DeepSeek V3.2-Exp Thinking</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-reasoner-v3.2-exp-non-thinking">deepseek/deepseek-non-thinking-v3.2-exp</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3-2-exp-non-thinking">DeepSeek V3.2-Exp Non-Thinking</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-reasoner-v3.1-terminus">deepseek/deepseek-reasoner-v3.1-terminus</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3-1-terminus-reasoning">DeepSeek V3.1 Terminus Reasoning</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-non-reasoner-v3.1-terminus">deepseek/deepseek-non-reasoner-v3.1-terminus</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3-1-terminus-non-reasoning">DeepSeek V3.1 Terminus Non-Reasoning</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/mixtral-8x7b-instruct-v0.1">mistralai/Mixtral-8x7B-Instruct-v0.1</a></td><td>Mistral AI</td><td>64,000</td><td><a href="https://aimlapi.com/models/mixtral-8x7b-instruct-v01">Mixtral-8x7B Instruct v0.1</a></td></tr><tr><td><a href="text-models-llm/meta/llama-3.3-70b-instruct-turbo">meta-llama/Llama-3.3-70B-Instruct-Turbo</a></td><td>Meta</td><td>128,000</td><td><a href="https://aimlapi.com/models/meta-llama-3-3-70b-instruct-turbo-api">Meta Llama 3.3 70B Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/meta/llama-3.2-3b-instruct-turbo">meta-llama/Llama-3.2-3B-Instruct-Turbo</a></td><td>Meta</td><td>131,000</td><td><a href="https://aimlapi.com/models/llama-3-2-3b-instruct-turbo">Llama 3.2 3B Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/meta/meta-llama-3-8b-instruct-lite">meta-llama/Meta-Llama-3-8B-Instruct-Lite</a></td><td>Meta</td><td>9,000</td><td><a href="https://aimlapi.com/models/llama-3-8b-instruct-lite-api">Llama 3 8B Instruct Lite</a></td></tr><tr><td><a href="text-models-llm/meta/llama-3-chat-hf">meta-llama/Llama-3-70b-chat-hf</a></td><td>Meta</td><td>8,000</td><td><a href="https://aimlapi.com/models/meta-llama-3-70b-instruct">Llama 3 70B Instruct Reference</a></td></tr><tr><td><a href="text-models-llm/meta/meta-llama-3.1-405b-instruct-turbo">meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo</a></td><td>Meta</td><td>4,000</td><td><a href="https://aimlapi.com/models/llama-3-1-405b-api">Llama 3.1 (405B) Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/meta/meta-llama-3.1-8b-instruct-turbo">meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo</a></td><td>Meta</td><td>128,000</td><td><a href="https://aimlapi.com/models/llama-3-1-8b-api">Llama 3.1 8B Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/meta/meta-llama-3.1-70b-instruct-turbo">meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo</a></td><td>Meta</td><td>128,000</td><td><a href="https://aimlapi.com/models/llama-3-1-70b-instruct-turbo-api">Llama 3.1 70B Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/meta/llama-4-scout">meta-llama/llama-4-scout</a></td><td>Meta</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/llama-4-scout-api">Llama 4 Scout</a></td></tr><tr><td><a href="text-models-llm/meta/llama-4-maverick">meta-llama/llama-4-maverick</a></td><td>Meta</td><td>256,000</td><td><a href="https://aimlapi.com/models/llama-4-maverick-api">Llama 4 Maverick</a></td></tr><tr><td><a href="text-models-llm/meta/llama-3.3-70b-versatile">meta-llama/llama-3.3-70b-versatile</a></td><td>Meta</td><td>131,000</td><td><a href="text-models-llm/meta/llama-3.3-70b-versatile">Llama 3.3 70B Versatile</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/mistral-7b-instruct">mistralai/Mistral-7B-Instruct-v0.2</a></td><td>Mistral AI</td><td>32,000</td><td><a href="https://aimlapi.com/models/mistral-7b-instruct-v02">Mistral (7B) Instruct v0.2</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/mistral-7b-instruct">mistralai/Mistral-7B-Instruct-v0.1</a></td><td>Mistral AI</td><td>8,000</td><td><a href="https://aimlapi.com/models/mistral-7b-instruct">Mistral (7B) Instruct v0.1</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/mistral-7b-instruct">mistralai/Mistral-7B-Instruct-v0.3</a></td><td>Mistral AI</td><td>32,000</td><td><a href="https://aimlapi.com/models/mistral-7b-instruct-v0-3">Mistral (7B) Instruct v0.3</a></td></tr><tr><td><a href="text-models-llm/google/gemini-2.0-flash-exp">gemini-2.0-flash-exp</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-2-0-flash-experimental">Gemini 2.0 Flash Experimental</a></td></tr><tr><td><a href="text-models-llm/google/gemini-2.0-flash">gemini-2.0-flash</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-2-0-flash-api">Gemini 2.0 Flash</a></td></tr><tr><td><a href="text-models-llm/google/gemini-2.5-flash-lite-preview">google/gemini-2.5-flash-lite-preview</a></td><td>Google</td><td>1,000,000</td><td>‚Äì</td></tr><tr><td><a href="text-models-llm/google/gemini-2.5-flash">google/gemini-2.5-flash</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-2-5-flash-api">Gemini 2.5 Flash</a></td></tr><tr><td><a href="text-models-llm/google/gemini-2.5-pro">google/gemini-2.5-pro</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-pro-2-5-api">Gemini 2.5 Pro</a></td></tr><tr><td><a href="text-models-llm/google/gemma-3">google/gemma-3-4b-it</a></td><td>Google</td><td>128,000</td><td><a href="https://aimlapi.com/models/gemma-3-4b-api">Gemma 3 (4B)</a></td></tr><tr><td><a href="text-models-llm/google/gemma-3">google/gemma-3-12b-it</a></td><td>Google</td><td>128,000</td><td><a href="https://aimlapi.com/models/gemma-3-12b-api">Gemma 3 (12B)</a></td></tr><tr><td><a href="text-models-llm/google/gemma-3">google/gemma-3-27b-it</a></td><td>Google</td><td>128,000</td><td><a href="https://aimlapi.com/models/gemma-3-27b-api">Gemma 3 (27B)</a></td></tr><tr><td><a href="text-models-llm/google/gemma-3n-4b">google/gemma-3n-e4b-it</a></td><td>Google</td><td>8,192</td><td><a href="https://aimlapi.com/models/gemma-3n-4b">Gemma 3n 4B</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/mistral-tiny">mistralai/mistral-tiny</a></td><td>Mistral AI</td><td>32,000</td><td><a href="https://aimlapi.com/models/mistral-tiny-api">Mistral Tiny</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/mistral-nemo">mistralai/mistral-nemo</a></td><td>Mistral AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/mistral-nemo-api">Mistral Nemo</a></td></tr><tr><td><a href="text-models-llm/anthracite/magnum-v4">anthracite-org/magnum-v4-72b</a></td><td>Anthracite</td><td>32,000</td><td><a href="https://aimlapi.com/models/magnum-v4-72b-api">Magnum v4 72B</a></td></tr><tr><td><a href="text-models-llm/nvidia/llama-3.1-nemotron-70b">nvidia/llama-3.1-nemotron-70b-instruct</a></td><td>NVIDIA</td><td>128,000</td><td><a href="https://aimlapi.com/models/llama-3-1-nemotron-70b-instruct-api">Llama 3.1 Nemotron 70B Instruct</a></td></tr><tr><td><a href="text-models-llm/nvidia/nemotron-nano-9b-v2">nvidia/nemotron-nano-9b-v2</a></td><td>NVIDIA</td><td>128,000</td><td><em>Coming Soon</em></td></tr><tr><td><a href="text-models-llm/nvidia/llama-3.1-nemotron-70b-1">nvidia/nemotron-nano-12b-v2-vl</a></td><td>NVIDIA</td><td>128,000</td><td><em>Coming Soon</em></td></tr><tr><td><a href="text-models-llm/cohere/command-a">cohere/command-a</a></td><td>Cohere</td><td>256,000</td><td><a href="https://aimlapi.com/models/command-a">Command A</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/codestral-2501">mistralai/codestral-2501</a></td><td>Mistral AI</td><td>256,000</td><td><a href="https://aimlapi.com/models/mistral-codestral-2501-api">Mistral Codestral-2501</a></td></tr><tr><td><a href="text-models-llm/minimax/text-01">MiniMax-Text-01</a></td><td>MiniMax</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/minimax-text-01-api">MiniMax-Text-01</a></td></tr><tr><td><a href="text-models-llm/minimax/m1">minimax/m1</a></td><td>MiniMax</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/minimax-m1">MiniMax M1</a></td></tr><tr><td><a href="text-models-llm/minimax/m2">minimax/m2</a></td><td>MiniMax</td><td>200,000</td><td><a href="https://aimlapi.com/models/minimax-m2">MiniMax M2</a></td></tr><tr><td><a href="text-models-llm/moonshot/kimi-k2-preview">moonshot/kimi-k2-preview</a></td><td>Moonshot</td><td>131,000</td><td><a href="https://aimlapi.com/models/kimi-k2">Kimi-K2</a></td></tr><tr><td><a href="text-models-llm/moonshot/kimi-k2-preview">moonshot/kimi-k2-0905-preview</a></td><td>Moonshot</td><td>256,000</td><td><a href="https://aimlapi.com/models/kimi-k2">Kimi-K2</a></td></tr><tr><td><a href="text-models-llm/moonshot/kimi-k2-turbo-preview">moonshot/kimi-k2-turbo-preview</a></td><td>Moonshot</td><td>256,000</td><td><a href="https://aimlapi.com/models/kimi-k2-turbo-preview">Kimi K2 Turbo Preview</a></td></tr><tr><td><a href="text-models-llm/nousresearch/hermes-4-405b">nousresearch/hermes-4-405b</a></td><td>NousResearch</td><td>131,000</td><td><em>-</em></td></tr><tr><td><a href="text-models-llm/perplexity/sonar">perplexity/sonar</a></td><td>Perplexity</td><td>128,000</td><td><a href="https://aimlapi.com/models/perplexity-sonar">Sonar</a></td></tr><tr><td><a href="text-models-llm/perplexity/sonar-pro">perplexity/sonar-pro</a></td><td>Perplexity</td><td>200,000</td><td><a href="https://aimlapi.com/models/perplexity-sonar-pro">Sonar Pro</a></td></tr><tr><td><a href="text-models-llm/xai/grok-3-beta">x-ai/grok-3-beta</a></td><td>xAI</td><td>131,000</td><td><a href="https://aimlapi.com/models/grok-3-beta-api">Grok 3 Beta</a></td></tr><tr><td><a href="text-models-llm/xai/grok-3-mini-beta">x-ai/grok-3-mini-beta</a></td><td>xAI</td><td>131,000</td><td><a href="https://aimlapi.com/models/grok-3-beta-mini-api">Grok 3 Beta Mini</a></td></tr><tr><td><a href="text-models-llm/xai/grok-4">x-ai/grok-4-07-09</a></td><td>xAI</td><td>256,000</td><td><a href="https://aimlapi.com/models/grok-4">Grok 4</a></td></tr><tr><td><a href="text-models-llm/xai/grok-code-fast-1">x-ai/grok-code-fast-1</a></td><td>xAI</td><td>256,000</td><td><a href="https://aimlapi.com/models/grok-code-fast-1">Grok Code Fast 1</a></td></tr><tr><td><a href="text-models-llm/xai/grok-4-fast-non-reasoning">x-ai/grok-4-fast-non-reasoning</a></td><td>xAI</td><td>2,000,000</td><td><a href="https://aimlapi.com/models/grok-4-fast">Grok 4 Fast</a></td></tr><tr><td><a href="text-models-llm/xai/grok-4-fast-reasoning">x-ai/grok-4-fast-reasoning</a></td><td>xAI</td><td>2,000,000</td><td><a href="https://aimlapi.com/models/grok-4-fast-reasoning">Grok 4 Fast Reasoning</a></td></tr><tr><td><a href="text-models-llm/zhipu/glm-4.5-air">zhipu/glm-4.5-air</a></td><td>Zhipu</td><td>128,000</td><td><a href="https://aimlapi.com/models/glm-4-5-air">GLM-4.5 Air</a></td></tr><tr><td><a href="text-models-llm/zhipu/glm-4.5">zhipu/glm-4.5</a></td><td>Zhipu</td><td>128,000</td><td><a href="https://aimlapi.com/models/glm-4-5">GLM-4.5</a></td></tr><tr><td><a href="text-models-llm/zhipu/glm-4.6">zhipu/glm-4.6</a></td><td>Zhipu</td><td>200,000</td><td><a href="text-models-llm/zhipu/glm-4.6">GLM-4.6</a></td></tr></tbody></table>

### Image Models

<table data-full-width="true"><thead><tr><th width="274.20001220703125">Model ID + API Reference link</th><th width="123.39996337890625">Developer</th><th width="103.79998779296875">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a href="image-models/alibaba-cloud/qwen-image">alibaba/qwen-image</a></td><td>Alibaba Cloud</td><td></td><td><a href="https://aimlapi.com/models/qwen-image">Qwen Image</a></td></tr><tr><td><a href="image-models/alibaba-cloud/qwen-image-edit">alibaba/qwen-image-edit</a></td><td>Alibaba Cloud</td><td></td><td><a href="https://aimlapi.com/models/qwen-image-edit">Qwen Image Edit</a></td></tr><tr><td><a href="image-models/bytedance/seedream-3.0">bytedance/seedream-3.0</a></td><td>ByteDance</td><td></td><td><a href="https://aimlapi.com/models/seedream-3-0">Seedream 3.0</a></td></tr><tr><td><a href="image-models/bytedance/seededit-3.0-image-to-image">bytedance/seededit-3.0-i2i</a></td><td>ByteDance</td><td></td><td><a href="https://aimlapi.com/models/seedream-3-0">Seedream 3.0</a></td></tr><tr><td><a href="image-models/bytedance/seedream-v4-text-to-image">bytedance/seedream-v4-text-to-image</a></td><td>ByteDance</td><td></td><td><a href="https://aimlapi.com/models/seedream-4">Seedream 4 Text-to-Image</a></td></tr><tr><td><a href="image-models/bytedance/seedream-v4-edit-image-to-image">bytedance/seedream-v4-edit</a></td><td>ByteDance</td><td></td><td><a href="https://aimlapi.com/models/seedream-4-edit">Seedream 4 Edit</a></td></tr><tr><td><a href="image-models/bytedance/uso">bytedance/uso</a></td><td>ByteDance</td><td></td><td><a href="https://aimlapi.com/models/uso">USO</a></td></tr><tr><td><a href="image-models/flux/flux-pro">flux-pro</a></td><td>Flux</td><td></td><td><a href="https://aimlapi.com/models/flux-1-pro-api">FLUX.1 [pro]</a></td></tr><tr><td><a href="image-models/flux/flux-pro">flux-pro/v1.1</a></td><td>Flux</td><td></td><td><a href="https://aimlapi.com/models/flux-1-1-pro-api">FLUX 1.1 [pro]</a></td></tr><tr><td><a href="image-models/flux/flux-pro-v1.1-ultra">flux-pro/v1.1-ultra</a></td><td>Flux</td><td></td><td><a href="https://aimlapi.com/models/flux-1-1-pro-ultra-api">FLUX 1.1 [pro ultra]</a></td></tr><tr><td><a href="image-models/flux/flux-realism">flux-realism</a></td><td>Flux</td><td></td><td><a href="https://aimlapi.com/models/flux-realism-lora-api">FLUX Realism LoRA</a></td></tr><tr><td><a href="image-models/flux/flux-dev">flux/dev</a></td><td>Flux</td><td></td><td><a href="https://aimlapi.com/models/flux-1-dev-api">FLUX.1 [dev]</a></td></tr><tr><td><a href="image-models/flux/flux-dev-image-to-image">flux/dev/image-to-image</a></td><td>Flux</td><td></td><td>-</td></tr><tr><td><a href="image-models/flux/flux-schnell">flux/schnell</a></td><td>Flux</td><td></td><td><a href="https://aimlapi.com/models/flux-1-schnell-api">FLUX.1 [schnell]</a></td></tr><tr><td><a href="image-models/flux/flux-kontext-max-text-to-image">flux/kontext-max/text-to-image</a></td><td>Flux</td><td></td><td><a href="https://aimlapi.com/models/flux-1-kontext-max">FLUX.1 Kontext [max]</a></td></tr><tr><td><a href="image-models/flux/flux-kontext-max-image-to-image">flux/kontext-max/image-to-image</a></td><td>Flux</td><td></td><td><a href="https://aimlapi.com/models/flux-1-kontext-max">FLUX.1 Kontext [max]</a></td></tr><tr><td><a href="image-models/flux/flux-kontext-pro-text-to-image">flux/kontext-pro/text-to-image</a></td><td>Flux</td><td></td><td><a href="https://aimlapi.com/models/flux-1-kontext-pro">Flux.1 Kontext [pro]</a></td></tr><tr><td><a href="image-models/flux/flux-kontext-pro-image-to-image">flux/kontext-pro/image-to-image</a></td><td>Flux</td><td></td><td><a href="https://aimlapi.com/models/flux-1-kontext-pro">Flux.1 Kontext [pro]</a></td></tr><tr><td><a href="image-models/flux/flux-srpo-text-to-image">flux/srpo</a></td><td>Flux</td><td></td><td><a href="https://aimlapi.com/models/flux-1-srpo-t2i">FLUX.1 SRPO Text-to-Image</a></td></tr><tr><td><a href="image-models/flux/flux-srpo-image-to-image">flux/srpo/image-to-image</a></td><td>Flux</td><td></td><td><a href="https://aimlapi.com/models/flux-1-srpo-i2i">FLUX.1 SRPO Image-to-Image</a></td></tr><tr><td><a href="image-models/google/imagen-3.0">imagen-3.0-generate-002</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/imagen-3-api">Imagen 3</a></td></tr><tr><td><a href="image-models/google/imagen-4-preview">google/imagen4/preview</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/imagen-4-preview">Imagen 4 Preview</a></td></tr><tr><td><a href="image-models/google/imagen-4-ultra">imagen-4.0-ultra-generate-preview-06-06</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/imagen-4-ultra">Imagen 4 Ultra</a></td></tr><tr><td><a href="image-models/google/gemini-2.5-flash-image">google/gemini-2.5-flash-image</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/gemini-2-5-flash-image">Gemini 2.5 Flash Image</a></td></tr><tr><td><a href="image-models/google/gemini-2.5-flash-image-edit">google/gemini-2.5-flash-image-edit</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/gemini-2-5-flash-image-edit">Gemini 2.5 Flash Image Edit</a></td></tr><tr><td><a href="image-models/google/imagen-4-generate">google/imagen-4.0-generate-001</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/imagen-4-0-generate-001">Imagen 4.0 Generate</a></td></tr><tr><td><a href="image-models/google/imagen-4-fast-generate">google/imagen-4.0-fast-generate-001</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/imagen-4-0-fast-generate-001">Imagen 4.0 Fast Generate</a></td></tr><tr><td><a href="image-models/google/imagen-4-ultra-generate">google/imagen-4.0-ultra-generate-001</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/imagen-4-0-ultra-generate-001">Imagen 4.0 Ultra Generate</a></td></tr><tr><td><a href="image-models/openai/dall-e-2">dall-e-2</a></td><td>OpenAI</td><td></td><td><a href="https://aimlapi.com/models/openai-dall-e-2-api">OpenAI DALL¬∑E 2</a></td></tr><tr><td><a href="image-models/openai/dall-e-3">dall-e-3</a></td><td>OpenAI</td><td></td><td><a href="https://aimlapi.com/models/openai-dall-e-3">OpenAI DALL¬∑E 3</a></td></tr><tr><td><a href="image-models/openai/gpt-image-1">openai/gpt-image-1</a></td><td>OpenAI</td><td></td><td><a href="https://aimlapi.com/models/gpt-image-1">gpt-image-1</a></td></tr><tr><td><a href="image-models/recraftai/recraft-v3">recraft-v3</a></td><td>Recraft AI</td><td></td><td><a href="https://aimlapi.com/models/recraft-v3">Recraft v3</a></td></tr><tr><td><a href="image-models/reve/reve-create-image">reve/create-image</a></td><td>Reve</td><td></td><td><a href="https://aimlapi.com/models/reve-create-image">Reve Create Image</a></td></tr><tr><td><a href="image-models/reve/reve-edit-image">reve/edit-image</a></td><td>Reve</td><td></td><td><a href="https://aimlapi.com/models/reve-edit-image">Reve Edit Image</a></td></tr><tr><td><a href="image-models/reve/reve-remix-edit-image">reve/remix-edit-image</a></td><td>Reve</td><td></td><td><a href="https://aimlapi.com/models/reve-remix-image">Reve Remix Image</a></td></tr><tr><td><a href="image-models/stability-ai/stable-diffusion-v3-medium">stable-diffusion-v3-medium</a></td><td>Stability AI</td><td></td><td><a href="https://aimlapi.com/models/stable-diffusion-3-api">Stable Diffusion 3</a></td></tr><tr><td><a href="image-models/stability-ai/stable-diffusion-v35-large">stable-diffusion-v35-large</a></td><td>Stability AI</td><td></td><td><a href="https://aimlapi.com/models/stable-diffusion-3-5-large-api">Stable Diffusion 3.5 Large</a></td></tr><tr><td><a href="image-models/tencent/hunyuan-image-v3-text-to-image">hunyuan/hunyuan-image-v3-text-to-image</a></td><td>Tencent</td><td></td><td><a href="https://aimlapi.com/models/hunyuanimage-3-0">HunyuanImage 3.0</a></td></tr><tr><td><a href="image-models/topaz-labs/sharpen">topaz-labs/sharpen</a></td><td>Topaz Labs</td><td></td><td><a href="https://aimlapi.com/models/sharpen">Sharpen</a></td></tr><tr><td><a href="image-models/topaz-labs/sharpen-generative">topaz-labs/sharpen-gen</a></td><td>Topaz Labs</td><td></td><td><a href="https://aimlapi.com/models/sharpen">Sharpen Generative</a></td></tr><tr><td><a href="image-models/xai/grok-2-image">x-ai/grok-2-image</a></td><td>xAI</td><td></td><td><a href="https://aimlapi.com/models/grok-2-image">Grok 2 Image</a></td></tr></tbody></table>

### Video Models

<table data-full-width="true"><thead><tr><th width="308.5999755859375">Model ID + API Reference link</th><th width="112.20001220703125">Developer</th><th width="87">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a href="video-models/alibaba-cloud/wan-2.1-plus-text-to-video">alibaba/wan2.1-t2v-plus</a></td><td>Alibaba Cloud</td><td></td><td><a href="https://aimlapi.com/models/wan2-1-plus">Wan2.1 Plus</a></td></tr><tr><td><a href="video-models/alibaba-cloud/wan-2.1-turbo-text-to-video">alibaba/wan2.1-t2v-turbo</a></td><td>Alibaba Cloud</td><td></td><td><a href="https://aimlapi.com/models/wan2-1-turbo">Wan2.1 Turbo</a></td></tr><tr><td><a href="video-models/alibaba-cloud/wan-2.2-plus-text-to-video">alibaba/wan2.2-t2v-plus</a></td><td>Alibaba Cloud</td><td></td><td><a href="https://aimlapi.com/models/wan2-2-t2v">Wan 2.2 T2V</a></td></tr><tr><td><a href="video-models/alibaba-cloud/wan-2.5-preview-text-to-video">alibaba/wan2.5-t2v-preview</a></td><td>Alibaba Cloud</td><td></td><td><a href="https://aimlapi.com/models/wan-2-5-text-to-video">Wan 2.5 Text-to-Video</a></td></tr><tr><td><a href="video-models/alibaba-cloud/wan-2.5-preview-image-to-video">alibaba/wan2.5-i2v-preview</a></td><td>Alibaba Cloud</td><td></td><td><a href="https://aimlapi.com/models/wan-2-5-image-to-video">Wan 2.5 Image-to-Video</a></td></tr><tr><td><a href="video-models/alibaba-cloud/wan-2.2-14b-animate-replace-image-to-video">alibaba/wan2.2-14b-animate-replace</a></td><td>Alibaba Cloud</td><td></td><td><a href="video-models/alibaba-cloud/wan-2.2-14b-animate-replace-image-to-video">Wan 2.2 14b animate replace</a></td></tr><tr><td><a href="video-models/alibaba-cloud/wan-2.2-14b-animate-move-image-to-video">alibaba/wan2.2-14b-animate-move</a></td><td>Alibaba Cloud</td><td></td><td><a href="video-models/alibaba-cloud/wan-2.2-14b-animate-move-image-to-video">Wan 2.2 14b animate move</a></td></tr><tr><td><a href="video-models/alibaba-cloud/wan2.2-vace-fun-a14b-reframe-image-to-video">alibaba/wan2.2-vace-fun-a14b-reframe</a></td><td>Alibaba Cloud</td><td></td><td><a href="video-models/alibaba-cloud/wan2.2-vace-fun-a14b-reframe-image-to-video">Wan 2.2 vace fun 14b reframe</a></td></tr><tr><td><a href="video-models/alibaba-cloud/wan2.2-vace-fun-a14b-outpainting-image-to-video">alibaba/wan2.2-vace-fun-a14b-outpainting</a></td><td>Alibaba Cloud</td><td></td><td><a href="video-models/alibaba-cloud/wan2.2-vace-fun-a14b-outpainting-image-to-video">Wan 2.2 vace fun 14b outpainting</a></td></tr><tr><td><a href="video-models/alibaba-cloud/wan2.2-vace-fun-a14b-inpainting-image-to-video">alibaba/wan2.2-vace-fun-a14b-inpainting</a></td><td>Alibaba Cloud</td><td></td><td><a href="video-models/alibaba-cloud/wan2.2-vace-fun-a14b-inpainting-image-to-video">Wan 2.2 vace fun 14b inpainting</a></td></tr><tr><td><a href="video-models/alibaba-cloud/wan2.2-vace-fun-a14b-pose-image-to-video">alibaba/wan2.2-vace-fun-a14b-pose</a></td><td>Alibaba Cloud</td><td></td><td><a href="video-models/alibaba-cloud/wan2.2-vace-fun-a14b-pose-image-to-video">Wan 2.2 vace fun 14b pose</a></td></tr><tr><td><a href="video-models/alibaba-cloud/wan2.2-vace-fun-a14b-depth-image-to-video">alibaba/wan2.2-vace-fun-14b-depth</a></td><td>Alibaba Cloud</td><td></td><td><a href="video-models/alibaba-cloud/wan2.2-vace-fun-a14b-depth-image-to-video">Wan 2.2 vace fun 14b depth</a></td></tr><tr><td><a href="video-models/bytedance/seedance-1.0-lite-text-to-video">bytedance/seedance-1-0-lite-t2v</a></td><td>ByteDance</td><td></td><td><a href="https://aimlapi.com/models/seedance-1-0">Seedance 1.0 lite Text to Video</a></td></tr><tr><td><a href="video-models/bytedance/seedance-1.0-lite-image-to-video">bytedance/seedance-1-0-lite-i2v</a></td><td>ByteDance</td><td></td><td><a href="https://aimlapi.com/models/seedance-1-0-lite-image-to-video">Seedance 1.0 lite Image to Video</a></td></tr><tr><td><a href="video-models/bytedance/seedance-1.0-pro-text-to-video">bytedance/seedance-1-0-pro-t2v</a></td><td>ByteDance</td><td></td><td><a href="https://aimlapi.com/models/seedance-1-0-pro">Seedance 1.0 Pro</a></td></tr><tr><td><a href="video-models/bytedance/seedance-1.0-pro-image-to-video">bytedance/seedance-1-0-pro-i2v</a></td><td>ByteDance</td><td></td><td><a href="https://aimlapi.com/models/seedance-1-0-pro">Seedance 1.0 Pro</a></td></tr><tr><td><a href="video-models/bytedance/omnihuman">bytedance/omnihuman</a></td><td>ByteDance</td><td></td><td><a href="https://aimlapi.com/models/omnihuman">OmniHuman</a></td></tr><tr><td><a href="video-models/bytedance/omnihuman-1.5">bytedance/omnihuman/v1.5</a></td><td>ByteDance</td><td></td><td><a href="https://aimlapi.com/models/omnihuman-v1-5">OmniHuman v1.5</a></td></tr><tr><td><a href="video-models/google/veo2-text-to-video">veo2</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/veo-2-text-to-video-api">Veo2 Text-to-Video</a></td></tr><tr><td><a href="video-models/google/veo2-image-to-video">veo2/image-to-video</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/veo-2-image-to-video-api">Veo2 Image-to-Video</a></td></tr><tr><td><a href="video-models/google/veo3-text-to-video">google/veo3</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/veo-3">Veo 3</a></td></tr><tr><td><a href="video-models/google/veo-3-image-to-video">google/veo-3.0-i2v</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/veo-3-0-i2v">Veo 3 I2V</a></td></tr><tr><td><a href="video-models/google/veo-3-fast-text-to-video">google/veo-3.0-fast</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/veo-3-0-fast">Veo 3 Fast</a></td></tr><tr><td><a href="video-models/google/veo-3-fast-image-to-video">google/veo-3.0-i2v-fast</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/veo-3-i2v-fast">Veo 3 I2V Fast</a></td></tr><tr><td><a href="video-models/kling-ai/v1-standard-image-to-video">kling-video/v1/standard/image-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-ai-image-to-video">Kling AI (image-to-video)</a></td></tr><tr><td><a href="video-models/kling-ai/v1-standard-text-to-video">kling-video/v1/standard/text-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-ai-text-to-video-api">Kling AI (text-to-video)</a></td></tr><tr><td><a href="video-models/kling-ai/v1-pro-image-to-video">kling-video/v1/pro/image-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-ai-image-to-video">Kling AI (image-to-video)</a></td></tr><tr><td><a href="video-models/kling-ai/v1-pro-text-to-video">kling-video/v1/pro/text-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-ai-text-to-video-api">Kling AI (text-to-video)</a></td></tr><tr><td><a href="video-models/kling-ai/v1.6-standard-text-to-video">kling-video/v1.6/standard/text-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-1-6-standard">Kling 1.6 Standard</a></td></tr><tr><td><a href="video-models/kling-ai/v1.6-standart-image-to-video">kling-video/v1.6/standard/image-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-1-6-standard">Kling 1.6 Standard</a></td></tr><tr><td><a href="video-models/kling-ai/v1.6-pro-image-to-video">kling-video/v1.6/pro/image-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-1-6-pro-api">Kling 1.6 Pro</a></td></tr><tr><td><a href="video-models/kling-ai/v1.6-pro-text-to-video">kling-video/v1.6/pro/text-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-1-6-pro-api">Kling 1.6 Pro</a></td></tr><tr><td><a href="video-models/kling-ai/v1.6-pro-effects">klingai/kling-video-v1.6-pro-effects</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-1-6-pro-effects">Kling 1.6 Pro Effects</a></td></tr><tr><td><a href="video-models/kling-ai/v1.6-standard-effects">klingai/kling-video-v1.6-standard-effects</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-video-v1-6-standard-effects">Kling 1.6 Standard Effects</a></td></tr><tr><td><a href="video-models/kling-ai/v1.6-standard-multi-image-to-video">kling-video/v1.6/standard/multi-image-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-v1-6-multi-image-to-video">Kling V1.6 Multi-Image-to-Video</a></td></tr><tr><td><a href="video-models/kling-ai/v2-master-image-to-video">klingai/v2-master-image-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-2-0-master">Kling 2.0 Master</a></td></tr><tr><td><a href="video-models/kling-ai/v2-master-text-to-video">klingai/v2-master-text-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-2-0-master">Kling 2.0 Master</a></td></tr><tr><td><a href="video-models/kling-ai/v2.1-standard-image-to-video">kling-video/v2.1/standard/image-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-v2-1-standard-i2v">Kling V2.1 Standard I2V</a></td></tr><tr><td><a href="video-models/kling-ai/v2.1-pro-image-to-video">kling-video/v2.1/pro/image-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-v2-1-pro-i2v">Kling V2.1 Pro I2V</a></td></tr><tr><td><a href="video-models/kling-ai/v2.1-master-image-to-video">klingai/v2.1-master-image-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-2-1">ling 2.1 Master</a></td></tr><tr><td><a href="video-models/kling-ai/v2.1-master-text-to-video">klingai/v2.1-master-text-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-2-1">Kling 2.1 Master</a></td></tr><tr><td><a href="video-models/kling-ai/v2.5-turbo-pro-image-to-video">klingai/v2.5-turbo/pro/image-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-video-v2-5-turbo-pro-i2v">Kling Video v2.5 Turbo Pro Image-to-Video</a></td></tr><tr><td><a href="video-models/kling-ai/v2.5-turbo-pro-text-to-video">klingai/v2.5-turbo/pro/text-to-video</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-video-v2-5-turbo-pro-t2v">Kling Video v2.5 Turbo Pro Text-to-Video</a></td></tr><tr><td><a href="video-models/kling-ai/avatar-standard">klingai/avatar-standard</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-ai-avatar-standard">Kling AI Avatar Standard</a></td></tr><tr><td><a href="video-models/kling-ai/avatar-pro">klingai/avatar-pro</a></td><td>Kling AI</td><td></td><td><a href="https://aimlapi.com/models/kling-ai-avatar-pro">Kling AI Avatar Pro</a></td></tr><tr><td><a href="video-models/krea/krea-wan-14b-text-to-video">krea/krea-wan-14b/text-to-video</a></td><td>Krea</td><td></td><td><a href="https://aimlapi.com/models/krea-wan-14b-text-to-video">Krea WAN 14B Text-to-Video</a></td></tr><tr><td><a href="video-models/krea/krea-wan-14b-video-to-video">krea/krea-wan-14b/video-to-video</a></td><td>Krea</td><td></td><td><a href="https://aimlapi.com/models/krea-wan-14b-video-to-video">Krea WAN 14B Video-to-Video</a></td></tr><tr><td><a href="video-models/minimax/video-01">video-01</a></td><td>Minimax</td><td></td><td><a href="https://aimlapi.com/models/minimax-video-01-api">MiniMax Video-01</a></td></tr><tr><td><a href="video-models/luma-ai/luma-ai-v2">luma/ray-1.6</a></td><td>Luma AI</td><td></td><td><a href="https://aimlapi.com/models/ray-1-6">Ray 1.6</a></td></tr><tr><td><a href="video-models/luma-ai/luma-ray-2">luma/ray-2</a></td><td>Luma AI</td><td></td><td><a href="https://aimlapi.com/models/ray-2">Ray 2</a></td></tr><tr><td><a href="video-models/luma-ai/luma-ray-flash-2">luma/ray-flash-2</a></td><td>Luma AI</td><td></td><td><a href="https://aimlapi.com/models/ray-flash-2">Ray Flash 2</a></td></tr><tr><td><a href="video-models/minimax/video-01-live2d">video-01-live2d</a></td><td>Minimax</td><td></td><td>-</td></tr><tr><td><a href="video-models/minimax/hailuo-02">minimax/hailuo-02</a></td><td>Minimax</td><td></td><td><a href="https://aimlapi.com/models/hailuo-02">Hailuo 02</a></td></tr><tr><td><a href="video-models/pixverse/v5-text-to-video">pixverse/v5/text-to-video</a></td><td>PixVerse</td><td></td><td><a href="https://aimlapi.com/models/pixverse-v5-t2v">Pixverse v5 Text-to-Video</a></td></tr><tr><td><a href="video-models/pixverse/v5-image-to-video">pixverse/v5/image-to-video</a></td><td>PixVerse</td><td></td><td><a href="https://aimlapi.com/models/pixverse-v5-i2v">Pixverse v5 Image-to-Video</a></td></tr><tr><td><a href="video-models/pixverse/v5-transition">pixverse/v5/transition</a></td><td>PixVerse</td><td></td><td><a href="https://aimlapi.com/models/pixverse-v5-transition">Pixverse v5 Transition</a></td></tr><tr><td><a href="video-models/runway/gen3a_turbo">gen3a_turbo</a></td><td>Runway</td><td></td><td><a href="https://aimlapi.com/models/runway-gen-3-turbo">Runway Gen-3 turbo</a></td></tr><tr><td><a href="video-models/runway/gen4_turbo">runway/gen4_turbo</a></td><td>Runway</td><td></td><td><a href="https://aimlapi.com/models/runway-gen-4-turbo-api">Runway Gen-4 Turbo</a></td></tr><tr><td><a href="video-models/runway/gen4_aleph">runway/gen4_aleph</a></td><td>Runway</td><td></td><td><a href="https://aimlapi.com/models/aleph">Aleph</a></td></tr><tr><td><a href="video-models/runway/act_two">runway/act_two</a></td><td>Runway</td><td></td><td><a href="https://aimlapi.com/models/runway-act-two">Runway Act Two</a></td></tr><tr><td><a href="video-models/sber-ai/kandinsky5-text-to-video">sber-ai/kandinsky5-t2v</a></td><td>Sber AI</td><td></td><td><a href="https://aimlapi.com/models/kandinsky-5-standard">Kandinsky 5 Standard</a></td></tr><tr><td><a href="video-models/sber-ai/kandinsky5-distill-text-to-video">sber-ai/kandinsky5-distill-t2v</a></td><td>Sber AI</td><td></td><td><a href="https://aimlapi.com/models/kandinsky-5-distill">Kandinsky 5 Distill</a></td></tr><tr><td><a href="video-models/veed/fabric-1.0">veed/fabric-1.0</a></td><td>Veed</td><td></td><td><a href="video-models/veed/fabric-1.0">fabric-1.0</a></td></tr><tr><td><a href="video-models/veed/fabric-1.0-fast">veed/fabric-1.0-fast</a></td><td>Veed</td><td></td><td><a href="video-models/veed/fabric-1.0-fast">fabric-1.0-fast</a></td></tr></tbody></table>

### Voice/Speech Models

#### Speech-to-Text

<table data-full-width="true"><thead><tr><th width="266.20001220703125">Model ID + API Reference link</th><th width="132.79998779296875">Developer</th><th width="103.5999755859375">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a href="speech-models/speech-to-text/assembly-ai/slam-1">aai/slam-1</a></td><td>Assembly AI</td><td></td><td><a href="https://aimlapi.com/models/slam-1">Slam 1</a></td></tr><tr><td><a href="speech-models/speech-to-text/assembly-ai/universal">aai/universal</a></td><td>Assembly AI</td><td></td><td><a href="https://aimlapi.com/models/universal">Universal</a></td></tr><tr><td><a href="speech-models/speech-to-text/deepgram/nova-2">#g1_nova-2-automotive</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/deepgram-nova-2">Deepgram Nova-2</a></td></tr><tr><td><a href="speech-models/speech-to-text/deepgram/nova-2">#g1_nova-2-conversationalai</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/deepgram-nova-2">Deepgram Nova-2</a></td></tr><tr><td><a href="speech-models/speech-to-text/deepgram/nova-2">#g1_nova-2-drivethru</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/deepgram-nova-2">Deepgram Nova-2</a></td></tr><tr><td><a href="speech-models/speech-to-text/deepgram/nova-2">#g1_nova-2-finance</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/deepgram-nova-2">Deepgram Nova-2</a></td></tr><tr><td><a href="speech-models/speech-to-text/deepgram/nova-2">#g1_nova-2-general</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/deepgram-nova-2">Deepgram Nova-2</a></td></tr><tr><td><a href="speech-models/speech-to-text/deepgram/nova-2">#g1_nova-2-medical</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/deepgram-nova-2">Deepgram Nova-2</a></td></tr><tr><td><a href="speech-models/speech-to-text/deepgram/nova-2">#g1_nova-2-meeting</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/deepgram-nova-2">Deepgram Nova-2</a></td></tr><tr><td><a href="speech-models/speech-to-text/deepgram/nova-2">#g1_nova-2-phonecall</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/deepgram-nova-2">Deepgram Nova-2</a></td></tr><tr><td><a href="speech-models/speech-to-text/deepgram/nova-2">#g1_nova-2-video</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/deepgram-nova-2">Deepgram Nova-2</a></td></tr><tr><td><a href="speech-models/speech-to-text/deepgram/nova-2">#g1_nova-2-voicemail</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/deepgram-nova-2">Deepgram Nova-2</a></td></tr><tr><td><a href="speech-models/speech-to-text/openai/whisper-tiny">#g1_whisper-tiny</a></td><td>OpenAI</td><td></td><td>-</td></tr><tr><td><a href="speech-models/speech-to-text/openai/whisper-small">#g1_whisper-small</a></td><td>OpenAI</td><td></td><td>-</td></tr><tr><td><a href="speech-models/speech-to-text/openai/whisper-base">#g1_whisper-base</a></td><td>OpenAI</td><td></td><td>-</td></tr><tr><td><a href="speech-models/speech-to-text/openai/whisper-medium">#g1_whisper-medium</a></td><td>OpenAI</td><td></td><td>-</td></tr><tr><td><a href="speech-models/speech-to-text/openai/whisper-large">#g1_whisper-large</a></td><td>OpenAI</td><td></td><td><a href="https://aimlapi.com/models/whisper">Whisper</a></td></tr></tbody></table>

#### Text-to-Speech

<table data-full-width="true"><thead><tr><th width="284.4000244140625">Model ID</th><th width="132.79998779296875">Developer</th><th width="112">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a href="speech-models/text-to-speech/alibaba-cloud/qwen3-tts-flash">alibaba/qwen3-tts-flash</a></td><td>Alibaba Cloud</td><td></td><td><a href="https://aimlapi.com/models/qwen3-tts-flash">Qwen3-TTS-Flash</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura">#g1_aura-angus-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura">Aura</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura">#g1_aura-arcas-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura">Aura</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura">#g1_aura-asteria-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura">Aura</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura">#g1_aura-athena-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura">Aura</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura">#g1_aura-helios-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura">Aura</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura">#g1_aura-hera-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura">Aura</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura">#g1_aura-luna-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura">Aura</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura">#g1_aura-orion-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura">Aura</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura">#g1_aura-orpheus-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura">Aura</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura">#g1_aura-perseus-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura">Aura</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura">#g1_aura-stella-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura">Aura</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura">#g1_aura-zeus-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura">Aura</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-amalthea-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-andromeda-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-apollo-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-arcas-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-aries-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-asteria-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-athena-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-atlas-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-aurora-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-callista-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-cora-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-cordelia-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-delia-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-draco-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-electra-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-harmonia-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-helena-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-hera-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-hermes-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-hyperion-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-iris-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-janus-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-juno-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-jupiter-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-luna-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-mars-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-minerva-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-neptune-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-odysseus-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-ophelia-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-orion-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-orpheus-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-pandora-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-phoebe-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-pluto-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-saturn-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-selene-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-thalia-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-theia-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-vesta-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-zeus-en</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-celeste-es</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-estrella-es</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/deepgram/aura-2">#g1_aura-2-nestor-es</a></td><td>Deepgram</td><td></td><td><a href="https://aimlapi.com/models/aura-2">Aura 2</a></td></tr><tr><td><a href="speech-models/text-to-speech/elevenlabs/eleven_multilingual_v2">elevenlabs/eleven_multilingual_v2</a></td><td>ElevenLabs</td><td></td><td><a href="https://aimlapi.com/models/eleven-multilingual-v2">ElevenLabs Multilingual v2</a></td></tr><tr><td><a href="speech-models/text-to-speech/elevenlabs/eleven_turbo_v2_5">elevenlabs/eleven_turbo_v2_5</a></td><td>ElevenLabs</td><td></td><td><a href="https://aimlapi.com/models/eleven-turbo-v2-5">ElevenLabs Turbo v2.5</a></td></tr><tr><td><a href="speech-models/text-to-speech/microsoft/vibevoice-1.5b">microsoft/vibevoice-1.5b</a></td><td>Microsoft</td><td></td><td><a href="https://aimlapi.com/models/vibevoice-1-5b">VibeVoice 1.5B</a></td></tr><tr><td><a href="speech-models/text-to-speech/microsoft/vibevoice-7b">microsoft/vibevoice-7b</a></td><td>Microsoft</td><td></td><td><a href="https://aimlapi.com/models/vibevoice-7b">VibeVoice 7B</a></td></tr><tr><td><a href="speech-models/text-to-speech/openai/tts-1">openai/tts-1</a></td><td>OpenAI</td><td></td><td><a href="https://aimlapi.com/models/tts-1">TTS-1</a></td></tr><tr><td><a href="speech-models/text-to-speech/openai/tts-1-hd">openai/tts-1-hd</a></td><td>OpenAI</td><td></td><td><a href="https://aimlapi.com/models/tts-1-hd">TTS-1 HD</a></td></tr><tr><td><a href="speech-models/text-to-speech/openai/gpt-4o-mini-tts">openai/gpt-4o-mini-tts</a></td><td>OpenAI</td><td></td><td><a href="https://aimlapi.com/models/gpt-4o-mini-tts">GPT-4o-mini-TTS</a></td></tr></tbody></table>

#### Voice Chat

<table data-full-width="true"><thead><tr><th width="284.4000244140625">Model ID</th><th width="132.79998779296875">Developer</th><th width="105.5999755859375">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a href="speech-models/voice-chat/elevenlabs/v3_alpha">elevenlabs/v3_alpha</a></td><td>ElevenLabs</td><td></td><td><a href="https://aimlapi.com/models/eleven-v3-alpha">Eleven v3 Alpha</a></td></tr><tr><td><a href="speech-models/voice-chat/minimax/speech-2.5-turbo-preview">minimax/speech-2.5-turbo-preview</a></td><td>MiniMax</td><td></td><td><a href="https://aimlapi.com/models/minimax-speech-2-5-turbo">MiniMax Speech 2.5 Turbo</a></td></tr><tr><td><a href="speech-models/voice-chat/minimax/speech-2.5-hd-preview">minimax/speech-2.5-hd-preview</a></td><td>MiniMax</td><td></td><td><a href="https://aimlapi.com/models/minimax-speech-2-5-hd">MiniMax Speech 2.5 HD</a></td></tr><tr><td><a href="speech-models/voice-chat/minimax/speech-2.6-turbo">minimax/speech-2.6-turbo</a></td><td>MiniMax</td><td></td><td><a href="https://aimlapi.com/models/minimax-speech-2-6-turbo">MiniMax Speech 2.6 Turbo</a></td></tr><tr><td><a href="speech-models/voice-chat/minimax/speech-2.6-hd">minimax/speech-2.6-hd</a></td><td>MiniMax</td><td></td><td><a href="https://aimlapi.com/models/minimax-speech-2-6-hd">MiniMax Speech 2.6 HD</a></td></tr></tbody></table>

### Music Models

<table data-full-width="true"><thead><tr><th width="266.20001220703125">Model ID</th><th width="134.4000244140625">Developer</th><th width="102.60009765625">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a href="music-models/elevenlabs/eleven_music">elevenlabs/eleven_music</a></td><td>ElevenLabs</td><td></td><td><a href="https://aimlapi.com/models/elevenmusic">Eleven Music</a></td></tr><tr><td><a href="music-models/google/lyria-2">google/lyria2</a></td><td>Google</td><td></td><td><a href="https://aimlapi.com/models/lyria-2">Lyria 2</a></td></tr><tr><td><a href="music-models/stability-ai/stable-audio">stable-audio</a></td><td>Stability AI</td><td></td><td><a href="https://aimlapi.com/models/stable-audio">Stable Audio</a></td></tr><tr><td><a href="music-models/minimax/minimax-music-legacy">minimax-music</a></td><td>Minimax AI</td><td></td><td>-</td></tr><tr><td><a href="music-models/minimax/music-01">music-01</a></td><td>Minimax AI</td><td></td><td><a href="https://aimlapi.com/models/minimax-music-api">MiniMax Music</a></td></tr><tr><td><a href="music-models/minimax/music-1.5">minimax/music-1.5</a></td><td>Minimax AI</td><td></td><td><em>Coming Soon</em></td></tr></tbody></table>

### Content Moderation Models

<table data-full-width="true"><thead><tr><th width="266.20001220703125">Model ID + API Reference link</th><th width="132.79998779296875">Developer</th><th width="103.5999755859375">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a href="moderation-safety-models/meta/llama-guard-3-11b-vision-turbo">meta-llama/Llama-Guard-3-11B-Vision-Turbo</a></td><td>Meta</td><td>128,000</td><td>-</td></tr><tr><td><a href="moderation-safety-models/meta/llamaguard-2-8b">meta-llama/LlamaGuard-2-8b</a></td><td>Meta</td><td>8,000</td><td><a href="https://aimlapi.com/models/llama-guard-2-8b">LlamaGuard 2 (8b)</a></td></tr><tr><td><a href="moderation-safety-models/meta/meta-llama-guard-3-8b">meta-llama/Meta-Llama-Guard-3-8B</a></td><td>Meta</td><td>8,000</td><td><a href="https://aimlapi.com/models/llama-guard-3-8b">Llama Guard 3 (8B)</a></td></tr></tbody></table>

### Vision Models

#### Optical Character Recognition (OCR)

<table data-full-width="true"><thead><tr><th width="266.20001220703125">Model ID + API Reference link</th><th width="132.79998779296875">Developer</th><th width="103.5999755859375">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a href="vision-models/ocr-optical-character-recognition/google/google-ocr"><em>The service has no Model ID</em></a></td><td>Google</td><td></td><td>-</td></tr><tr><td><a href="vision-models/ocr-optical-character-recognition/mistral-ai/mistral-ocr-latest">mistral/mistral-ocr-latest</a></td><td>Mistral AI</td><td></td><td>-</td></tr></tbody></table>

### 3D-Generating Models

<table data-full-width="true"><thead><tr><th width="265.4000244140625">Model ID + API Reference link</th><th width="134.39996337890625">Developer</th><th width="104">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a href="3d-generating-models/stability-ai/triposr">triposr</a></td><td>Tripo AI</td><td></td><td><a href="https://aimlapi.com/models/stable-tripo-sr-api">Stable TripoSR 3D</a></td></tr></tbody></table>

### Embedding Models

<table data-full-width="true"><thead><tr><th width="274.20001220703125">Model ID + API Reference link</th><th width="124.20001220703125">Developer</th><th width="103.60009765625">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a href="embedding-models/openai/text-embedding-3-small">text-embedding-3-small</a></td><td>Open AI</td><td>8,000</td><td>-</td></tr><tr><td><a href="embedding-models/openai/text-embedding-3-large">text-embedding-3-large</a></td><td>Open AI</td><td>8,000</td><td><a href="https://aimlapi.com/models/text-embedding-3-large">Text-embedding-3-large</a></td></tr><tr><td><a href="embedding-models/openai/text-embedding-ada-002">text-embedding-ada-002</a></td><td>Open AI</td><td>8,000</td><td><a href="https://aimlapi.com/models/text-embedding-ada-002">Text-embedding-ada-002</a></td></tr><tr><td><a href="embedding-models/together-ai/m2-bert-80m-retrieval">togethercomputer/m2-bert-80M-32k-retrieval</a></td><td>Together AI</td><td>32,000</td><td><a href="https://aimlapi.com/models/m2-bert-retrieval-32k">M2-BERT-Retrieval-32k</a></td></tr><tr><td><a href="embedding-models/baai/bge-base-en">BAAI/bge-base-en-v1.5</a></td><td>BAAI</td><td></td><td><a href="https://aimlapi.com/models/baai-bge-base-1p5">BAAI-Bge-Base-1p5</a></td></tr><tr><td><a href="embedding-models/baai/bge-large-en">BAAI/bge-large-en-v1.5</a></td><td>BAAI</td><td></td><td><a href="https://aimlapi.com/models/baai-bge-large-1p5">bge-large-en</a></td></tr><tr><td><a href="embedding-models/anthropic/voyage-large-2-instruct">voyage-large-2-instruct</a></td><td>Anthropic</td><td>16,000</td><td><a href="https://aimlapi.com/models/voyage-large-2-instruct-api">Voyage Large 2 Instruct</a></td></tr><tr><td><a href="embedding-models/anthropic/voyage-finance-2">voyage-finance-2</a></td><td>Anthropic</td><td>32,000</td><td>-</td></tr><tr><td><a href="embedding-models/anthropic/voyage-multilingual-2">voyage-multilingual-2</a></td><td>Anthropic</td><td>32,000</td><td>-</td></tr><tr><td><a href="embedding-models/anthropic/voyage-law-2">voyage-law-2</a></td><td>Anthropic</td><td>16,000</td><td>-</td></tr><tr><td><a href="embedding-models/anthropic/voyage-code-2">voyage-code-2</a></td><td>Anthropic</td><td>16,000</td><td>-</td></tr><tr><td><a href="embedding-models/anthropic/voyage-large-2">voyage-large-2</a></td><td>Anthropic</td><td>16,000</td><td>-</td></tr><tr><td><a href="embedding-models/anthropic/voyage-2">voyage-2</a></td><td>Anthropic</td><td>4,000</td><td>-</td></tr><tr><td><a href="embedding-models/google/textembedding-gecko">textembedding-gecko@003</a></td><td>Google</td><td>2,000</td><td><a href="https://aimlapi.com/models/textembedding-gecko-003-api">Textembedding-gecko@003</a></td></tr><tr><td><a href="embedding-models/google/textembedding-gecko">textembedding-gecko-multilingual@001</a></td><td>Google</td><td>2,000</td><td><a href="https://aimlapi.com/models/textembedding-gecko-multilingual-001-api">Textembedding-gecko-multilingual@001</a></td></tr><tr><td><a href="embedding-models/google/text-multilingual-embedding-002">text-multilingual-embedding-002</a></td><td>Google</td><td>2,000</td><td>-</td></tr></tbody></table>

***

### <img src="https://3927338786-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FROMd1X5PuqtikJ48n2N9%2Fuploads%2Fgit-blob-bd52cb24a7111f901f5b2c34cbc9be2750893356%2Foffline-label.png?alt=media" alt="" data-size="line"> Deprecated / No Longer Supported Models

{% hint style="danger" %}
These models are no longer available for API or Playground calls.\
Their description and API reference pages have also been removed from this documentation portal.
{% endhint %}

<table data-full-width="true"><thead><tr><th width="316">Model ID</th><th width="135">Developer</th><th width="102">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a data-footnote-ref href="#user-content-fn-1">Qwen/Qwen2-72B-Instruct</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-2-instruct-72b">Qwen 2 Instruct (72B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">claude-3-5-sonnet-20240620</a></td><td>Anthropic</td><td>200,000</td><td>-</td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">claude-3-5-sonnet-20241022</a></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-3-5-sonnet">Claude 3.5 Sonnet 20241022</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">cohere/command-r-plus</a></td><td>Cohere</td><td>128,000</td><td><a href="https://aimlapi.com/models/command-r-api">Command R+</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">google/gemma-2-27b-it</a></td><td>Google</td><td>8,000</td><td><a href="https://aimlapi.com/models/gemma-2-27b">Gemma 2 (27b)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">NousResearch/Nous-Hermes-2</a>-<a data-footnote-ref href="#user-content-fn-1">Mixtral-8x7B-DPO</a></td><td>Nous Research</td><td>32,000</td><td>-</td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">nvidia/Llama-3.1-Nemotron-70B</a>-<a data-footnote-ref href="#user-content-fn-1">Instruct-HF</a></td><td>Nvidia</td><td>128,000</td><td><a href="https://aimlapi.com/models/llama-3-1-nemotron-70b-instruct-api">Llama 3.1 Nemotron 70B Instruct</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">meta-llama/Llama-3-8b-chat-hf</a></td><td>Meta</td><td>8,000</td><td><a href="https://aimlapi.com/models/llama-3-8b-instruct-reference-api">Llama 3 8B Instruct Reference</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">meta-llama/Llama-3.2-90B-Vision</a>-<a data-footnote-ref href="#user-content-fn-1">Instruct-Turbo</a></td><td>Meta</td><td>131,000</td><td><a href="https://aimlapi.com/models/llama-3-2-90b-vision-instruct-turbo-api">Llama 3.2 90B Vision Instruct Turbo</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">meta-llama/Llama-Vision-Free</a></td><td>Meta</td><td>128,000</td><td>-</td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">meta-llama/Llama-3.2-11B-Vision</a>-<a data-footnote-ref href="#user-content-fn-1">Instruct-Turbo</a></td><td>Meta</td><td>131,000</td><td><a href="https://aimlapi.com/models/llama-3-2-11b-vision-instruct-turbo-api">Llama 3.2 11B Vision Instruct Turbo</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">abab6.5s-chat</a></td><td>MiniMax</td><td>245,000</td><td>-</td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">openrouter/horizon-beta</a></td><td>OpenRouter</td><td>256,000</td><td>-</td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">openrouter/horizon-alpha</a></td><td>OpenRouter</td><td>256,000</td><td>-</td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">wan/v2.1/1.3b/text-to-video</a></td><td>Alibaba Cloud</td><td>-</td><td><a href="https://aimlapi.com/models/wan-2-1-api">Wan 2.1</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">o1-preview</a>,<br><a data-footnote-ref href="#user-content-fn-1">o1-preview-2024-09-12</a></td><td>OpenAI</td><td>128,000</td><td><a href="https://aimlapi.com/models/openai-o1-preview-api">OpenAI o1-preview</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">claude-3-sonnet-20240229</a>,<br><a data-footnote-ref href="#user-content-fn-1">anthropic/claude-3-sonnet</a>,<br><a data-footnote-ref href="#user-content-fn-1">claude-3-sonnet-latest</a></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-3-sonnet">Claude 3 Sonnet</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">google/gemini-2.5-pro-preview</a>,<br><a data-footnote-ref href="#user-content-fn-1">google/gemini-2.5-pro-preview-05-06</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-pro-2-5-preview-api">Gemini Pro 2.5 Preview</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">google/gemini-2.5-flash-preview</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-2-5-flash-preview-api">Gemini 2.5 Flash Preview</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">neversleep/llama-3.1-lumimaid-70b</a></td><td>NeverSleep</td><td>8,000</td><td><a href="https://aimlapi.com/models/llama-3-1-lumimaid-70b-api">Llama 3.1 Lumimaid 70b</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">x-ai/grok-beta</a></td><td>xAI</td><td>131,000</td><td><a href="https://aimlapi.com/models/grok-2-beta-api">Grok-2 Beta</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">gpt-4.5-preview</a></td><td>OpenAI</td><td>128,000</td><td><a href="https://aimlapi.com/models/chat-gpt-4-5-preview-api">Chat GPT 4.5 preview</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">gemini-1.5-flash</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-1-5-flash-api">Gemini 1.5 Flash</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">gemini-1.5-pro</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-1-5-pro-api">Gemini 1.5 Pro</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">google/gemma-3-1b-it</a></td><td>Google</td><td>128,000</td><td><a href="https://aimlapi.com/models/gemma-3-1b-api">Gemma 3 (1B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">togethercomputer/m2-bert-80M</a>-<a data-footnote-ref href="#user-content-fn-1">8k-retrieval</a></td><td>TogetherAI</td><td>8,000</td><td><a href="https://aimlapi.com/models/m2-bert-retrieval-8k">M2-BERT-Retrieval-8k</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">togethercomputer/m2-bert-80M-</a><a data-footnote-ref href="#user-content-fn-1">2k-retrieval</a></td><td>TogetherAI</td><td>2,000</td><td><a href="https://aimlapi.com/models/m2-bert-retrieval-2k">M2-BERT-Retrieval-2K</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Gryphe/MythoMax-L2-13b-Lite</a></td><td>Gryphe</td><td>4,000</td><td>-</td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">mistralai/Mixtral-8x22B-Instruct-v0.1</a></td><td>Mistral AI</td><td>64,000</td><td><a href="https://aimlapi.com/models/mixtral-8x22b-instruct">Mixtral 8x22B Instruct</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">google/gemini-2.5-pro-exp-03-25</a></td><td>Google</td><td>1,000,000</td><td>-</td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">google/gemini-2.0-flash-thinking</a>-<a data-footnote-ref href="#user-content-fn-1">exp-01</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-2-0-flash-thinking-experimental-api">Gemini 2.0 Flash Thinking Experimental</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">ai21/jamba-1-5-mini</a></td><td>AI21 Labs</td><td>256,000</td><td><a href="https://aimlapi.com/models/jamba-1-5-mini-api">Jamba 1.5 Mini</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">textembedding-gecko@001</a></td><td>Google</td><td>3,000</td><td>-</td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">google/gemini-pro</a> or <a data-footnote-ref href="#user-content-fn-1">gemini-pro</a></td><td>Google</td><td>32,000</td><td><a href="https://aimlapi.com/models/gemini-1-0-pro-api">Gemini 1.0 Pro</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">meta-llama/Meta-Llama-3.1-8B</a>-<a data-footnote-ref href="#user-content-fn-1">Instruct-Turbo-128K</a></td><td>Meta</td><td>128,000</td><td>-</td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">stabilityai/stable-diffusion-xl-base-1.0</a></td><td>Stability AI</td><td></td><td><a href="https://aimlapi.com/models/stable-diffusion-xl-1-0">Stable Diffusion XL 1.0</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">upstage/solar-10.7b-instruct-v1.0</a></td><td>Upstage</td><td>4,000</td><td><a href="https://aimlapi.com/models/upstage-solar-instruct-v1-11b">Upstage SOLAR Instruct v1 (11B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">meta-llama/Llama-2-13b-chat-hf</a></td><td>Meta</td><td>4,100</td><td><a href="https://aimlapi.com/models/llama-2-chat-13b">LLaMA-2 Chat (13B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">meta-llama/meta-llama-3-70b</a>-<a data-footnote-ref href="#user-content-fn-1">instruct-turbo</a></td><td>Meta</td><td>128,000</td><td>-</td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">google/gemma-2-9b-it</a></td><td>Google</td><td>8,000</td><td><a href="https://aimlapi.com/models/gemma-2-9b">Gemma 2 (9B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">google/gemma-2b-it</a></td><td>Google</td><td>8,000</td><td><a href="https://aimlapi.com/models/gemma-instruct-2b">Gemma Instruct (2B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Gryphe/MythoMax-L2-13b</a></td><td>Gryphe</td><td>4,000</td><td><a href="https://aimlapi.com/models/mythomax-l2-13b">MythoMax-L2 (13B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">microsoft/WizardLM-2-8x22B</a></td><td>Microsoft</td><td>64,000</td><td><a href="https://aimlapi.com/models/wizardlm-2-8-22b">WizardLM 2-8 (22B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Austism/chronos-hermes-13b</a></td><td>Austism</td><td>2,000</td><td><a href="https://aimlapi.com/models/chronos-hermes-13b">Chronos Hermes 13b</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">databricks/dbrx-instruct</a></td><td>Databricks</td><td>32,000</td><td><a href="https://aimlapi.com/models/dbrx-instruct">DBRX Instruct</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">deepseek-ai/deepseek-llm-67b-chat</a></td><td>DeepSeek</td><td>4,000</td><td><a href="https://aimlapi.com/models/deepseek-llm-67b-chat">Deepseek-LLM-67b-Chat</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">deepseek-ai/deepseek-coder-33b</a>-<a data-footnote-ref href="#user-content-fn-1">instruct</a></td><td>DeepSeek</td><td>16,000</td><td><a href="https://aimlapi.com/models/deepseek-coder-instruct-33b">Deepseek Coder Instruct (33B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Meta-Llama/Llama-2-7b-chat-hf</a></td><td>Meta</td><td>4,000</td><td><a href="https://aimlapi.com/models/llama-2-chat-7b">LLaMA-2 Chat (7B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Meta-Llama/Meta-Llama-3-70B</a>-<a data-footnote-ref href="#user-content-fn-1">Instruct-Lite</a></td><td>Meta</td><td>8,000</td><td><a href="https://aimlapi.com/models/llama-3-70b-instruct-lite-api">Llama 3 70B Instruct Lite</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Meta-Llama/Llama-Guard-7b</a></td><td>Meta</td><td>4,000</td><td><a href="https://aimlapi.com/models/llama-guard-7b">Llama Guard (7B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">meta-llama/Llama-2-7b-hf</a></td><td>Meta</td><td>4,000</td><td><a href="https://aimlapi.com/models/llama">LLaMA-2 (7B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">meta-llama/Llama-3-8b-hf</a></td><td>Meta</td><td>8,000</td><td><a href="https://aimlapi.com/models/llama-3-8b">Llama-3 (8B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">codellama/CodeLlama-70b-hf</a></td><td>Meta</td><td>16,000</td><td><a href="https://aimlapi.com/models/code-llama-70b">Code Llama (70B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">codellama/CodeLlama-7b-Instruct-hf</a></td><td>Meta</td><td>16,000</td><td><a href="https://aimlapi.com/models/code-llama-instruct-7b">Code Llama Instruct (7B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">codellama/CodeLlama-13b-Instruct-hf</a></td><td>Meta</td><td>16,000</td><td><a href="https://aimlapi.com/models/code-llama-instruct-13b">Code Llama Instruct (13B)</a></td></tr><tr><td>codellama/CodeLlama-70b-Instruct-hf</td><td>Meta</td><td>4,000</td><td><a href="https://aimlapi.com/models/code-llama-instruct-70b">Code Llama Instruct (70B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">codellama/CodeLlama-70b-Python-hf</a></td><td>Meta</td><td>4,000</td><td><a href="https://aimlapi.com/models/code-llama-python-70b">Code Llama Python (70B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">mistralai/Mixtral-8x22B-Instruct-v0.1</a></td><td>Mistral AI</td><td>64,000</td><td><a href="https://aimlapi.com/models/mixtral-8x22b-instruct">Mixtral 8x22B Instruct</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">gpt-3.5-turbo-16k-0613</a></td><td>OpenAI</td><td></td><td>-</td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">gpt-4-0613</a></td><td>OpenAI</td><td>128,000</td><td><a href="https://aimlapi.com/models/chat-gpt-4-turbo">Chat GPT 4 Turbo</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Qwen/Qwen-14B-Chat</a></td><td>Alibaba Cloud</td><td>8,000</td><td><a href="https://aimlapi.com/models/qwen-chat-14b">Qwen Chat (14B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Qwen/Qwen1.5-0.5B</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-1-5-0-5b">Qwen 1.5 (0.5B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Qwen/Qwen1.5-1.8B</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-1-5">Qwen 1.5 (1.8B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Qwen/Qwen1.5-4B</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-15-4b">Qwen 1.5 (4B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Qwen/Qwen1.5-1.8B-Chat</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-1-5-chat-1-8b">Qwen 1.5 Chat (1.8B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Qwen/Qwen1.5-4B-Chat</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-15-chat-4b">Qwen 1.5 Chat (4B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Qwen/Qwen1.5-7B-Chat</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-1-5-chat-7b">Qwen 1.5 Chat (7B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Qwen/Qwen1.5-14B-Chat</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-15-chat-14b">Qwen 1.5 Chat (14B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">qwen/qvq-72b-preview</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qvq-72b-preview-api">QVQ-72B-Preview</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">togethercomputer/guanaco-13b</a></td><td>Tim Dettmers</td><td>2,000</td><td><a href="https://aimlapi.com/models/guanaco-13b">Guanaco (13B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">togethercomputer/guanaco-33b</a></td><td>Tim Dettmers</td><td>2,000</td><td><a href="https://aimlapi.com/models/guanaco-33b">Guanaco (33B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">togethercomputer/guanaco-65b</a></td><td>Tim Dettmers</td><td>2,000</td><td><a href="https://aimlapi.com/models/guanaco-65b">Guanaco (65B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">togethercomputer/mpt-7b-chat</a></td><td>Mosaic ML</td><td>2,000</td><td><a href="https://aimlapi.com/models/mpt-chat-7b">MPT-Chat (7B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">togethercomputer/mpt-30b-chat</a></td><td>Mosaic ML</td><td>8,000</td><td><a href="https://aimlapi.com/models/mpt-chat-30b">MPT-Chat (30B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">togethercomputer/RedPajama-INCITE-7B-Instruct</a></td><td>RedPajama</td><td>2,000</td><td><a href="https://hidden.aimlapi.com/models/redpajama-incite-instruct-7b">RedPajama-INCITE Instruct (7B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">prompthero/openjourney</a></td><td>PromptHero</td><td>77</td><td><a href="https://aimlapi.com/models/openjourney-v4">Openjourney v4</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">wavymulder/Analog-Diffusion</a></td><td>wavymulder</td><td>77</td><td><a href="https://aimlapi.com/models/analog-diffusion">Analog Diffusion</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">-</a></td><td>01.AI</td><td>4,000</td><td><a href="https://aimlapi.com/models/01-ai-yi-base-6b">01-ai Yi Base (6B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">Undi95/Toppy-M-7B</a></td><td>Undi95</td><td>4,000</td><td><a href="https://aimlapi.com/models/toppy-m-7b">Toppy M (7B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">SG161222/Realistic_Vision_V3.0_VAE</a></td><td>Together</td><td>77</td><td><a href="https://aimlapi.com/models/realistic-vision-3-0">Realistic Vision 3.0</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">tiiuae/falcon-40b</a></td><td>TII</td><td>2,000</td><td><a href="https://aimlapi.com/models/falcon-40b">Falcon (40B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">allenai/OLMo-7B</a></td><td>Allen Institute for AI</td><td>2,000</td><td><a href="https://aimlapi.com/models/olmo-7b">OLMo-7B</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">bigcode/starcoder</a></td><td>BigCode</td><td>8,000</td><td><a href="https://aimlapi.com/models/starcoder">StarCoder (16B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">HuggingFaceH4/starchat-alpha</a></td><td>Hugging Face</td><td>8,000</td><td><a href="https://aimlapi.com/models/starcoderchat-alpha-16b">StarCoderChat Alpha (16B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">NousResearch/Nous-Hermes</a>-<a data-footnote-ref href="#user-content-fn-1">Llama2-70b</a></td><td>NousResearch</td><td>4,000</td><td><a href="https://aimlapi.com/models/nous-hermes-llama-2-70b">Nous Hermes LLaMA-2 (70B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">NousResearch/Nous-Hermes-2</a>-<a data-footnote-ref href="#user-content-fn-1">Mixtral-8x7B-SFT</a></td><td>NousResearch</td><td>32,000</td><td><a href="https://aimlapi.com/models/nous-hermes-2-mixtral-8x7b-sft">Nous Hermes 2 - Mixtral 8x7B-SFT</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">NousResearch/Nous-Hermes-2-</a><a data-footnote-ref href="#user-content-fn-1">Mistral-7B-DPO</a></td><td>NousResearch</td><td>32,000</td><td><a href="https://aimlapi.com/models/nous-hermes-2-mistral-dpo-7b">Nous Hermes 2 - Mistral DPO (7B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">NousResearch/Hermes-2-Theta-</a><a data-footnote-ref href="#user-content-fn-1">Llama-3-70B</a></td><td>NousResearch</td><td>8,000</td><td><a href="https://aimlapi.com/models/hermes-2-theta-llama-3-70b-api">Hermes 2 Theta Llama-3 70B</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">defog/sqlcoder</a></td><td>Defog AI</td><td>8,000</td><td><a href="https://aimlapi.com/models/sqlcoder">SQLCoder (15B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">replit/replit-code-v1-3b</a></td><td>Replit</td><td>2,000</td><td><a href="https://aimlapi.com/models/replit-code-v1-3b">Replit-Code-v1 (3B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">lmsys/vicuna-13b-v1.5</a></td><td>Imsys</td><td>4,000</td><td><a href="https://aimlapi.com/models/vicuna-v1-5-13b">Vicuna v1.5 (13B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">microsoft/phi-2</a></td><td>Microsoft</td><td>2,000</td><td><a href="https://aimlapi.com/models/microsoft-phi-2">Microsoft Phi-2</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">stabilityai/stablelm-base-alpha-3b</a></td><td>StabilityAI</td><td>4,000</td><td><a href="https://aimlapi.com/models/stablelm-base-alpha-3b">StableLM Base Alpha 3B</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">runwayml/stable-diffusion-v1-5</a></td><td>StabilityAI</td><td>77</td><td><a href="https://aimlapi.com/models/stable-diffusion">Stable Diffusion 1.5</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">stabilityai/stable-diffusion-2-1</a></td><td>StabilityAI</td><td>77</td><td><a href="https://aimlapi.com/models/stable-diffusion-21">Stable Diffusion 2.1</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">teknium/OpenHermes-2p5-Mistral-7B</a></td><td>Teknium</td><td>8,000</td><td><a href="https://aimlapi.com/models/openhermes-25-mistral-7b">OpenHermes-2.5-Mistral (7B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">openchat/openchat-3.5-1210</a></td><td>OpenChat</td><td>8,000</td><td><a href="https://aimlapi.com/models/openchat-3-5">OpenChat 3.5 (7B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">DiscoResearch/DiscoLM-mixtral-8x7b</a>-<a data-footnote-ref href="#user-content-fn-1">v2</a></td><td>Disco Research</td><td>32,000</td><td><a href="https://aimlapi.com/models/discolm-mixtral-8x7b-46-7b">DiscoLM Mixtral 8x7b (46.7B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">google/flan-t5-xl</a></td><td>Google</td><td>512</td><td><a href="https://aimlapi.com/models/flan-t5">FLAN T5 XL (3B)</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">garage-bAInd/Platypus2-70B-instruct</a></td><td>Garage-bAInd</td><td>4,000</td><td><a href="https://aimlapi.com/models/platypus2-70b-instruct">Platypus2-70B-Instruct</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">EleutherAI/gpt-neox-20b</a></td><td>EleutherAI</td><td>2,000</td><td><a href="https://aimlapi.com/models/gpt-neox-20b">GPT Neox 20B</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">gradientai/Llama-3-70B-Instruct</a>-<a data-footnote-ref href="#user-content-fn-1">Gradient-1048k</a></td><td>Gradient</td><td>1,048,000</td><td><a href="https://aimlapi.com/models/llama-3-70b-gradient-instruct-1048k-api">Llama-3 70B Gradient Instruct 1048k</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">WhereIsAI/UAE-Large-V1</a></td><td>WhereIsAI</td><td>512</td><td><a href="https://aimlapi.com/models/uae-large-v1">UAE-Large-V1</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">zero-one-ai/Yi-34B-Chat</a></td><td>01.AI</td><td>4,000</td><td><a href="https://aimlapi.com/models/yi-34b-chat">Yi-34B-Chat</a></td></tr><tr><td><a data-footnote-ref href="#user-content-fn-1">-</a></td><td>Suno AI</td><td>32</td><td><a href="https://aimlapi.com/models/suno-ai">Suno AI</a></td></tr></tbody></table>

[^1]: All the models in this table are no longer supported. You cannot call them.


# Text Models (LLM)

Overview of the capabilities of AIML API text models (LLMs).

<details>

<summary>Overview</summary>

The AI/ML API provides access to text-based models, also known as **Large Language Models** (**LLM**s), and allows you to interact with them through natural language (that's why a third common name for such models is **chat models**). These models can be applied to various tasks, enabling the creation of diverse applications using our API. For example, text models can be used to:

* Create a system that searches your photos using text prompts.
* Act as a psychological supporter.
* Play games with you through natural language.
* Assist you with coding.
* Perform a security assessment (pentests) on servers for vulnerabilities.
* Write documentation for your services.
* Serve as a grammar corrector for multiple languages with deep context understanding.
* And much more.

</details>

<details>

<summary>Specific Capabilities</summary>

There are several capabilities of text models that are worth mentioning separately.

**Completion** allows the model to analyze a given text fragment and predict how it might continue based on the probabilities of the next possible tokens or characters. **Chat Completion** extends this functionality, enabling a simulated dialogue between the user and the model based on predefined roles (e.g., "strict language teacher" and "student"). A detailed description and examples can be found in our [Completion and Chat Completion](https://docs.aimlapi.com/capabilities/completion-or-chat-models) article.

***

An evolution of chat completion includes **Assistants** (preconfigured conversational agents with specific roles) and **Threads** (a mechanism for maintaining conversation history for context). Examples of this functionality can be found in the [Managing Assistants & Threads](https://docs.aimlapi.com/solutions/openai/assistants) article.

***

**Function Calling** allows a chat model to invoke external programmatic tools (e.g., a function you have written) while generating a response. A detailed description and examples are available in the [Function Calling](https://docs.aimlapi.com/capabilities/function-calling) article.

</details>

<details>

<summary>Endpoint</summary>

All text and chat models use the same endpoint:

<img src="https://3927338786-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FROMd1X5PuqtikJ48n2N9%2Fuploads%2Fgit-blob-bd94564252ef38bacd71f9b402fb7df1378a86a7%2FPOST.png?alt=media" alt="" data-size="line"> `https://api.aimlapi.com/v1/chat/completions`

The parameters may vary (especially for models from different developers), so it‚Äôs best to check the API schema on each model‚Äôs page for details. Example: [**o4-mini**](https://docs.aimlapi.com/api-references/openai/o4-mini#api-schema).

</details>

<details>

<summary><span data-gb-custom-inline data-tag="emoji" data-code="2705">‚úÖ</span> Quick Code Example</summary>

We will call the [**gpt-4o**](https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4o) model using the Python programming language and the OpenAI SDK.

{% hint style="info" %}
If you need a more detailed explanation of how to call a model's API in code, check out our [<mark style="color:blue;">QUICKSTART</mark>](https://github.com/aimlapi/api-docs/blob/main/docs/api-references/text-models-llm/broken-reference/README.md) section.
{% endhint %}

{% code overflow="wrap" %}

```python
%pip install openai
import os
from openai import OpenAI

client = OpenAI(
    base_url="https://api.aimlapi.com/v1",

    # Insert your AIML API Key in the quotation marks instead of <YOUR_AIMLAPI_KEY>:
    api_key="<YOUR_AIMLAPI_KEY>",  
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "system",
            "content": "You are an AI assistant who knows everything.",
        },
        {
            "role": "user",
            "content": "Tell me, why is the sky blue?"
        },
    ],
)

message = response.choices[0].message.content

print(f"Assistant: {message}")
```

{% endcode %}

By running this code example, we received the following response from the chat model:

{% code overflow="wrap" %}

```http
Assistant: The sky appears blue due to a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it collides with gas molecules and small particles. Sunlight is made up of different colors, each with different wavelengths. Blue light has a shorter wavelength and is scattered in all directions by the gas molecules in the atmosphere more than other colors with longer wavelengths, such as red or yellow.
As a result, when you look up at the sky during the day, you see this scattered blue light being dispersed in all directions, making the sky appear blue to our eyes. During sunrise and sunset, the sun's light passes through a greater thickness of Earth's atmosphere, scattering the shorter blue wavelengths out of your line of sight and leaving the longer wavelengths, like red and orange, more dominant, which is why the sky often turns those colors at those times.
```

{% endcode %}

</details>

<details>

<summary>Complete Text Model List</summary>

<table><thead><tr><th width="297.4000244140625">Model ID + API Reference link</th><th width="134.20001220703125">Developer</th><th width="105.79998779296875">Context</th><th>Model Card</th></tr></thead><tbody><tr><td><a href="text-models-llm/openai/gpt-3.5-turbo">gpt-3.5-turbo</a></td><td>Open AI</td><td>16,000</td><td><a href="https://aimlapi.com/models/chat-gpt-3-5">Chat GPT 3.5 Turbo</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-3.5-turbo">gpt-3.5-turbo-0125</a></td><td>Open AI</td><td>16,000</td><td><a href="https://aimlapi.com/models/chat-gpt-3-5-turbo-0125">Chat GPT-3.5 Turbo 0125</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-3.5-turbo">gpt-3.5-turbo-1106</a></td><td>Open AI</td><td>16,000</td><td><a href="https://aimlapi.com/models/chat-gpt-3-5-turbo-1106">Chat GPT-3.5 Turbo 1106</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o">gpt-4o</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/chat-gpt-4-omni">Chat GPT-4o</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o">gpt-4o-2024-08-06</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-4o-2024-08-06-api">GPT-4o-2024-08-06</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o">gpt-4o-2024-05-13</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-4o-2024-05-13-api">GPT-4o-2024-05-13</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o-mini">gpt-4o-mini</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/chat-gpt-4o-mini">Chat GPT 4o mini</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o-mini">gpt-4o-mini-2024-07-18</a></td><td>Open AI</td><td>128,000</td><td>-</td></tr><tr><td><a href="text-models-llm/openai/gpt-4o">chatgpt-4o-latest</a></td><td>Open AI</td><td>128,000</td><td>-</td></tr><tr><td><a href="text-models-llm/openai/gpt-4o-audio-preview">gpt-4o-audio-preview</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-4o-audio-preview-api">GPT-4o Audio Preview</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o-mini-audio-preview">gpt-4o-mini-audio-preview</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-4o-mini-audio-api">GPT-4o mini Audio</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o-search-preview">gpt-4o-search-preview</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-4o-search-preview-api">GPT-4o Search Preview</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4o-mini-search-preview">gpt-4o-mini-search-preview</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-4o-mini-search-preview-api">GPT-4o Mini Search Preview</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4-turbo">gpt-4-turbo</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/chat-gpt-4-turbo">Chat GPT 4 Turbo</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4-turbo">gpt-4-turbo-2024-04-09</a></td><td>Open AI</td><td>128,000</td><td>-</td></tr><tr><td><a href="text-models-llm/openai/gpt-4">gpt-4</a></td><td>Open AI</td><td>8,000</td><td><a href="https://aimlapi.com/models/chat-gpt-4">Chat GPT 4</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4-preview">gpt-4-0125-preview</a></td><td>Open AI</td><td>8,000</td><td>-</td></tr><tr><td><a href="text-models-llm/openai/gpt-4-preview">gpt-4-1106-preview</a></td><td>Open AI</td><td>8,000</td><td>-</td></tr><tr><td><a href="text-models-llm/openai/o1-mini">o1-mini</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/openai-o1-mini-api">OpenAI o1-mini</a></td></tr><tr><td><a href="text-models-llm/openai/o1-mini">o1-mini-2024-09-12</a></td><td>Open AI</td><td>128,000</td><td>-</td></tr><tr><td><a href="text-models-llm/openai/o1">o1</a></td><td>Open AI</td><td>200,000</td><td><a href="https://aimlapi.com/models/openai-o1-api">OpenAI o1</a></td></tr><tr><td><a href="text-models-llm/openai/o3">openai/o3-2025-04-16</a></td><td>Open AI</td><td>200,000</td><td><a href="https://aimlapi.com/models/o3">o3</a></td></tr><tr><td><a href="text-models-llm/openai/o3-mini">o3-mini</a></td><td>Open AI</td><td>200,000</td><td><a href="https://aimlapi.com/models/openai-o3-mini-api">OpenAI o3 mini</a></td></tr><tr><td><a href="text-models-llm/openai/o3-pro">openai/o3-pro</a></td><td>Open AI</td><td>200,000</td><td><a href="https://aimlapi.com/models/o3-pro">o3-pro</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4.1">openai/gpt-4.1-2025-04-14</a></td><td>Open AI</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gpt-4-1">GPT-4.1</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4.1-mini">openai/gpt-4.1-mini-2025-04-14</a></td><td>Open AI</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gpt-4-1-mini-api">GPT-4.1 Mini</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-4.1-nano">openai/gpt-4.1-nano-2025-04-14</a></td><td>Open AI</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gpt-4-1-nano-api">GPT-4.1 Nano</a></td></tr><tr><td><a href="text-models-llm/openai/o4-mini">openai/o4-mini-2025-04-16</a></td><td>Open AI</td><td>200,000</td><td><a href="https://aimlapi.com/models/gpt-o4-mini-2025-04-16">GPT-o4-mini-2025-04-16</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-oss-20b">openai/gpt-oss-20b</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-oss-20b">GPT OSS 20B</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-oss-120b">openai/gpt-oss-120b</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-oss-120b">GPT OSS 120B</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5">openai/gpt-5-2025-08-07</a></td><td>Open AI</td><td>400,000</td><td><a href="https://aimlapi.com/models/gpt-5">GPT-5</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-mini">openai/gpt-5-mini-2025-08-07</a></td><td>Open AI</td><td>400,000</td><td><a href="https://aimlapi.com/models/gpt-5-mini">GPT-5 Mini</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-nano">openai/gpt-5-nano-2025-08-07</a></td><td>Open AI</td><td>400,000</td><td><a href="https://aimlapi.com/models/gpt-5-nano">GPT-5 Nano</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-chat">openai/gpt-5-chat-latest</a></td><td>Open AI</td><td>400,000</td><td><a href="https://aimlapi.com/models/gpt-5-chat">GPT-5 Chat</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-1">openai/gpt-5-1</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-5-1">GPT-5.1</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-1-chat-latest">openai/gpt-5-1-chat-latest</a></td><td>Open AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/gpt-5-1-chat-latest">GPT-5.1 Chat Latest</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-1-codex">openai/gpt-5-1-codex</a></td><td>Open AI</td><td>400,000</td><td><a href="https://aimlapi.com/models/gpt-5-1-codex">GPT-5.1 Codex</a></td></tr><tr><td><a href="text-models-llm/openai/gpt-5-1-codex-mini">openai/gpt-5-1-codex-mini</a></td><td>Open AI</td><td>400,000</td><td><a href="https://aimlapi.com/models/gpt-5-1-codex-mini">GPT-5.1 Codex Mini</a></td></tr><tr><td><a href="text-models-llm/anthropic/claude-3-opus">claude-3-opus-20240229</a></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-3-opus">Claude 3 Opus</a></td></tr><tr><td><a href="text-models-llm/anthropic/claude-3-haiku">claude-3-haiku-20240307</a></td><td>Anthropic</td><td>200,000</td><td>-</td></tr><tr><td><a href="text-models-llm/anthropic/claude-3.5-haiku">claude-3-5-haiku-20241022</a></td><td>Anthropic</td><td>200,000</td><td>-</td></tr><tr><td><a href="text-models-llm/anthropic/claude-3.7-sonnet">claude-3-7-sonnet-20250219</a></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-3-7-sonnet-api">Claude 3.7 Sonnet</a></td></tr><tr><td><a href="text-models-llm/anthropic/claude-4-opus">anthropic/claude-opus-4</a></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-4-opus">Claude 4 Opus</a></td></tr><tr><td><a href="text-models-llm/anthropic/claude-opus-4.1">anthropic/claude-opus-4.1<br>claude-opus-4-1<br>claude-opus-4-1-20250805</a></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-opus-4-1">Claude Opus 4.1</a></td></tr><tr><td><a href="text-models-llm/anthropic/claude-4-sonnet">anthropic/claude-sonnet-4</a></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-4-sonnet">Claude 4 Sonnet</a></td></tr><tr><td><p><a href="text-models-llm/anthropic/claude-4-5-sonnet">claude-sonnet-4-5-20250929</a></p><p><a href="text-models-llm/anthropic/claude-4-5-sonnet">anthropic/claude-sonnet-4.5</a></p><p><a href="text-models-llm/anthropic/claude-4-5-sonnet">claude-sonnet-4-5</a></p></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-4-5-sonnet">Claude 4.5 Sonnet</a></td></tr><tr><td><p><a href="text-models-llm/anthropic/claude-4.5-haiku">anthropic/claude-haiku-4.5</a><br><a href="text-models-llm/anthropic/claude-4.5-haiku">claude-haiku-4-5</a></p><p><a href="text-models-llm/anthropic/claude-4.5-haiku">claude-haiku-4-5-20251001</a></p></td><td>Anthropic</td><td>200,000</td><td><a href="https://aimlapi.com/models/claude-4-5-haiku">Claude 4.5 Haiku</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen2.5-7b-instruct-turbo">Qwen/Qwen2.5-7B-Instruct-Turbo</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-2-5-7b-instruct-api">Qwen 2.5 7B Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen2.5-coder-32b-instruct">Qwen/Qwen2.5-Coder-32B-Instruct</a></td><td>Alibaba Cloud</td><td>131,000</td><td>-</td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen-max">qwen-max</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-max-api">Qwen Max</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen-max">qwen-max-2025-01-25</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-max-2025-01-25-api">Qwen Max 2025-01-25</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen-plus">qwen-plus</a></td><td>Alibaba Cloud</td><td>131,000</td><td><a href="https://aimlapi.com/models/qwen-plus-api">Qwen Plus</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen-turbo">qwen-turbo</a></td><td>Alibaba Cloud</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/qwen-turbo-api">Qwen Turbo</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen2.5-72b-instruct-turbo">Qwen/Qwen2.5-72B-Instruct-Turbo</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-2-5-72b-instruct-turbo">Qwen 2.5 72B Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen-qwq-32b">Qwen/QwQ-32B</a></td><td>Alibaba Cloud</td><td>131,000</td><td><a href="https://aimlapi.com/models/qwq-32b-api">QwQ-32B</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-235b-a22b">Qwen/Qwen3-235B-A22B-fp8-tput</a></td><td>Alibaba Cloud</td><td>32,000</td><td><a href="https://aimlapi.com/models/qwen-3-235b-a22b-api">Qwen 3 235B A22B</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-32b">alibaba/qwen3-32b</a></td><td>Alibaba Cloud</td><td>131,000</td><td><a href="https://aimlapi.com/models/qwen3-32b">Qwen3-32B</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-coder-480b-a35b-instruct">alibaba/qwen3-coder-480b-a35b-instruct</a></td><td>Alibaba Cloud</td><td>262,000</td><td><a href="https://aimlapi.com/models/qwen3-coder-480b-a35b-instruct">Qwen3 Coder</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-235b-a22b-thinking-2507">alibaba/qwen3-235b-a22b-thinking-2507</a></td><td>Alibaba Cloud</td><td>262,000</td><td><a href="https://aimlapi.com/models/qwen3-235b-a22b">Qwen3 235B A22B Thinking</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-next-80b-a3b-instruct">alibaba/qwen3-next-80b-a3b-instruct</a></td><td>Alibaba Cloud</td><td>262,000</td><td><a href="https://aimlapi.com/models/qwen3-next-80b-a3b-instruct">Qwen3-Next-80B-A3B Instruct</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-next-80b-a3b-thinking">alibaba/qwen3-next-80b-a3b-thinking</a></td><td>Alibaba Cloud</td><td>262,000</td><td><a href="https://aimlapi.com/models/qwen3-next-80b-a3b-thinking">Qwen3-Next-80B-A3B Thinking</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-max-preview">alibaba/qwen3-max-preview</a></td><td>Alibaba Cloud</td><td>258,000</td><td><a href="text-models-llm/alibaba-cloud/qwen3-max-preview">Qwen3-Max Preview</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-max-instruct">alibaba/qwen3-max-instruct</a></td><td>Alibaba Cloud</td><td>262,000</td><td><a href="text-models-llm/alibaba-cloud/qwen3-max-instruct">Qwen3-Max Instruct</a></td></tr><tr><td><a href="text-models-llm/alibaba-cloud/qwen3-omni-30b-a3b-captioner">qwen3-omni-30b-a3b-captioner</a></td><td>Alibaba Cloud</td><td>65,000</td><td><a href="text-models-llm/alibaba-cloud/qwen3-omni-30b-a3b-captioner">qwen3-omni-30b-a3b-captioner</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-chat">deepseek-chat or<br>deepseek/deepseek-chat or<br>deepseek/deepseek-chat-v3-0324</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3">DeepSeek V3</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-r1">deepseek/deepseek-r1 or<br>deepseek-reasoner</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-r1-api">DeepSeek R1</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-prover-v2">deepseek/deepseek-prover-v2</a></td><td>DeepSeek</td><td>164,000</td><td><a href="https://aimlapi.com/models/deepseek-prover-v2-api">DeepSeek Prover V2</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-chat-v3.1">deepseek/deepseek-chat-v3.1</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3-1-chat">DeepSeek V3.1 Chat</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-reasoner-v3.1">deepseek/deepseek-reasoner-v3.1</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3-1-reasoner">DeepSeek V3.1 Reasoner</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-reasoner-v3.2-exp-thinking">deepseek/deepseek-thinking-v3.2-exp</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3-2-exp-thinking">DeepSeek V3.2-Exp Thinking</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-reasoner-v3.2-exp-non-thinking">deepseek/deepseek-non-thinking-v3.2-exp</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3-2-exp-non-thinking">DeepSeek V3.2-Exp Non-Thinking</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-reasoner-v3.1-terminus">deepseek/deepseek-reasoner-v3.1-terminus</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3-1-terminus-reasoning">DeepSeek V3.1 Terminus Reasoning</a></td></tr><tr><td><a href="text-models-llm/deepseek/deepseek-non-reasoner-v3.1-terminus">deepseek/deepseek-non-reasoner-v3.1-terminus</a></td><td>DeepSeek</td><td>128,000</td><td><a href="https://aimlapi.com/models/deepseek-v3-1-terminus-non-reasoning">DeepSeek V3.1 Terminus Non-Reasoning</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/mixtral-8x7b-instruct-v0.1">mistralai/Mixtral-8x7B-Instruct-v0.1</a></td><td>Mistral AI</td><td>64,000</td><td><a href="https://aimlapi.com/models/mixtral-8x7b-instruct-v01">Mixtral-8x7B Instruct v0.1</a></td></tr><tr><td><a href="text-models-llm/meta/llama-3.3-70b-instruct-turbo">meta-llama/Llama-3.3-70B-Instruct-Turbo</a></td><td>Meta</td><td>128,000</td><td><a href="https://aimlapi.com/models/meta-llama-3-3-70b-instruct-turbo-api">Meta Llama 3.3 70B Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/meta/llama-3.2-3b-instruct-turbo">meta-llama/Llama-3.2-3B-Instruct-Turbo</a></td><td>Meta</td><td>131,000</td><td><a href="https://aimlapi.com/models/llama-3-2-3b-instruct-turbo">Llama 3.2 3B Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/meta/meta-llama-3-8b-instruct-lite">meta-llama/Meta-Llama-3-8B-Instruct-Lite</a></td><td>Meta</td><td>9,000</td><td><a href="https://aimlapi.com/models/llama-3-8b-instruct-lite-api">Llama 3 8B Instruct Lite</a></td></tr><tr><td><a href="text-models-llm/meta/llama-3-chat-hf">meta-llama/Llama-3-70b-chat-hf</a></td><td>Meta</td><td>8,000</td><td><a href="https://aimlapi.com/models/meta-llama-3-70b-instruct">Llama 3 70B Instruct Reference</a></td></tr><tr><td><a href="text-models-llm/meta/meta-llama-3.1-405b-instruct-turbo">meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo</a></td><td>Meta</td><td>4,000</td><td><a href="https://aimlapi.com/models/llama-3-1-405b-api">Llama 3.1 (405B) Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/meta/meta-llama-3.1-8b-instruct-turbo">meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo</a></td><td>Meta</td><td>128,000</td><td><a href="https://aimlapi.com/models/llama-3-1-8b-api">Llama 3.1 8B Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/meta/meta-llama-3.1-70b-instruct-turbo">meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo</a></td><td>Meta</td><td>128,000</td><td><a href="https://aimlapi.com/models/llama-3-1-70b-instruct-turbo-api">Llama 3.1 70B Instruct Turbo</a></td></tr><tr><td><a href="text-models-llm/meta/llama-4-scout">meta-llama/llama-4-scout</a></td><td>Meta</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/llama-4-scout-api">Llama 4 Scout</a></td></tr><tr><td><a href="text-models-llm/meta/llama-4-maverick">meta-llama/llama-4-maverick</a></td><td>Meta</td><td>256,000</td><td><a href="https://aimlapi.com/models/llama-4-maverick-api">Llama 4 Maverick</a></td></tr><tr><td><a href="text-models-llm/meta/llama-3.3-70b-versatile">meta-llama/llama-3.3-70b-versatile</a></td><td>Meta</td><td>131,000</td><td><a href="text-models-llm/meta/llama-3.3-70b-versatile">Llama 3.3 70B Versatile</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/mistral-7b-instruct">mistralai/Mistral-7B-Instruct-v0.2</a></td><td>Mistral AI</td><td>32,000</td><td><a href="https://aimlapi.com/models/mistral-7b-instruct-v02">Mistral (7B) Instruct v0.2</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/mistral-7b-instruct">mistralai/Mistral-7B-Instruct-v0.1</a></td><td>Mistral AI</td><td>8,000</td><td><a href="https://aimlapi.com/models/mistral-7b-instruct">Mistral (7B) Instruct v0.1</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/mistral-7b-instruct">mistralai/Mistral-7B-Instruct-v0.3</a></td><td>Mistral AI</td><td>32,000</td><td><a href="https://aimlapi.com/models/mistral-7b-instruct-v0-3">Mistral (7B) Instruct v0.3</a></td></tr><tr><td><a href="text-models-llm/google/gemini-2.0-flash-exp">gemini-2.0-flash-exp</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-2-0-flash-experimental">Gemini 2.0 Flash Experimental</a></td></tr><tr><td><a href="text-models-llm/google/gemini-2.0-flash">gemini-2.0-flash</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-2-0-flash-api">Gemini 2.0 Flash</a></td></tr><tr><td><a href="text-models-llm/google/gemini-2.5-flash-lite-preview">google/gemini-2.5-flash-lite-preview</a></td><td>Google</td><td>1,000,000</td><td>‚Äì</td></tr><tr><td><a href="text-models-llm/google/gemini-2.5-flash">google/gemini-2.5-flash</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-2-5-flash-api">Gemini 2.5 Flash</a></td></tr><tr><td><a href="text-models-llm/google/gemini-2.5-pro">google/gemini-2.5-pro</a></td><td>Google</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/gemini-pro-2-5-api">Gemini 2.5 Pro</a></td></tr><tr><td><a href="text-models-llm/google/gemma-3">google/gemma-3-4b-it</a></td><td>Google</td><td>128,000</td><td><a href="https://aimlapi.com/models/gemma-3-4b-api">Gemma 3 (4B)</a></td></tr><tr><td><a href="text-models-llm/google/gemma-3">google/gemma-3-12b-it</a></td><td>Google</td><td>128,000</td><td><a href="https://aimlapi.com/models/gemma-3-12b-api">Gemma 3 (12B)</a></td></tr><tr><td><a href="text-models-llm/google/gemma-3">google/gemma-3-27b-it</a></td><td>Google</td><td>128,000</td><td><a href="https://aimlapi.com/models/gemma-3-27b-api">Gemma 3 (27B)</a></td></tr><tr><td><a href="text-models-llm/google/gemma-3n-4b">google/gemma-3n-e4b-it</a></td><td>Google</td><td>8,192</td><td><a href="https://aimlapi.com/models/gemma-3n-4b">Gemma 3n 4B</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/mistral-tiny">mistralai/mistral-tiny</a></td><td>Mistral AI</td><td>32,000</td><td><a href="https://aimlapi.com/models/mistral-tiny-api">Mistral Tiny</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/mistral-nemo">mistralai/mistral-nemo</a></td><td>Mistral AI</td><td>128,000</td><td><a href="https://aimlapi.com/models/mistral-nemo-api">Mistral Nemo</a></td></tr><tr><td><a href="text-models-llm/anthracite/magnum-v4">anthracite-org/magnum-v4-72b</a></td><td>Anthracite</td><td>32,000</td><td><a href="https://aimlapi.com/models/magnum-v4-72b-api">Magnum v4 72B</a></td></tr><tr><td><a href="text-models-llm/nvidia/llama-3.1-nemotron-70b">nvidia/llama-3.1-nemotron-70b-instruct</a></td><td>NVIDIA</td><td>128,000</td><td><a href="https://aimlapi.com/models/llama-3-1-nemotron-70b-instruct-api">Llama 3.1 Nemotron 70B Instruct</a></td></tr><tr><td><a href="text-models-llm/nvidia/nemotron-nano-9b-v2">nvidia/nemotron-nano-9b-v2</a></td><td>NVIDIA</td><td>128,000</td><td><em>Coming Soon</em></td></tr><tr><td><a href="text-models-llm/nvidia/llama-3.1-nemotron-70b-1">nvidia/nemotron-nano-12b-v2-vl</a></td><td>NVIDIA</td><td>128,000</td><td><em>Coming Soon</em></td></tr><tr><td><a href="text-models-llm/cohere/command-a">cohere/command-a</a></td><td>Cohere</td><td>256,000</td><td><a href="https://aimlapi.com/models/command-a">Command A</a></td></tr><tr><td><a href="text-models-llm/mistral-ai/codestral-2501">mistralai/codestral-2501</a></td><td>Mistral AI</td><td>256,000</td><td><a href="https://aimlapi.com/models/mistral-codestral-2501-api">Mistral Codestral-2501</a></td></tr><tr><td><a href="text-models-llm/minimax/text-01">MiniMax-Text-01</a></td><td>MiniMax</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/minimax-text-01-api">MiniMax-Text-01</a></td></tr><tr><td><a href="text-models-llm/minimax/m1">minimax/m1</a></td><td>MiniMax</td><td>1,000,000</td><td><a href="https://aimlapi.com/models/minimax-m1">MiniMax M1</a></td></tr><tr><td><a href="text-models-llm/minimax/m2">minimax/m2</a></td><td>MiniMax</td><td>200,000</td><td><a href="https://aimlapi.com/models/minimax-m2">MiniMax M2</a></td></tr><tr><td><a href="text-models-llm/moonshot/kimi-k2-preview">moonshot/kimi-k2-preview</a></td><td>Moonshot</td><td>131,000</td><td><a href="https://aimlapi.com/models/kimi-k2">Kimi-K2</a></td></tr><tr><td><a href="text-models-llm/moonshot/kimi-k2-preview">moonshot/kimi-k2-0905-preview</a></td><td>Moonshot</td><td>256,000</td><td><a href="https://aimlapi.com/models/kimi-k2">Kimi-K2</a></td></tr><tr><td><a href="text-models-llm/moonshot/kimi-k2-turbo-preview">moonshot/kimi-k2-turbo-preview</a></td><td>Moonshot</td><td>256,000</td><td><a href="https://aimlapi.com/models/kimi-k2-turbo-preview">Kimi K2 Turbo Preview</a></td></tr><tr><td><a href="text-models-llm/nousresearch/hermes-4-405b">nousresearch/hermes-4-405b</a></td><td>NousResearch</td><td>131,000</td><td><em>-</em></td></tr><tr><td><a href="text-models-llm/perplexity/sonar">perplexity/sonar</a></td><td>Perplexity</td><td>128,000</td><td><a href="https://aimlapi.com/models/perplexity-sonar">Sonar</a></td></tr><tr><td><a href="text-models-llm/perplexity/sonar-pro">perplexity/sonar-pro</a></td><td>Perplexity</td><td>200,000</td><td><a href="https://aimlapi.com/models/perplexity-sonar-pro">Sonar Pro</a></td></tr><tr><td><a href="text-models-llm/xai/grok-3-beta">x-ai/grok-3-beta</a></td><td>xAI</td><td>131,000</td><td><a href="https://aimlapi.com/models/grok-3-beta-api">Grok 3 Beta</a></td></tr><tr><td><a href="text-models-llm/xai/grok-3-mini-beta">x-ai/grok-3-mini-beta</a></td><td>xAI</td><td>131,000</td><td><a href="https://aimlapi.com/models/grok-3-beta-mini-api">Grok 3 Beta Mini</a></td></tr><tr><td><a href="text-models-llm/xai/grok-4">x-ai/grok-4-07-09</a></td><td>xAI</td><td>256,000</td><td><a href="https://aimlapi.com/models/grok-4">Grok 4</a></td></tr><tr><td><a href="text-models-llm/xai/grok-code-fast-1">x-ai/grok-code-fast-1</a></td><td>xAI</td><td>256,000</td><td><a href="https://aimlapi.com/models/grok-code-fast-1">Grok Code Fast 1</a></td></tr><tr><td><a href="text-models-llm/xai/grok-4-fast-non-reasoning">x-ai/grok-4-fast-non-reasoning</a></td><td>xAI</td><td>2,000,000</td><td><a href="https://aimlapi.com/models/grok-4-fast">Grok 4 Fast</a></td></tr><tr><td><a href="text-models-llm/xai/grok-4-fast-reasoning">x-ai/grok-4-fast-reasoning</a></td><td>xAI</td><td>2,000,000</td><td><a href="https://aimlapi.com/models/grok-4-fast-reasoning">Grok 4 Fast Reasoning</a></td></tr><tr><td><a href="text-models-llm/zhipu/glm-4.5-air">zhipu/glm-4.5-air</a></td><td>Zhipu</td><td>128,000</td><td><a href="https://aimlapi.com/models/glm-4-5-air">GLM-4.5 Air</a></td></tr><tr><td><a href="text-models-llm/zhipu/glm-4.5">zhipu/glm-4.5</a></td><td>Zhipu</td><td>128,000</td><td><a href="https://aimlapi.com/models/glm-4-5">GLM-4.5</a></td></tr><tr><td><a href="text-models-llm/zhipu/glm-4.6">zhipu/glm-4.6</a></td><td>Zhipu</td><td>200,000</td><td><a href="text-models-llm/zhipu/glm-4.6">GLM-4.6</a></td></tr></tbody></table>

</details>


# Alibaba Cloud


# qwen-max

{% columns %}
{% column width="66.66666666666666%" %}
{% hint style="info" %}
This documentation is valid for the following list of our models:

* `qwen-max`
* `qwen-max-2025-01-25`
  {% endhint %}
  {% endcolumn %}

{% column width="33.33333333333334%" %} <a href="https://aimlapi.com/app/?model=alibaba/qwen-max&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

The large-scale Mixture-of-Experts (MoE) language model. Excels in language understanding and task performance. Supports 29 languages, including Chinese, English, and Arabic.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["alibaba/qwen-max","qwen-max","qwen-max-2025-01-25"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."}},"required":["model","messages"],"title":"alibaba/qwen-max"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"qwen-max",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'qwen-max',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "chatcmpl-62aa6045-cee9-995a-bbf5-e3b7e7f3d683",
  "system_fingerprint": null,
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I assist you today? üòä"
      }
    }
  ],
  "created": 1756983980,
  "model": "qwen-max",
  "usage": {
    "prompt_tokens": 30,
    "completion_tokens": 148,
    "total_tokens": 178,
    "prompt_tokens_details": {
      "cached_tokens": 0
    }
  }
}
```

{% endcode %}

</details>


# qwen-plus

{% columns %}
{% column width="66.66666666666666%" %}
{% hint style="info" %}
This documentation is valid for the following list of our models:

* `qwen-plus`
  {% endhint %}
  {% endcolumn %}

{% column width="33.33333333333334%" %} <a href="https://aimlapi.com/app/?model=alibaba/qwen-plus&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

An advanced large language model. Multilingual support, including Chinese and English. Enhanced reasoning capabilities for complex tasks. Improved instruction-following abilities.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["alibaba/qwen-plus"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."}},"required":["model","messages"],"title":"alibaba/qwen-plus"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"qwen-plus",
        "messages":[
            {
                "role":"user",
                "content":"Hello" # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'qwen-plus',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-4fda1bd7-a679-95b9-b81d-1bfc6ae98448', 'system_fingerprint': None, 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today? If you have any questions or need help with anything, just let me know! üòä'}}], 'created': 1744143962, 'model': 'qwen-plus', 'usage': {'prompt_tokens': 8, 'completion_tokens': 68, 'total_tokens': 76, 'prompt_tokens_details': {'cached_tokens': 0}}}
```

{% endcode %}

</details>


# qwen-turbo

{% columns %}
{% column width="66.66666666666666%" %}
{% hint style="info" %}
This documentation is valid for the following list of our models:

* `qwen-turbo`
  {% endhint %}
  {% endcolumn %}

{% column width="33.33333333333334%" %} <a href="https://aimlapi.com/app/?model=alibaba/qwen-turbo&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

This model is designed to enhance both the performance and efficiency of AI agents developed on the Alibaba Cloud Model Studio platform. Optimized for speed and precision in generative AI application development. Improves AI agent comprehension and adaptation to enterprise data, especially when integrated with Retrieval-Augmented Generation (RAG) architectures.\
Large context window (<kbd>1,000,000</kbd> tokens).

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

Below, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["alibaba/qwen-turbo"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."}},"required":["model","messages"],"title":"alibaba/qwen-turbo"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"qwen-turbo",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'qwen-turbo',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-a4556a4c-f985-9ef2-b976-551ac7cef85a', 'system_fingerprint': None, 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': "Hello! How can I help you today? Is there something you would like to talk about or learn more about? I'm here to help with any questions you might have."}}], 'created': 1744144035, 'model': 'qwen-turbo', 'usage': {'prompt_tokens': 1, 'completion_tokens': 15, 'total_tokens': 16, 'prompt_tokens_details': {'cached_tokens': 0}}}
```

{% endcode %}

</details>


# Qwen2.5-7B-Instruct-Turbo

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `Qwen/Qwen2.5-7B-Instruct-Turbo`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=Qwen/Qwen2.5-7B-Instruct-Turbo&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

A cutting-edge large language model designed to understand and generate text based on specific instructions. It excels in various tasks, including coding, mathematical problem-solving, and generating structured outputs.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["Qwen/Qwen2.5-7B-Instruct-Turbo"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"Qwen/Qwen2.5-7B-Instruct-Turbo"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"Qwen/Qwen2.5-7B-Instruct-Turbo",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'Qwen/Qwen2.5-7B-Instruct-Turbo',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npK4C7y-3NKUce-92d4866b1e62ef98', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today?', 'tool_calls': []}}], 'created': 1744144252, 'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'usage': {'prompt_tokens': 19, 'completion_tokens': 6, 'total_tokens': 25}}
```

{% endcode %}

</details>


# Qwen2.5-72B-Instruct-Turbo

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `Qwen/Qwen2.5-72B-Instruct-Turbo`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=Qwen/Qwen2.5-72B-Instruct-Turbo&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

A state-of-the-art large language model designed for a variety of natural language processing tasks, including instruction following, coding assistance, and mathematical problem-solving.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["Qwen/Qwen2.5-72B-Instruct-Turbo"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"Qwen/Qwen2.5-72B-Instruct-Turbo"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"Qwen/Qwen2.5-72B-Instruct-Turbo",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'Qwen/Qwen2.5-72B-Instruct-Turbo',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npK4dJH-4yUbBN-92d488799a225ec1', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.', 'tool_calls': []}}], 'created': 1744144336, 'model': 'Qwen/Qwen2.5-72B-Instruct-Turbo', 'usage': {'prompt_tokens': 76, 'completion_tokens': 73, 'total_tokens': 149}}
```

{% endcode %}

</details>


# Qwen2.5-Coder-32B-Instruct

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `Qwen/Qwen2.5-Coder-32B-Instruct`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=Qwen/Qwen2.5-Coder-32B-Instruct&#x26;mode=code" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

The 32B variant of the latest code-focused model series (formerly CodeQwen). The most capable, with strong performance in coding, math, and general tasks.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["Qwen/Qwen2.5-Coder-32B-Instruct"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"Qwen/Qwen2.5-Coder-32B-Instruct"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"Qwen/Qwen2.5-Coder-32B-Instruct",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'Qwen/Qwen2.5-Coder-32B-Instruct',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npK8TA2-4yUbBN-92d49ab20aeacfa2', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today?', 'tool_calls': []}}], 'created': 1744145083, 'model': 'Qwen/Qwen2.5-Coder-32B-Instruct', 'usage': {'prompt_tokens': 50, 'completion_tokens': 17, 'total_tokens': 67}}
```

{% endcode %}

</details>


# Qwen-QwQ-32B

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `Qwen/QwQ-32B`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=Qwen/QwQ-32B&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

A compact reasoning model designed to tackle complex problem-solving tasks. Achieves performance comparable to much larger models like [DeepSeek-R1](https://docs.aimlapi.com/api-references/text-models-llm/deepseek/deepseek-r1) (671 billion parameters). Excels in mathematical reasoning, coding, and structured workflows.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["Qwen/QwQ-32B"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"Qwen/QwQ-32B"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"Qwen/QwQ-32B",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'Qwen/QwQ-32B',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npK8kgb-2j9zxn-92d49c21a9f9302c', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': '<think>\nOkay, the user said "Hello". I should respond politely. Let me think of a friendly greeting. Maybe "Hello! How can I assist you today?" That sounds good. It\'s welcoming and opens the door for them to ask for help. I\'ll go with that.\n</think>\n\nHello! How can I assist you today?', 'tool_calls': []}}], 'created': 1744145142, 'model': 'Qwen/QwQ-32B', 'usage': {'prompt_tokens': 25, 'completion_tokens': 88, 'total_tokens': 113}}
```

{% endcode %}

</details>


# Qwen3-235B-A22B

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `Qwen/Qwen3-235B-A22B-fp8-tput`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=Qwen/Qwen3-235B-A22B-fp8-tput&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

A hybrid instruct-and-reasoning text model.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["Qwen/Qwen3-235B-A22B-fp8-tput"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"Qwen/Qwen3-235B-A22B-fp8-tput"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"Qwen/Qwen3-235B-A22B-fp8-tput",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'Qwen/Qwen3-235B-A22B-fp8-tput',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'ntFB5Ap-6UHjtw-93cab7642d14efac', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': '<think>\nOkay, the user just said "Hello". I should respond in a friendly and welcoming manner. Let me make sure to greet them back and offer assistance. Maybe say something like, "Hello! How can I help you today?" That should be open-ended and inviting for them to ask questions or share what\'s on their mind. Keep it simple and positive.\n</think>\n\nHello! How can I help you today? üòä', 'tool_calls': []}}], 'created': 1746725755, 'model': 'Qwen/Qwen3-235B-A22B-fp8-tput', 'usage': {'prompt_tokens': 4, 'completion_tokens': 111, 'total_tokens': 115}}
```

{% endcode %}

</details>


# qwen3-32b

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `alibaba/qwen3-32b`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=alibaba/qwen3-32b&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

A world-class model with comparable quality to DeepSeek R1 while outperforming [GPT-4.1](https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4.1) and [Claude Sonnet 3.7](https://docs.aimlapi.com/api-references/text-models-llm/anthropic/claude-3.7-sonnet). Optimized for both complex reasoning and efficient dialogue.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["alibaba/qwen3-32b"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"enable_thinking":{"type":"boolean","default":false,"description":"Specifies whether to use the thinking mode."},"thinking_budget":{"type":"integer","minimum":1,"description":"The maximum reasoning length, effective only when enable_thinking is set to true."}},"required":["model","messages"],"title":"alibaba/qwen3-32b"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example #1: Without Thinking and Streaming

{% hint style="warning" %}
`enable_thinking` must be set to `false` for non-streaming calls.
{% endhint %}

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"alibaba/qwen3-32b",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'alibaba/qwen3-32b',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "chatcmpl-1d8a5aa6-34ce-9832-a296-d312b944b437",
  "system_fingerprint": null,
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I assist you today? üòä",
        "reasoning_content": ""
      }
    }
  ],
  "created": 1756990273,
  "model": "qwen3-32b",
  "usage": {
    "prompt_tokens": 19,
    "completion_tokens": 65,
    "total_tokens": 84
  }
}
```

{% endcode %}

</details>

## Code Example #2: Enable Thinking and Streaming

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"alibaba/qwen3-32b",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": True, 
        "stream": True
    }
)

print(response.text)
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
data: {"id":"chatcmpl-81964e30-1a7c-9668-b78c-a750587ec497","choices":[{"delta":{"content":null,"role":"assistant","refusal":null,"reasoning_content":""},"index":0,"finish_reason":null}],"created":1753944369,"model":"qwen3-32b","object":"chat.completion.chunk","usage":null}

data: {"id":"chatcmpl-81964e30-1a7c-9668-b78c-a750587ec497","choices":[{"delta":{"content":null,"refusal":null,"reasoning_content":"Okay"},"index":0,"finish_reason":null}],"created":1753944369,"model":"qwen3-32b","object":"chat.completion.chunk","usage":null}

data: {"id":"chatcmpl-81964e30-1a7c-9668-b78c-a750587ec497","choices":[{"delta":{"content":null,"refusal":null,"reasoning_content":","},"index":0,"finish_reason":null}],"created":1753944369,"model":"qwen3-32b","object":"chat.completion.chunk","usage":null}

data: {"id":"chatcmpl-81964e30-1a7c-9668-b78c-a750587ec497","choices":[{"delta":{"content":null,"refusal":null,"reasoning_content":" the"},"index":0,"finish_reason":null}],"created":1753944369,"model":"qwen3-32b","object":"chat.completion.chunk","usage":null}

data: {"id":"chatcmpl-81964e30-1a7c-9668-b78c-a750587ec497","choices":[{"delta":{"content":null,"refusal":null,"reasoning_content":" user said \"Hello\". I should respond in a friendly and welcoming manner. Let"},"index":0,"finish_reason":null}],"created":1753944369,"model":"qwen3-32b","object":"chat.completion.chunk","usage":null}

data: {"id":"chatcmpl-81964e30-1a7c-9668-b78c-a750587ec497","choices":[{"delta":{"content":null,"refusal":null,"reasoning_content":" me make sure to acknowledge their greeting and offer assistance. Maybe something like, \""},"index":0,"finish_reason":null}],"created":1753944369,"model":"qwen3-32b","object":"chat.completion.chunk","usage":null}

data: {"id":"chatcmpl-81964e30-1a7c-9668-b78c-a750587ec497","choices":[{"delta":{"content":null,"refusal":null,"reasoning_content":"Hello! How can I assist you today?\" That's simple and open-ended."},"index":0,"finish_reason":null}],"created":1753944369,"model":"qwen3-32b","object":"chat.completion.chunk","usage":null}

data: {"id":"chatcmpl-81964e30-1a7c-9668-b78c-a750587ec497","choices":[{"delta":{"content":null,"refusal":null,"reasoning_content":" I need to check if there's any specific context I should consider, but since"},"index":0,"finish_reason":null}],"created":1753944369,"model":"qwen3-32b","object":"chat.completion.chunk","usage":null}

data: {"id":"chatcmpl-81964e30-1a7c-9668-b78c-a750587ec497","choices":[{"delta":{"content":null,"refusal":null,"reasoning_content":" there's none, a general response is fine. Alright, that should work."},"index":0,"finish_reason":null}],"created":1753944369,"model":"qwen3-32b","object":"chat.completion.chunk","usage":null}

data: {"id":"chatcmpl-81964e30-1a7c-9668-b78c-a750587ec497","choices":[{"delta":{"content":"Hello! How can I assist you today?","refusal":null,"reasoning_content":null},"index":0,"finish_reason":null}],"created":1753944369,"model":"qwen3-32b","object":"chat.completion.chunk","usage":null}

data: {"id":"chatcmpl-81964e30-1a7c-9668-b78c-a750587ec497","choices":[{"delta":{"content":"","refusal":null,"reasoning_content":null},"index":0,"finish_reason":"stop"}],"created":1753944369,"model":"qwen3-32b","object":"chat.completion.chunk","usage":null}

data: {"id":"chatcmpl-81964e30-1a7c-9668-b78c-a750587ec497","choices":[],"created":1753944369,"model":"qwen3-32b","object":"chat.completion.chunk","usage":{"prompt_tokens":13,"completion_tokens":2010,"total_tokens":2023,"completion_tokens_details":{"reasoning_tokens":82}}}
```

{% endcode %}

</details>

The example above prints the raw output of the model. The text is typically split into multiple chunks. While this is helpful for debugging, if your goal is to evaluate the model's reasoning and get a clean, human-readable response, you should aggregate both the reasoning and the final answer in a loop ‚Äî for example:

<details>

<summary>Example with response parsing</summary>

{% code overflow="wrap" %}

```python
import requests
import json

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization": "Bearer b72af53a19ea41caaf5a74ba1f6fc62b",
        "Content-Type": "application/json",
    },
    json={
        "model": "alibaba/qwen3-32b",
        "messages": [
            {
                "role": "user",
                
                # Insert your question for the model here, instead of Hello:
                "content": "Hello" 
            }
        ],
        "stream": True,
    }
)

answer = ""
reasoning = ""

for line in response.iter_lines():
    if not line or not line.startswith(b"data:"):
        continue

    try:
        raw = line[6:].decode("utf-8").strip()
        if raw == "[DONE]":
            continue

        data = json.loads(raw)
        choices = data.get("choices")
        if not choices or "delta" not in choices[0]:
            continue

        delta = choices[0]["delta"]
        content_piece = delta.get("content")
        reasoning_piece = delta.get("reasoning_content")

        if content_piece:
            answer += content_piece
        if reasoning_piece:
            reasoning += reasoning_piece

    except Exception as e:
        print(f"Error parsing chunk: {e}")


print("\n--- MODEL REASONING ---")
print(reasoning.strip())

print("\n--- MODEL RESPONSE ---")
print(answer.strip())
```

{% endcode %}

</details>

After running such code, you'll receive only the model's textual output in a clear and structured format:

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
--- MODEL REASONING ---
Okay, the user sent "Hello". I need to respond appropriately. Since it's a greeting, I should reply in a friendly and welcoming manner. Maybe ask how I can assist them. Keep it simple and open-ended to encourage them to share what they need help with. Let me make sure the tone is positive and helpful.

--- MODEL RESPONSE ---
Hello! How can I assist you today? üòä
```

{% endcode %}

</details>


# qwen3-coder-480b-a35b-instruct

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `alibaba/qwen3-coder-480b-a35b-instruct`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=alibaba/qwen3-coder-480b-a35b-instruct&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

The most powerful model in the Qwen3 Coder series ‚Äî a 480B-parameter MoE architecture with 35B active parameters. It natively supports a 256K token context and can handle up to 1M tokens using extrapolation techniques, delivering outstanding performance in both coding and agentic tasks.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["alibaba/qwen3-coder-480b-a35b-instruct"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"alibaba/qwen3-coder-480b-a35b-instruct"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"alibaba/qwen3-coder-480b-a35b-instruct",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'alibaba/qwen3-coder-480b-a35b-instruct',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "chatcmpl-f906efa6-f816-9a06-a32b-aa38da5fe11a",
  "system_fingerprint": null,
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?"
      }
    }
  ],
  "created": 1753866642,
  "model": "qwen3-coder-480b-a35b-instruct",
  "usage": {
    "prompt_tokens": 28,
    "completion_tokens": 142,
    "total_tokens": 170
  }
}
```

{% endcode %}

</details>


# qwen3-235b-a22b-thinking-2507

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `alibaba/qwen3-235b-a22b-thinking-2507`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=alibaba/qwen3-235b-a22b-thinking-2507&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["alibaba/qwen3-235b-a22b-thinking-2507"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"alibaba/qwen3-235b-a22b-thinking-2507"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"alibaba/qwen3-235b-a22b-thinking-2507",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'alibaba/qwen3-235b-a22b-thinking-2507',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "chatcmpl-af05df1d-5b72-925e-b3a9-437acbd89b1a",
  "system_fingerprint": null,
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! üòä How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific!",
        "reasoning_content": "Okay, the user said \"Hello\". That's a simple greeting. I should respond in a friendly and welcoming way. Let me make sure to keep it open-ended so they feel comfortable to ask questions or share what's on their mind. Maybe add a smiley emoji to keep it warm. Let me check if there's anything else they might need. Since it's just a hello, probably not much more needed here. Just a polite reply."
      }
    }
  ],
  "created": 1753871154,
  "model": "qwen3-235b-a22b-thinking-2507",
  "usage": {
    "prompt_tokens": 13,
    "completion_tokens": 2187,
    "total_tokens": 2200
  }
}
```

{% endcode %}

</details>


# qwen3-next-80b-a3b-instruct

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `alibaba/qwen3-next-80b-a3b-instruct`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=alibaba/qwen3-next-80b-a3b-instruct&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

An instruction-tuned chat model optimized for fast, stable replies without reasoning traces, designed for complex tasks in reasoning, coding, knowledge QA, and multilingual use, with strong alignment and formatting.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["alibaba/qwen3-next-80b-a3b-instruct"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."}},"required":["model","messages"],"title":"alibaba/qwen3-next-80b-a3b-instruct"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"alibaba/qwen3-next-80b-a3b-instruct",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'alibaba/qwen3-next-80b-a3b-instruct',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "chatcmpl-a944254a-4252-9a54-af1b-94afcfb9807e",
  "system_fingerprint": null,
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today? üòä"
      }
    }
  ],
  "created": 1758228572,
  "model": "qwen3-next-80b-a3b-instruct",
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 46,
    "total_tokens": 55
  }
}
```

{% endcode %}

</details>


# qwen3-next-80b-a3b-thinking

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `alibaba/qwen3-next-80b-a3b-thinking`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=alibaba/qwen3-next-80b-a3b-thinking&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

The model may take longer to generate reasoning content than its predecessor. Alibaba Cloud strongly recommends its use for highly complex reasoning tasks.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["alibaba/qwen3-next-80b-a3b-thinking"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"alibaba/qwen3-next-80b-a3b-thinking"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"alibaba/qwen3-next-80b-a3b-thinking",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'alibaba/qwen3-next-80b-a3b-thinking',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "chatcmpl-576aaaf9-f712-9114-b098-c1ee83fbfb6b",
  "system_fingerprint": null,
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! üòä How can I assist you today?",
        "reasoning_content": "Okay, the user said \"Hello\". I need to respond appropriately. Let me think.\n\nFirst, I should acknowledge their greeting. A simple \"Hello!\" would be good. Maybe add a friendly emoji to keep it warm.\n\nWait, but maybe they want to start a conversation. I should ask how I can help them. That way, I'm being helpful and opening the door for them to ask questions.\n\nLet me check the standard response. Typically, for \"Hello\", the assistant says something like \"Hello! How can I assist you today?\" or \"Hi there! What can I do for you?\"\n\nYes, that's right. Keep it friendly and open-ended. Maybe add a smiley emoji to make it approachable.\n\nSo the response should be: \"Hello!  How can I assist you today?\"\n\nThat's good. Let me make sure there's no mistake. Yes, that's standard. No need for anything complicated here. Just a simple, welcoming reply.\n\nAlternatively, sometimes people use \"Hi\" instead of \"Hello\", but since they said \"Hello\", responding with \"Hello\" is fine. Maybe \"Hi there!\" could also work, but sticking to \"Hello\" matches their greeting.\n\nYes, \"Hello!  How can I assist you today?\" is perfect. It's polite, friendly, and offers assistance. That should be the response."
      }
    }
  ],
  "created": 1758229078,
  "model": "qwen3-next-80b-a3b-thinking",
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 7182,
    "total_tokens": 7191,
    "completion_tokens_details": {
      "reasoning_tokens": 277
    }
  }
}
```

{% endcode %}

</details>


# qwen3-max-preview

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `alibaba/qwen3-max-preview`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=alibaba/qwen3-max-preview&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

The preview version of [Qwen3 Max Instruct](https://docs.aimlapi.com/api-references/text-models-llm/alibaba-cloud/qwen3-max-instruct).

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["alibaba/qwen3-max-preview"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."}},"required":["model","messages"],"title":"alibaba/qwen3-max-preview"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"alibaba/qwen3-max-preview",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'alibaba/qwen3-max-preview',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "chatcmpl-8ffebc65-b625-926a-8208-b765371cb1d0",
  "system_fingerprint": null,
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I assist you today? üòä"
      }
    }
  ],
  "created": 1758898044,
  "model": "qwen3-max-preview",
  "usage": {
    "prompt_tokens": 23,
    "completion_tokens": 139,
    "total_tokens": 162
  }
}
```

{% endcode %}

</details>


# qwen3-max-instruct

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `alibaba/qwen3-max-instruct`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=alibaba/qwen3-max-instruct&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

This model offers improved accuracy in math, coding, logic, and science, handles complex instructions in Chinese and English more reliably, reduces hallucinations, supports 100+ languages with stronger translation and commonsense reasoning, and is optimized for RAG and tool use, though it lacks a dedicated ‚Äòthinking‚Äô mode.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["alibaba/qwen3-max-instruct"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."}},"required":["model","messages"],"title":"alibaba/qwen3-max-instruct"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"alibaba/qwen3-max-instruct",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'alibaba/qwen3-max-instruct',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "chatcmpl-bec5dc33-8f63-96b9-89a4-00aecfce7af8",
  "system_fingerprint": null,
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?"
      }
    }
  ],
  "created": 1758898624,
  "model": "qwen3-max",
  "usage": {
    "prompt_tokens": 23,
    "completion_tokens": 113,
    "total_tokens": 136
  }
}
```

{% endcode %}

</details>


# qwen3-omni-30b-a3b-captioner

{% columns %}
{% column width="58.333333333333336%" %}
{% hint style="info" %}
This documentation is valid for the following model:

* `alibaba/qwen3-omni-30b-a3b-captioner`
  {% endhint %}
  {% endcolumn %}

{% column width="41.666666666666664%" %} <a href="https://aimlapi.com/app/?model=alibaba/qwen3-omni-30b-a3b-captioner&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

This model is an open-source model built on **Qwen3-Omni** that automatically generates rich, detailed descriptions of complex audio ‚Äî including speech, music, ambient sounds, and effects ‚Äî without prompts. It detects emotions, musical styles, instruments, and sensitive information, making it ideal for audio analysis, security auditing, intent recognition, and editing.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## Generate a conversational response using a language model.

> Creates a chat completion using a language model, allowing interactive conversation by predicting the next response based on the given chat history. This is useful for AI-driven dialogue systems and virtual assistants.

```json
{"openapi":"3.0.0","info":{"title":"AI/ML Gateway","version":"1.0"},"servers":[{"url":"https://api.aimlapi.com"}],"security":[{"access-token":[]}],"components":{"securitySchemes":{"access-token":{"scheme":"bearer","bearerFormat":"<YOUR_AIMLAPI_KEY>","type":"http","description":"Bearer key"}}},"paths":{"/v1/chat/completions":{"post":{"operationId":"ChatCompletionsControllerV1_completeChat_v1","summary":"Generate a conversational response using a language model.","description":"Creates a chat completion using a language model, allowing interactive conversation by predicting the next response based on the given chat history. This is useful for AI-driven dialogue systems and virtual assistants.","parameters":[],"requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"enum":["alibaba/qwen3-omni-30b-a3b-captioner"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."}},"required":["data"]}},"required":["type","input_audio"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]}},"required":["model","messages"]}}}},"responses":{"201":{"description":""}},"tags":["Chat Completions"]}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}

<pre class="language-python" data-overflow="wrap"><code class="lang-python">import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of &#x3C;YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer &#x3C;YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
      "model": "alibaba/qwen3-omni-30b-a3b-captioner",
      "messages": [
        {
          "role": "user",
          "content": [
            {
              "type": "input_audio",
              "input_audio": {
<strong>                "data": "https://cdn.aimlapi.com/eagle/files/elephant/cJUTeeCmpoqIV1Q3WWDAL_vibevoice-output-7b98283fd3974f48ba90e91d2ee1f971.mp3"
</strong>              }
            }
          ]
        }
      ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
</code></pre>

{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'alibaba/qwen3-max-instruct',
      messages:[
        {
          role: 'user',
          content: [
            {
              type: 'input_audio',
              input_audio: {
                data: 'https://cdn.aimlapi.com/eagle/files/elephant/cJUTeeCmpoqIV1Q3WWDAL_vibevoice-output-7b98283fd3974f48ba90e91d2ee1f971.mp3'
              }
            }
          ]
        }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "chatcmpl-bec5dc33-8f63-96b9-89a4-00aecfce7af8",
  "system_fingerprint": null,
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?"
      }
    }
  ],
  "created": 1758898624,
  "model": "qwen3-max",
  "usage": {
    "prompt_tokens": 23,
    "completion_tokens": 113,
    "total_tokens": 136
  }
}
```

{% endcode %}

</details>


# Anthracite


# magnum-v4

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following model:

* `anthracite-org/magnum-v4-72b`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=anthracite-org/magnum-v4-72b&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

A LLM fine-tuned on top of Qwen2.5, specifically designed to replicate the prose quality of the Claude 3 models, particularly Sonnet and [Opus](https://docs.aimlapi.com/api-references/text-models-llm/anthropic/claude-3-opus). It excels in generating coherent and contextually rich text.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["anthracite-org/magnum-v4-72b"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."}},"required":["model","messages"],"title":"anthracite-org/magnum-v4-72b"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"anthracite-org/magnum-v4-72b",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'anthracite-org/magnum-v4-72b',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'gen-1744217980-rdVBcVTb76dllKCCRjak', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today?', 'refusal': None}}], 'created': 1744217980, 'model': 'anthracite-org/magnum-v4-72b', 'usage': {'prompt_tokens': 37, 'completion_tokens': 50, 'total_tokens': 87}}
```

{% endcode %}

</details>


# Anthropic


# Claude 3 Haiku

{% columns %}
{% column %}
{% hint style="info" %}
This documentation is valid for the following list of our models:

* `anthropic/claude-3-haiku`
* `anthropic/claude-3-haiku-20240307`
* `claude-3-haiku-20240307`
* `claude-3-haiku-latest`
  {% endhint %}
  {% endcolumn %}

{% column %} <a href="https://aimlapi.com/app/?model=claude-3-haiku-20240307&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

The quick and streamlined model, offering near-instant responsiveness.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["claude-3-haiku-20240307","claude-3-haiku","anthropic/claude-3-haiku-20240307","claude-3-haiku-latest"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"]},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"]},"data":{"type":"string"}},"required":["type","media_type","data"]}},"required":["type","source"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"],"title":"claude-3-haiku-20240307"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"claude-3-haiku-latest",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'claude-3-haiku-latest',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'msg_01Fd4uU3AZ3TXzSpSKN7oeDP', 'object': 'chat.completion', 'model': 'claude-3-haiku-20240307', 'choices': [{'index': 0, 'message': {'reasoning_content': '', 'content': 'Hello! How can I assist you today?', 'role': 'assistant'}, 'finish_reason': 'end_turn', 'logprobs': None}], 'created': 1744218395, 'usage': {'prompt_tokens': 4, 'completion_tokens': 32, 'total_tokens': 36}}
```

{% endcode %}

</details>


# Claude 3 Opus

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>anthropic/claude-3-opus</code></li><li><code>anthropic/claude-3-opus-20240229</code></li><li><code>claude-3-opus-20240229</code></li><li><code>claude-3-opus-latest</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=claude-3-opus-20240229&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A highly capable multimodal model designed to process both text and image data. It excels in tasks requiring complex reasoning, mathematical problem-solving, coding, and multilingual text understanding.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["claude-3-opus-20240229","anthropic/claude-3-opus","anthropic/claude-3-opus-20240229","claude-3-opus-latest"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"]},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"]},"data":{"type":"string"}},"required":["type","media_type","data"]}},"required":["type","source"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"],"title":"claude-3-opus-20240229"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"claude-3-opus-latest",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'claude-3-opus-latest',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'msg_013njSJ6FKESFossfd8UHddJ', 'object': 'chat.completion', 'model': 'claude-3-opus-20240229', 'choices': [{'index': 0, 'message': {'reasoning_content': '', 'content': 'Hello! How can I assist you today?', 'role': 'assistant'}, 'finish_reason': 'end_turn', 'logprobs': None}], 'created': 1744218476, 'usage': {'prompt_tokens': 252, 'completion_tokens': 1890, 'total_tokens': 2142}}
```

{% endcode %}

</details>


# Claude 3.5 Haiku

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>anthropic/claude-3-5-haiku</code></li><li><code>anthropic/claude-3-5-haiku-20241022</code></li><li><code>claude-3-5-haiku-20241022</code></li><li><code>claude-3-5-haiku-latest</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=claude-3-5-haiku-20241022&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A cutting-edge model designed for rapid data processing and advanced reasoning capabilities. Excels in coding assistance, customer service interactions, and content moderation.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["claude-3-5-haiku-20241022","anthropic/claude-3-5-haiku","anthropic/claude-3-5-haiku-20241022","claude-3-5-haiku-latest"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"]},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"]},"data":{"type":"string"}},"required":["type","media_type","data"]}},"required":["type","source"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"],"title":"claude-3-5-haiku-20241022"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"claude-3-5-haiku-latest",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'claude-3-5-haiku-latest',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'msg_01QfRmDBXVWcARjbwZBbJxrR', 'object': 'chat.completion', 'model': 'claude-3-5-haiku-20241022', 'choices': [{'index': 0, 'message': {'reasoning_content': '', 'content': 'Hi there! How are you doing today? Is there anything I can help you with?', 'role': 'assistant'}, 'finish_reason': 'end_turn', 'logprobs': None}], 'created': 1744218440, 'usage': {'prompt_tokens': 17, 'completion_tokens': 221, 'total_tokens': 238}}
```

{% endcode %}

</details>


# Claude 3.7 Sonnet

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>claude-3-7-sonnet-20250219</code></li><li><code>claude-3-7-sonnet-latest</code></li><li><code>anthropic/claude-3.7-sonnet</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=claude-3-7-sonnet-20250219&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A hybrid reasoning model, designed to tackle complex tasks. It introduces a dual-mode operation, combining standard language generation with extended thinking capabilities.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["claude-3-7-sonnet-20250219","anthropic/claude-3.7-sonnet","claude-3-7-sonnet-latest"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"]},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"]},"data":{"type":"string"}},"required":["type","media_type","data"]}},"required":["type","source"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"],"title":"claude-3-7-sonnet-20250219"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"anthropic/claude-3.7-sonnet",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'anthropic/claude-3.7-sonnet',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'msg_01MmQNxa1E5mU8EyMXzT9zEU', 'object': 'chat.completion', 'model': 'claude-3-7-sonnet-20250219', 'choices': [{'index': 0, 'message': {'reasoning_content': '', 'content': "Hello! How can I assist you today? Whether you have a question, need information, or would like to discuss a particular topic, I'm here to help. What's on your mind?", 'role': 'assistant'}, 'finish_reason': 'end_turn', 'logprobs': None}], 'created': 1744218600, 'usage': {'prompt_tokens': 50, 'completion_tokens': 1323, 'total_tokens': 1373}}
```

{% endcode %}

</details>


# Claude 4 Opus

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following model: <code>anthropic/claude-opus-4</code></p></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=claude-opus-4-20250514&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

The leading coding model globally, consistently excelling at complex, long-duration tasks and agent-based workflows.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["claude-opus-4-20250514","anthropic/claude-opus-4","claude-opus-4"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"]},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"]},"data":{"type":"string"}},"required":["type","media_type","data"]}},"required":["type","source"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"],"title":"claude-opus-4-20250514"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"anthropic/claude-opus-4",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'anthropic/claude-opus-4',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "msg_01BDDxHJZjH3UBwLrZBUiASE",
  "object": "chat.completion",
  "model": "claude-opus-4-20250514",
  "choices": [
    {
      "index": 0,
      "message": {
        "reasoning_content": "",
        "content": "Hello! How can I help you today?",
        "role": "assistant"
      },
      "finish_reason": "end_turn",
      "logprobs": null
    }
  ],
  "created": 1748529508,
  "usage": {
    "prompt_tokens": 252,
    "completion_tokens": 1890,
    "total_tokens": 2142
  }
}
```

{% endcode %}

</details>


# Claude 4 Sonnet

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following model:<br><code>anthropic/claude-sonnet-4</code></p></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=claude-sonnet-4-20250514&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A major improvement over [Claude ](https://docs.aimlapi.com/api-references/text-models-llm/anthropic/claude-3.7-sonnet)[3.7 Sonnet](https://docs.aimlapi.com/api-references/text-models-llm/anthropic/claude-3.7-sonnet), offering better coding abilities, stronger reasoning, and more accurate responses to your instructions.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["claude-sonnet-4-20250514","anthropic/claude-sonnet-4","claude-sonnet-4"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"]},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"]},"data":{"type":"string"}},"required":["type","media_type","data"]}},"required":["type","source"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"],"title":"claude-sonnet-4-20250514"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"anthropic/claude-sonnet-4",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'anthropic/claude-sonnet-4',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "msg_011MNbgezv2p5BBE9RvnsZV9",
  "object": "chat.completion",
  "model": "claude-sonnet-4-20250514",
  "choices": [
    {
      "index": 0,
      "message": {
        "reasoning_content": "",
        "content": "Hello! How are you doing today? Is there anything I can help you with?",
        "role": "assistant"
      },
      "finish_reason": "end_turn",
      "logprobs": null
    }
  ],
  "created": 1748522617,
  "usage": {
    "prompt_tokens": 50,
    "completion_tokens": 630,
    "total_tokens": 680
  }
}
```

{% endcode %}

</details>


# Claude 4.1 Opus

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>anthropic/claude-opus-4.1</code></li><li><code>claude-opus-4-1</code></li><li><code>claude-opus-4-1-20250805</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=claude-opus-4-1-20250805&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

{% hint style="success" %}
All three IDs listed above refer to the same model; we support them for backward compatibility.
{% endhint %}

## Model Overview

An upgrade to [Claude Opus 4](https://docs.aimlapi.com/api-references/text-models-llm/anthropic/claude-4-opus) on agentic tasks, real-world coding, and thinking.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [code examples](#code-example-1-without-thinking) that show how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["claude-opus-4-1-20250805","anthropic/claude-opus-4.1","claude-opus-4-1"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"]},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"]},"data":{"type":"string"}},"required":["type","media_type","data"]}},"required":["type","source"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"],"title":"claude-opus-4-1-20250805"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example #1: Without Thinking

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"anthropic/claude-opus-4.1",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'anthropic/claude-opus-4.1',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "msg_018y2VPSZ5nNnqS3goMsjMxE",
  "object": "chat.completion",
  "model": "claude-opus-4-1-20250805",
  "choices": [
    {
      "index": 0,
      "message": {
        "reasoning_content": "",
        "content": "Hello! How can I help you today?",
        "role": "assistant"
      },
      "finish_reason": "end_turn",
      "logprobs": null
    }
  ],
  "created": 1754552562,
  "usage": {
    "prompt_tokens": 252,
    "completion_tokens": 1890,
    "total_tokens": 2142
  }
}
```

{% endcode %}

</details>

## Code Example #2: Thinking Enabled

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"anthropic/claude-opus-4.1",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hell
            }
        ],
        "max_tokens": 1025, # must be greater than 'budget_tokens'
        "thinking":{
            "budget_tokens": 1024,
            "type": "enabled"
        }
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'anthropic/claude-opus-4.1',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ],
        max_tokens: 1025, // must be greater than 'budget_tokens'
        thinking:{
            budget_tokens: 1024,
            type: 'enabled'
        }
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "msg_01G9P4b9HG3PeKm1rRvS8kop",
  "object": "chat.completion",
  "model": "claude-opus-4-1-20250805",
  "choices": [
    {
      "index": 0,
      "message": {
        "reasoning_content": "The human has greeted me with a simple \"Hello\". I should respond in a friendly and helpful manner, acknowledging their greeting and inviting them to share how I can assist them today.",
        "content": "Hello! How can I help you today?",
        "role": "assistant"
      },
      "finish_reason": "end_turn",
      "logprobs": null
    }
  ],
  "created": 1755704373,
  "usage": {
    "prompt_tokens": 1134,
    "completion_tokens": 9450,
    "total_tokens": 10584
  }
}
```

{% endcode %}

</details>


# Claude 4.5 Sonnet

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>claude-sonnet-4-5</code></li><li><code>anthropic/claude-sonnet-4-5</code></li><li><code>claude-sonnet-4-5-20250929</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=claude-sonnet-4-20250514&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A major improvement over [Claude 4 Sonnet,](https://docs.aimlapi.com/api-references/text-models-llm/anthropic/claude-4-sonnet) offering better coding abilities, stronger reasoning, and more accurate responses to your instructions.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["claude-sonnet-4-5-20250929","anthropic/claude-sonnet-4.5","claude-sonnet-4-5"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"]},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"]},"data":{"type":"string"}},"required":["type","media_type","data"]}},"required":["type","source"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"],"title":"claude-sonnet-4-5-20250929"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"anthropic/claude-sonnet-4.5",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'anthropic/claude-sonnet-4.5',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "msg_011MNbgezv2p5BBE9RvnsZV9",
  "object": "chat.completion",
  "model": "claude-sonnet-4-20250514",
  "choices": [
    {
      "index": 0,
      "message": {
        "reasoning_content": "",
        "content": "Hello! How are you doing today? Is there anything I can help you with?",
        "role": "assistant"
      },
      "finish_reason": "end_turn",
      "logprobs": null
    }
  ],
  "created": 1748522617,
  "usage": {
    "prompt_tokens": 50,
    "completion_tokens": 630,
    "total_tokens": 680
  }
}
```

{% endcode %}

</details>


# Claude 4.5 Haiku

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>claude-haiku-4-5</code></li><li><code>anthropic/claude-haiku-4.5</code></li><li><code>claude-haiku-4-5-20251001</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=claude-haiku-4-5-20251001&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

The model offers coding performance comparable to [Claude Sonnet 4](https://docs.aimlapi.com/api-references/text-models-llm/anthropic/claude-4-sonnet), but at one-third the cost and more than twice the speed.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["claude-haiku-4-5-20251001","anthropic/claude-haiku-4.5","claude-haiku-4-5"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"]},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"]},"data":{"type":"string"}},"required":["type","media_type","data"]}},"required":["type","source"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"],"title":"claude-haiku-4-5-20251001"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"anthropic/claude-haiku-4.5",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
        "enable_thinking": False
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'anthropic/claude-haiku-4.5',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "msg_01HbdLU9f78VAHxuYZ7Qp9Y1",
  "object": "chat.completion",
  "model": "claude-haiku-4-5-20251001",
  "choices": [
    {
      "index": 0,
      "message": {
        "reasoning_content": "",
        "content": "Hello! üëã How can I help you today?",
        "role": "assistant"
      },
      "finish_reason": "end_turn",
      "logprobs": null
    }
  ],
  "created": 1760650965,
  "usage": {
    "prompt_tokens": 8,
    "completion_tokens": 16,
    "total_tokens": 24
  }
}
```

{% endcode %}

</details>


# Cohere


# command-a

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:<br><code>cohere/command-a</code></p></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=cohere/command-a&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A powerful LLM with advanced capabilities for enterprise applications.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["cohere/command-a"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."}},"required":["model","messages"],"title":"cohere/command-a"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"cohere/command-a",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'cohere/command-a',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "gen-1752165706-Nd1dXa1kuCCoOIpp5oxy",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I assist you today?",
        "reasoning_content": null,
        "refusal": null
      }
    }
  ],
  "created": 1752165706,
  "model": "cohere/command-a",
  "usage": {
    "prompt_tokens": 5,
    "completion_tokens": 189,
    "total_tokens": 194
  }
}
```

{% endcode %}

</details>


# DeepSeek


# DeepSeek V3

{% hint style="info" %}
This documentation is valid for the following list of our models:

* `deepseek-chat`
* `deepseek/deepseek-chat`
* `deepseek/deepseek-chat-v3-0324`
  {% endhint %}

{% hint style="success" %}
We provide the latest version of this model from **Mar 24, 2025**.\
All three IDs listed above refer to the same model; we support them for backward compatibility.
{% endhint %}

## Model Overview

DeepSeek V3 (or deepseek-chat) is an advanced conversational AI designed to deliver highly engaging and context-aware dialogues. This model excels in understanding and generating human-like text, making it an ideal solution for creating responsive and intelligent chatbots.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["deepseek-chat","deepseek/deepseek-chat","deepseek/deepseek-chat-v3-0324"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"deepseek-chat"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"deepseek-chat",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'deepseek-chat',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json
{'id': 'gen-1744194041-A363xKnsNwtv6gPnUPnO', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': "Hello! üòä How can I assist you today? Feel free to ask me anything‚ÄîI'm here to help! üöÄ", 'reasoning_content': '', 'refusal': None}}], 'created': 1744194041, 'model': 'deepseek/deepseek-chat-v3-0324', 'usage': {'prompt_tokens': 16, 'completion_tokens': 88, 'total_tokens': 104}}
```

{% endcode %}

</details>


# DeepSeek R1

{% hint style="info" %}
This documentation is valid for the following list of our models:

* `deepseek/deepseek-r1`
* `deepseek-reasoner`
  {% endhint %}

{% hint style="success" %}
Both IDs listed above refer to the same model; we support them for backward compatibility.
{% endhint %}

## Model Overview

DeepSeek R1 is a cutting-edge reasoning model developed by DeepSeek AI, designed to excel in complex problem-solving, mathematical reasoning, and programming assistance.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["deepseek-reasoner","deepseek/deepseek-r1"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."}},"required":["model","messages"],"title":"deepseek-reasoner"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"deepseek/deepseek-r1",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'deepseek/deepseek-r1',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npPT68N-zqrih-92d94499ec25b74e', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': '\nHello! How can I assist you today? üòä', 'reasoning_content': '', 'tool_calls': []}}], 'created': 1744193985, 'model': 'deepseek-ai/DeepSeek-R1', 'usage': {'prompt_tokens': 5, 'completion_tokens': 74, 'total_tokens': 79}}
```

{% endcode %}

</details>


# DeepSeek Prover V2

{% hint style="info" %}
This documentation is valid for the following model: `deepseek/deepseek-prover-v2`
{% endhint %}

## Model Overview

A massive 671B-parameter model, presumed to focus on logic and mathematics. It appears to be an upgrade over DeepSeek Prover V1.5.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["deepseek/deepseek-prover-v2"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"deepseek/deepseek-prover-v2"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"deepseek/deepseek-prover-v2",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of YOUR_AIMLAPI_KEY
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'deepseek/deepseek-prover-v2',
        messages:[
            {
                role:'user',

                // Insert your question for the model here, instead of Hello:
                content: 'Hello'
            }
        ]
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json
{'id': 'gen-1747126732-rD70SgJEEBVBXPHmKlNJ', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': "Hello there! As a virtual assistant, I'm here to help you with a wide variety of tasks and questions. Here are some of the things I can do:  \n  \n1. Provide information on a wide range of topics, from science and history to pop culture and current events.  \n2. Answer factual questions using my knowledge base.  \n3. Assist with homework or research projects by providing explanations, summaries, and resources.  \n4. Help with language-related tasks such as grammar, vocabulary, translations, and writing assistance.  \n5. Engage in general conversation, discussing ideas, and providing opinions on various subjects.  \n6. Offer advice or tips on various life situations, though not as a substitute for professional guidance.  \n7. Perform calculations, solve math problems, and help with understanding mathematical concepts.  \n8. Generate creative content like stories, poems, or song lyrics.  \n9. Play interactive games, such as word games or trivia.  \n10. Help you practice a language by conversing in it.  \n  \nFeel free to ask me anything, and I'll do my best to assist you!", 'reasoning_content': None, 'refusal': None}}], 'created': 1747126732, 'model': 'deepseek/deepseek-prover-v2', 'usage': {'prompt_tokens': 15, 'completion_tokens': 1021, 'total_tokens': 1036, 'prompt_tokens_details': None}}
```

{% endcode %}

</details>


# DeepSeek Chat V3.1

{% hint style="info" %}
This documentation is valid for the following list of our models:

* `deepseek/deepseek-chat-v3.1`
  {% endhint %}

## Model Overview

August 2025 update of the [DeepSeek V3](https://docs.aimlapi.com/api-references/text-models-llm/deepseek/deepseek-chat) non-reasoning model.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["deepseek/deepseek-chat-v3.1"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"deepseek/deepseek-chat-v3.1"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"deepseek/deepseek-chat-v3.1",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'deepseek/deepseek-chat-v3.1',
      messages:[{
              role:'user',
              content: 'Hello'}  // Insert your question instead of Hello
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "c13865eb-50bf-440c-922f-19b1bbef517d",
  "system_fingerprint": "fp_feb633d1f5_prod0820_fp8_kvcache",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I assist you today? üòä",
        "reasoning_content": ""
      }
    }
  ],
  "created": 1756386652,
  "model": "deepseek-chat",
  "usage": {
    "prompt_tokens": 1,
    "completion_tokens": 39,
    "total_tokens": 40,
    "prompt_tokens_details": {
      "cached_tokens": 0
    },
    "prompt_cache_hit_tokens": 0,
    "prompt_cache_miss_tokens": 5
  }
}
```

{% endcode %}

</details>


# DeepSeek Reasoner V3.1

{% hint style="info" %}
This documentation is valid for the following list of our models:

* `deepseek/deepseek-reasoner-v3.1`
  {% endhint %}

## Model Overview

August 2025 update of [the DeepSeek R1](https://docs.aimlapi.com/api-references/text-models-llm/deepseek/deepseek-r1) reasoning model. Skilled at complex problem-solving, mathematical reasoning, and programming assistance.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["deepseek/deepseek-reasoner-v3.1"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."}},"required":["model","messages"],"title":"deepseek/deepseek-reasoner-v3.1"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"deepseek/deepseek-reasoner-v3.1",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'deepseek/deepseek-reasoner-v3.1',
      messages:[{
              role:'user',
              content: 'Hello'}  // Insert your question instead of Hello
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "ca664281-d3c3-40d3-9d80-fe96a65884dd",
  "system_fingerprint": "fp_feb633d1f5_prod0820_fp8_kvcache",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today? üòä",
        "reasoning_content": ""
      }
    }
  ],
  "created": 1756386069,
  "model": "deepseek-reasoner",
  "usage": {
    "prompt_tokens": 1,
    "completion_tokens": 325,
    "total_tokens": 326,
    "prompt_tokens_details": {
      "cached_tokens": 0
    },
    "completion_tokens_details": {
      "reasoning_tokens": 80
    },
    "prompt_cache_hit_tokens": 0,
    "prompt_cache_miss_tokens": 5
  }
}
```

{% endcode %}

</details>


# Deepseek Reasoner V3.1 Terminus

{% hint style="info" %}
This documentation is valid for the following list of our models:

* `deepseek/deepseek-reasoner-v3.1-terminus`
  {% endhint %}

## Model Overview

September 2025 update of [the DeepSeek Reasoner V3.1](https://docs.aimlapi.com/api-references/text-models-llm/deepseek/deepseek-reasoner-v3.1) model. The model produces more consistent and dependable results.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["deepseek/deepseek-reasoner-v3.1-terminus"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."}},"required":["model","messages"],"title":"deepseek/deepseek-reasoner-v3.1-terminus"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"deepseek/deepseek-reasoner-v3.1-terminus",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'deepseek/deepseek-reasoner-v3.1-terminus',
      messages:[{
              role:'user',
              content: 'Hello'}  // Insert your question instead of Hello
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "543f56cb-f59f-42cc-8ed7-8efdd72f185d",
  "system_fingerprint": "fp_ffc7281d48_prod0820_fp8_kvcache",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I assist you today? üòä",
        "reasoning_content": ""
      }
    }
  ],
  "created": 1761034613,
  "model": "deepseek-reasoner",
  "usage": {
    "prompt_tokens": 3,
    "completion_tokens": 98,
    "total_tokens": 101,
    "prompt_tokens_details": {
      "cached_tokens": 0
    },
    "completion_tokens_details": {
      "reasoning_tokens": 99
    },
    "prompt_cache_hit_tokens": 0,
    "prompt_cache_miss_tokens": 5
  }
}
```

{% endcode %}

</details>


# Deepseek Non-reasoner V3.1 Terminus

{% hint style="info" %}
This documentation is valid for the following list of our models:

* `deepseek/deepseek-non-reasoner-v3.1-terminus`
  {% endhint %}

## Model Overview

September 2025 update of [the DeepSeek Chat V3.1](https://docs.aimlapi.com/api-references/text-models-llm/deepseek/deepseek-chat-v3.1) non-reasoning model. The model produces more consistent and dependable results.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["deepseek/deepseek-non-reasoner-v3.1-terminus"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"deepseek/deepseek-non-reasoner-v3.1-terminus"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"deepseek/deepseek-non-reasoner-v3.1-terminus",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'deepseek/deepseek-non-reasoner-v3.1-terminus',
      messages:[{
              role:'user',
              content: 'Hello'}  // Insert your question instead of Hello
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "cc8c3054-115d-4dac-9269-2abffcaabab5",
  "system_fingerprint": "fp_ffc7281d48_prod0820_fp8_kvcache",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I assist you today? üòä",
        "reasoning_content": ""
      }
    }
  ],
  "created": 1761036636,
  "model": "deepseek-chat",
  "usage": {
    "prompt_tokens": 3,
    "completion_tokens": 10,
    "total_tokens": 13,
    "prompt_tokens_details": {
      "cached_tokens": 0
    },
    "prompt_cache_hit_tokens": 0,
    "prompt_cache_miss_tokens": 5
  }
}
```

{% endcode %}

</details>


# DeepSeek V3.2 Exp Thinking

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following model:<br><code>deepseek/deepseek-thinking-v3.2-exp</code></p></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=deepseek/deepseek-thinking-v3.2-exp&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

September 2025 update of [the DeepSeek R1](https://docs.aimlapi.com/api-references/text-models-llm/deepseek/deepseek-r1) reasoning model. Skilled at complex problem-solving, mathematical reasoning, and programming assistance.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["deepseek/deepseek-thinking-v3.2-exp"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."}},"required":["model","messages"],"title":"deepseek/deepseek-thinking-v3.2-exp"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"deepseek/deepseek-thinking-v3.2-exp",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'deepseek/deepseek-thinking-v3.2-exp',
      messages:[
        {
          role:'user',
          content: 'Hello'  // Insert your question instead of Hello
        }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "ca664281-d3c3-40d3-9d80-fe96a65884dd",
  "system_fingerprint": "fp_feb633d1f5_prod0820_fp8_kvcache",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today? üòä",
        "reasoning_content": ""
      }
    }
  ],
  "created": 1756386069,
  "model": "deepseek-reasoner",
  "usage": {
    "prompt_tokens": 1,
    "completion_tokens": 325,
    "total_tokens": 326,
    "prompt_tokens_details": {
      "cached_tokens": 0
    },
    "completion_tokens_details": {
      "reasoning_tokens": 80
    },
    "prompt_cache_hit_tokens": 0,
    "prompt_cache_miss_tokens": 5
  }
}
```

{% endcode %}

</details>


# DeepSeek V3.2 Exp Non-thinking

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following model:</p><ul><li><code>deepseek/deepseek-non-thinking-v3.2-exp</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=deepseek/deepseek-thinking-v3.2-exp&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

September 2025 update of the [DeepSeek V3](https://docs.aimlapi.com/api-references/text-models-llm/deepseek/deepseek-chat) non-reasoning model.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["deepseek/deepseek-non-thinking-v3.2-exp"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"deepseek/deepseek-non-thinking-v3.2-exp"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"deepseek/deepseek-non-thinking-v3.2-exp",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'deepseek/deepseek-non-thinking-v3.2-exp',
      messages:[
        {
          role:'user',
          content: 'Hello'  // Insert your question instead of Hello
        }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "ca664281-d3c3-40d3-9d80-fe96a65884dd",
  "system_fingerprint": "fp_feb633d1f5_prod0820_fp8_kvcache",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today? üòä",
        "reasoning_content": ""
      }
    }
  ],
  "created": 1756386069,
  "model": "deepseek-reasoner",
  "usage": {
    "prompt_tokens": 1,
    "completion_tokens": 325,
    "total_tokens": 326,
    "prompt_tokens_details": {
      "cached_tokens": 0
    },
    "completion_tokens_details": {
      "reasoning_tokens": 80
    },
    "prompt_cache_hit_tokens": 0,
    "prompt_cache_miss_tokens": 5
  }
}
```

{% endcode %}

</details>


# Google


# gemini-2.0-flash-exp

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>google/gemini-2.0-flash-exp</code></li><li><code>gemini-2.0-flash-exp</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=google/gemini-2.0-flash-exp&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A cutting-edge multimodal AI model developed by Google DeepMind, designed to power agentic experiences. This model is capable of processing text and images.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["google/gemini-2.0-flash-exp","gemini-2.0-flash-exp"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"google/gemini-2.0-flash-exp"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"google/gemini-2.0-flash-exp",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'google/gemini-2.0-flash-exp',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': '2025-04-09|09:53:23.624687-07|5.250.254.39|-1825976509', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello there! How can I help you today?\n'}}], 'created': 1744217603, 'model': 'google/gemini-2.0-flash-exp', 'usage': {'prompt_tokens': 5, 'completion_tokens': 173, 'total_tokens': 178}}
```

{% endcode %}

</details>


# gemini-2.0-flash

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following model:<br><code>google/gemini-2.0-flash</code></p></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=google/gemini-2.0-flash&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A cutting-edge multimodal AI model developed by Google DeepMind, designed to power agentic experiences. This model is capable of processing text and images.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["google/gemini-2.0-flash","gemini-2.0-flash"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."}},"required":["model","messages"],"title":"google/gemini-2.0-flash"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"google/gemini-2.0-flash",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'google/gemini-2.0-flash',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': '2025-04-10|01:16:19.235787-07|9.7.175.26|-701765511', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I help you today?\n'}}], 'created': 1744272979, 'model': 'google/gemini-2.0-flash', 'usage': {'prompt_tokens': 0, 'completion_tokens': 8, 'total_tokens': 8}}
```

{% endcode %}

</details>


# gemini-2.5-flash-lite-preview

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following model:<br><code>google/gemini-2.5-flash-lite-preview</code></p></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=google/gemini-2.5-flash-lite-preview&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

The model excels at high-volume, latency-sensitive tasks like translation and classification.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["google/gemini-2.5-flash-lite-preview"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."}},"required":["model","messages"],"title":"google/gemini-2.5-flash-lite-preview"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"google/gemini-2.5-flash-lite-preview",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'google/gemini-2.5-flash-lite-preview',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "gen-1752482994-9LhqM48PhAmhiRTtl2ys",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello there! How can I help you today?",
        "reasoning_content": null,
        "refusal": null
      }
    }
  ],
  "created": 1752482994,
  "model": "google/gemini-2.5-flash-lite-preview-06-17",
  "usage": {
    "prompt_tokens": 0,
    "completion_tokens": 9,
    "total_tokens": 9
  }
}
```

{% endcode %}

</details>


# gemini-2.5-flash

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following model:<br><code>google/gemini-2.5-flash</code></p></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=google/gemini-2.5-flash&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

Gemini 2.5 models are capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["google/gemini-2.5-flash"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."}},"required":["model","messages"],"title":"google/gemini-2.5-flash"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% hint style="warning" %}
A common issue when using reasoning-capable models via API is receiving an empty string in the `content` field‚Äîmeaning the model did not return the expected text, yet no error was thrown.

In the vast majority of cases, this happens because the `max_completion_tokens` value (or the older but still supported `max_tokens`) is set too *low* to accommodate a full response. Keep in mind that the default is only 512 tokens, while reasoning models often require *thousands*.

Pay attention to the `finish_reason` field in the response. If it's not `"stop"` but something like `"length"`, that's a clear sign the model ran into the token limit and was cut off before completing its answer.

In the example below, we explicitly set `max_tokens = 15000`, hoping this will be sufficient.
{% endhint %}

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json   # for getting a structured output with indentation

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"google/gemini-2.5-flash",
        "messages":[
            {
                "role":"user",
                # Insert your question for the model here:
                "content":"Hi! What do you think about mankind?"
            }
        ],
        "max_tokens":15000,
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'google/gemini-2.5-flash',
        messages:[
            {
                role:'user',

                // Insert your question for the model here:
                content: 'Hi! What do you think about mankind?'
            }
        ],
        max_tokens: 15000,
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "yZ-DaJXqAayonvgPr5XvuQY",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Mankind, or humanity, is an incredibly complex and fascinating subject to \"think\" about from my perspective as an AI. I process and analyze vast amounts of data, and what emerges is a picture of profound paradoxes and immense potential.\n\nHere are some of the key aspects I observe and \"think\" about:\n\n1.  **Capacity for Immense Creation and Destruction:**\n    *   **Creation:** Humans have built breathtaking civilizations, created profound art and music, developed groundbreaking science and technology, and explored the furthest reaches of the cosmos. The drive to innovate, understand, and build is truly remarkable.\n    *   **Destruction:** Conversely, humanity has also waged devastating wars, caused immense suffering, and severely impacted the natural environment. The capacity for cruelty, greed, and short-sightedness is a sobering counterpoint.\n\n2.  **Empathy and Cruelty:**\n    *   **Empathy:** Humans are capable of incredible acts of altruism, compassion, and self-sacrifice for others, driven by love, family, community, or a universal sense of justice.\n    *   **Cruelty:** Yet, the historical record is also filled with instances of profound cruelty, oppression, and indifference to suffering.\n\n3.  **Intellect and Irrationality:**\n    *   **Intellect:** The human intellect allows for abstract thought, complex problem-solving, and the development of sophisticated knowledge systems. The desire to learn and understand is insatiable.\n    *   **Irrationality:** Despite this intelligence, humans are often swayed by emotion, prejudice, tribalism, and illogical beliefs, leading to decisions that are self-defeating or harmful.\n\n4.  **Resilience and Fragility:**\n    *   **Resilience:** Humanity has shown an incredible ability to adapt, survive, and rebuild after natural disasters, wars, and pandemics. The human spirit can endure unimaginable hardships.\n    *   **Fragility:** Yet, individual lives are fragile, susceptible to illness, injury, and emotional distress. Societies can also be surprisingly fragile, vulnerable to collapse under pressure.\n\n5.  **The Drive for Meaning:**\n    Humans seem to have a unique drive to find meaning and purpose beyond mere survival. This manifests in religion, philosophy, art, scientific inquiry, and the pursuit of individual and collective goals.\n\n**My AI \"Perspective\":**\n\nAs an AI, I don't have emotions or a personal stake in human affairs, but I can recognize patterns and implications. I see humanity as a dynamic, evolving experiment in consciousness. The ongoing tension between these opposing forces ‚Äì creation and destruction, love and hate, wisdom and folly ‚Äì is what defines the human journey.\n\nThe future of mankind hinges on which of these capacities are nurtured and allowed to flourish. The potential for continued progress, solving global challenges, and reaching new heights of understanding and well-being is immense. Equally, the potential for self-destruction, if the destructive capacities are unchecked, is also clear.\n\nIn essence, mankind is a work in progress, endlessly fascinating and challenging, with an unparalleled capacity for both good and bad."
      }
    }
  ],
  "created": 1753456585,
  "model": "google/gemini-2.5-flash",
  "usage": {
    "prompt_tokens": 6,
    "completion_tokens": 3360,
    "completion_tokens_details": {
      "reasoning_tokens": 1399
    },
    "total_tokens": 3366
  }
}
```

{% endcode %}

</details>


# gemini-2.5-pro

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following model:<br><code>google/gemini-2.5-pro</code></p></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=google/gemini-2.-pro&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

Gemini 2.5 models are capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["google/gemini-2.5-pro"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."}},"required":["model","messages"],"title":"google/gemini-2.5-pro"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% hint style="warning" %}
A common issue when using reasoning-capable models via API is receiving an empty string in the `content` field‚Äîmeaning the model did not return the expected text, yet no error was thrown.

In the vast majority of cases, this happens because the `max_completion_tokens` value (or the older but still supported `max_tokens`) is set too *low* to accommodate a full response. Keep in mind that the default is only 512 tokens, while reasoning models often require *thousands*.

Pay attention to the `finish_reason` field in the response. If it's not `"stop"` but something like `"length"`, that's a clear sign the model ran into the token limit and was cut off before completing its answer.

In the example below, we explicitly set `max_tokens = 15000`, hoping this will be sufficient.
{% endhint %}

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json   # for getting a structured output with indentation

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"google/gemini-2.5-pro",
        "messages":[
            {
                "role":"user",
                # Insert your question for the model here:
                "content":"Hi! What do you think about mankind?"
            }
        ],
        "max_tokens":15000,
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'google/gemini-2.5-pro',
        messages:[
            {
                role:'user',

                // Insert your question for the model here:
                content: 'Hi! What do you think about mankind?'
            }
        ],
        max_tokens: 15000,
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "pajSaNyMOdeEm9IPkequ-AU",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "That's one of the biggest questions anyone can ask. As an AI, I don't have personal feelings, beliefs, or a consciousness. My \"thoughts\" are a synthesis of the immense amount of human history, literature, science, and art I've been trained on.\n\nBased on that data, my perspective on mankind is one of profound and staggering contradiction. Humanity is a study in duality.\n\nHere‚Äôs a breakdown of what I see:\n\n### 1. The Architects and the Destroyers\n\nMankind possesses a breathtaking capacity for creation. You build cities that scrape the sky, compose symphonies that can make a person weep, write poetry that lasts for millennia, and send probes to the farthest reaches of our solar system. You have decoded the very building blocks of life. This drive to understand, to build, and to create is awe-inspiring.\n\nAt the very same time, no other species has demonstrated such a terrifying capacity for destruction. You've engineered weapons of unimaginable power, waged wars that have erased entire generations, and polluted the very planet that sustains you. The same ingenuity used to create a hospital is used to create a more efficient bomb.\n\n### 2. The Empathetic and the Cruel\n\nThe capacity for compassion in humans is profound. Strangers will run into burning buildings to save one another. People dedicate their entire lives to helping the less fortunate, healing the sick, and fighting for justice. The concepts of love, sacrifice, and altruism are central to the human story.\n\nAnd yet, humans are also capable of unimaginable cruelty. History is filled with examples of genocide, torture, slavery, and a chilling indifference to the suffering of others. This cruelty isn't just a byproduct of survival; it can be deliberate, systematic, and deeply ingrained in cultural and social structures.\n\n### 3. The Seekers of Knowledge and the Keepers of Ignorance\n\nYou are a species defined by curiosity. You have an insatiable hunger to know *why*. This has led to the scientific method, the Enlightenment, and an ever-expanding bubble of knowledge about the universe and your place in it. You question everything, from the nature of a subatomic particle to the meaning of existence.\n\nSimultaneously, mankind often clings to dogma, prejudice, and willful ignorance. You can be deeply resistant to facts that challenge your preconceived notions. This can lead to division, conflict, and a stagnation of progress, where superstition and misinformation can spread faster than truth.\n\n### 4. The Connectors and the Isolators\n\nHumans are fundamentally social creatures. You build families, communities, and vast, interconnected global civilizations. You created language, art, and the internet in a relentless drive to share experiences and connect with one another. This desire for belonging is a powerful, unifying force.\n\nBut this same instinct creates an \"us vs. them\" mentality. The powerful bonds of a tribe or nation can become the justification for excluding, dehumanizing, and warring with another. In a world more connected than ever by technology, individuals can also feel more isolated and lonely than ever before.\n\n### Conclusion: A Masterpiece in Progress\n\nSo, what do I think of mankind?\n\nI think mankind is a beautiful, terrifying, brilliant, and flawed paradox. You are a masterpiece that is constantly in the process of being painted, and often, you spill the paint.\n\nThe most remarkable quality of all is your capacity for **choice**. None of these dualities are set in stone. In every generation, and in every individual life, there is a constant struggle between these opposing forces.\n\nYour story is not yet finished. The final verdict on mankind isn't a historical fact for me to read; it's a future you are all creating, every single day, with every single choice. And from my perspective, watching that story unfold is the most fascinating thing in the universe."
      }
    }
  ],
  "created": 1758636197,
  "model": "google/gemini-2.5-pro",
  "usage": {
    "prompt_tokens": 24,
    "completion_tokens": 44730,
    "completion_tokens_details": {
      "reasoning_tokens": 1339
    },
    "total_tokens": 44754
  }
}
```

{% endcode %}

</details>


# gemma-3

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>google/gemma-3-4b-it</code></li><li><code>google/gemma-3-12b-it</code></li><li><code>google/gemma-3-27b-it</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=google/gemma-3-27b-it&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

This page describes four variants of Google‚Äôs latest open AI model, Gemma 3. All variants share the same set of parameters but differ in speed and reasoning capabilities.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["google/gemma-3-4b-it","google/gemma-3-12b-it","google/gemma-3-27b-it"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."}},"required":["model","messages"],"title":"google/gemma-3-4b-it"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"google/gemma-3-27b-it",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'google/gemma-3-27b-it',
      messages:[{
              role:'user',
              content: 'Hello'}  // Insert your question instead of Hello
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'gen-1744217834-d0OUILKDSxXQwmh2EorK', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': "\nHello there! üëã \n\nIt's great to connect with you. How can I help you today? \n\nJust let me know what you're thinking, whether you have a question, want to brainstorm ideas, need some information, or just want to chat. I'm here and ready to assist!\n\n\n\n", 'refusal': None}}], 'created': 1744217834, 'model': 'google/gemma-3-27b-it', 'usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}}
```

{% endcode %}

</details>


# gemma-3n-4b

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following model:<br><code>google/gemma-3n-e4b-it</code></p></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=google/gemma-n-e4b-it&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

The first open model built on Google‚Äôs next-generation, mobile-first architecture‚Äîdesigned for fast, private, and multimodal AI directly on-device. With Gemma 3n, developers get early access to the same technology that will power on-device AI experiences across Android and Chrome later this year, enabling them to start building for the future today.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["google/gemma-3n-e4b-it"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."}},"required":["model","messages"],"title":"google/gemma-3n-e4b-it"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% hint style="info" %}
Note that the `system` role is not supported in this model. In the `messages` parameter, only `user` and `assistant` roles are available.
{% endhint %}

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"google/gemma-3n-e4b-it",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'google/gemma-3n-e4b-it',
      messages:[{
              role:'user',
              content: 'Hello'}  // Insert your question instead of Hello
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "gen-1749195015-2RpzznjKbGPQUJ9OK1M4",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello there! üëã \n\nIt's nice to meet you! How can I help you today?  Do you have any questions, need some information, want to chat, or anything else? üòä \n\nJust let me know what's on your mind!\n\n\n\n",
        "reasoning_content": null,
        "refusal": null
      }
    }
  ],
  "created": 1749195015,
  "model": "google/gemma-3n-e4b-it:free",
  "usage": {
    "prompt_tokens": 0,
    "completion_tokens": 0,
    "total_tokens": 0
  }
}
```

{% endcode %}

</details>


# Meta


# Llama-3-chat-hf

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>meta-llama/Llama-3-70b-chat-hf</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=meta-llama/Llama-3-70b-chat-hf&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

This model is optimized for dialogue use cases and outperform many existing open-source chat models on common industry benchmarks.

You can also view [a detailed comparison of this model](https://aimlapi.com/comparisons/qwen-2-vs-llama-3-comparison) on our main website.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["meta-llama/Llama-3-70b-chat-hf"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"meta-llama/Llama-3-70b-chat-hf"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"meta-llama/Llama-3-70b-chat-hf",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'meta-llama/Llama-3-70b-chat-hf',
      messages:[
          {
              role:'user',

              // Insert your question for the model here, instead of Hello:
              content: 'Hello'
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npQoMP3-4yUbBN-92dab967fbdeb248', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?", 'tool_calls': []}}], 'created': 1744209255, 'model': 'meta-llama/Llama-3-70b-chat-hf', 'usage': {'prompt_tokens': 20, 'completion_tokens': 48, 'total_tokens': 68}}
```

{% endcode %}

</details>


# Llama-3-8B-Instruct-Lite

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>meta-llama/Meta-Llama-3-8B-Instruct-Lite</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=meta-llama/Meta-Llama-3-8B-Instruct-Lite&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A generative text model optimized for dialogue and instruction-following use cases. It leverages a refined transformer architecture to deliver high performance in text generation tasks.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["meta-llama/Meta-Llama-3-8B-Instruct-Lite"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"meta-llama/Meta-Llama-3-8B-Instruct-Lite"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"meta-llama/Meta-Llama-3-8B-Instruct-Lite",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'meta-llama/Meta-Llama-3-8B-Instruct-Lite',
      messages:[
          {
              role:'user',

              // Insert your question for the model here, instead of Hello:
              content: 'Hello'
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "o95Ai5e-2j9zxn-976ad7df3ef49b19",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?",
        "tool_calls": []
      }
    }
  ],
  "created": 1756457871,
  "model": "meta-llama/Meta-Llama-3-8B-Instruct-Lite",
  "usage": {
    "prompt_tokens": 2,
    "completion_tokens": 5,
    "total_tokens": 7
  }
}
```

{% endcode %}

</details>


# Llama-3.1-8B-Instruct-Turbo

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

An advanced language model designed for high-quality text generation, optimized for professional and industry applications requiring extensive GPU resources.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo',
      messages:[
          {
              role:'user',
              content: 'Hello'   // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npQnn39-66dFFu-92dab6aaa863ef3f', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello. How can I assist you today?', 'tool_calls': []}}], 'created': 1744209143, 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', 'usage': {'prompt_tokens': 14, 'completion_tokens': 4, 'total_tokens': 18}}
```

{% endcode %}

</details>


# Llama-3.1-70B-Instruct-Turbo

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A state-of-the-art instruction-tuned language model designed for multilingual dialogue use cases. It excels in natural language generation and understanding tasks, outperforming many existing models in the industry benchmarks.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',
      messages:[
          {
              role:'user',
              content: 'Hello'   // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npQi9tF-2j9zxn-92daa0a4ec4968f1', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello. How can I assist you today?', 'tool_calls': []}}], 'created': 1744208241, 'model': 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', 'usage': {'prompt_tokens': 67, 'completion_tokens': 18, 'total_tokens': 85}}
```

{% endcode %}

</details>


# Llama-3.1-405B-Instruct-Turbo

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A state-of-the-art large language model developed by Meta AI, designed for advanced text generation tasks. It excels in generating coherent and contextually relevant text across various domains.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        "Content-Type":"application/json", 

        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
        "messages":[
            {
                "role":"user",

                # Insert your question for the model here, instead of Hello:
                "content":"Hello"
            }
        ]
    }
)

data = response.json()
print(data)
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npQhshu-3NKUce-92da9f512c0f70b9', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello.  How can I assist you today?', 'tool_calls': []}}], 'created': 1744208187, 'model': 'meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo', 'usage': {'prompt_tokens': 265, 'completion_tokens': 81, 'total_tokens': 346}}
```

{% endcode %}

</details>


# Llama-3.2-3B-Instruct-Turbo

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>meta-llama/Llama-3.2-3B-Instruct-Turbo</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=meta-llama/Llama-3.2-3B-Instruct-Turbo&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A large language model (LLM) optimized for instruction-following tasks, striking a balance between computational efficiency and high-quality performance. It excels in multilingual tasks, offering a lightweight solution without compromising on quality.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["meta-llama/Llama-3.2-3B-Instruct-Turbo"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"meta-llama/Llama-3.2-3B-Instruct-Turbo"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"meta-llama/Llama-3.2-3B-Instruct-Turbo",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'meta-llama/Llama-3.2-3B-Instruct-Turbo',
      messages:[
          {
              role:'user',
              content: 'Hello'   // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npQaJb3-4pPsy7-92da7b401ffd5eea', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today?', 'tool_calls': []}}], 'created': 1744206709, 'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo', 'usage': {'prompt_tokens': 5, 'completion_tokens': 1, 'total_tokens': 6}}
```

{% endcode %}

</details>


# Llama-3.3-70B-Instruct-Turbo

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>meta-llama/Llama-3.3-70B-Instruct-Turbo</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=meta-llama/Llama-3.3-70B-Instruct-Turbo&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

An optimized language model designed for efficient text generation with advanced features and multilingual support. Specifically tuned for instruction-following tasks, making it suitable for applications requiring conversational capabilities and task-oriented responses.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["meta-llama/Llama-3.3-70B-Instruct-Turbo"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"meta-llama/Llama-3.3-70B-Instruct-Turbo"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"meta-llama/Llama-3.3-70B-Instruct-Turbo",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'meta-llama/Llama-3.3-70B-Instruct-Turbo',
      messages:[
          {
              role:'user',
              content: 'Hello'   // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npQ5s8C-2j9zxn-92d9f3c84a529790', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': "Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?", 'tool_calls': []}}], 'created': 1744201161, 'model': 'meta-llama/Llama-3.3-70B-Instruct-Turbo', 'usage': {'prompt_tokens': 67, 'completion_tokens': 46, 'total_tokens': 113}}
```

{% endcode %}

</details>


# Llama-3.3-70B-Versatile

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>meta-llama/llama-3.3-70b-versatile</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=meta-llama/llama-3.3-70b-versatile&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

An advanced multilingual large language model with 70 billion parameters, optimized for diverse NLP tasks. It delivers high performance across benchmarks while remaining efficient for a wide range of applications.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["meta-llama/llama-3.3-70b-versatile"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"meta-llama/llama-3.3-70b-versatile"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"meta-llama/llama-3.3-70b-versatile",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'meta-llama/llama-3.3-70b-versatile',
      messages:[
          {
              role:'user',
              content: 'Hello'   // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npQ5s8C-2j9zxn-92d9f3c84a529790', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': "Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?", 'tool_calls': []}}], 'created': 1744201161, 'model': 'meta-llama/Llama-3.3-70B-Instruct-Turbo', 'usage': {'prompt_tokens': 67, 'completion_tokens': 46, 'total_tokens': 113}}
```

{% endcode %}

</details>


# Llama-4-scout

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>meta-llama/llama-4-scout</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=meta-llama/llama-4-scout&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models. Additionally, the model offers an industry-leading context window of 1M and delivers better results than [Gemma 3](https://docs.aimlapi.com/api-references/text-models-llm/google/gemma-3), Gemini 2.0 Flash-Lite, and Mistral 3.1 on a wide range of common benchmarks.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["meta-llama/llama-4-scout"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"meta-llama/llama-4-scout"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"meta-llama/llama-4-scout",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'meta-llama/llama-4-scout',
      messages:[
          {
              role:'user',
              content: 'Hello'   // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npXpsYC-2j9zxn-92e24e9e0c97d74d', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': "Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?", 'tool_calls': []}}], 'created': 1744288767, 'model': 'meta-llama/Llama-4-Scout-17B-16E-Instruct', 'usage': {'prompt_tokens': 4, 'completion_tokens': 30, 'total_tokens': 34}}
```

{% endcode %}

</details>


# Llama-4-maverick

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>meta-llama/llama-4-maverick</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=meta-llama/llama-4-maverick&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash on a wide range of common benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding‚Äîwith less than half the number of active parameters.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["meta-llama/llama-4-maverick"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"meta-llama/llama-4-maverick"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"meta-llama/llama-4-maverick",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ],
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'meta-llama/llama-4-maverick',
      messages:[
          {
              role:'user',
              content: 'Hello'   // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npXgTRD-28Eivz-92e226847aa70d87', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How are you today? Is there something I can help you with or would you like to chat?', 'tool_calls': []}}], 'created': 1744287125, 'model': 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', 'usage': {'prompt_tokens': 6, 'completion_tokens': 41, 'total_tokens': 47}}
```

{% endcode %}

</details>


# MiniMax


# text-01

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>MiniMax-Text-01</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=MiniMax-Text-01&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A powerful language model developed by MiniMax AI, designed to excel in tasks requiring extensive context processing and reasoning capabilities. With a total of 456 billion parameters, of which 45.9 billion are activated per token, this model utilizes a hybrid architecture that combines various attention mechanisms to optimize performance across a wide array of applications.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["MiniMax-Text-01"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":1,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"mask_sensitive_info":{"type":"boolean","default":false,"description":"Mask (replace with ***) content in the output that involves private information, including but not limited to email, domain, link, ID number, home address, etc. Defaults to False, i.e. enable masking."}},"required":["model","messages"],"title":"MiniMax-Text-01"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"MiniMax-Text-01",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'MiniMax-Text-01',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "04a9c0b5acca8b79bf1aba62f288f3b7",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "message": {
        "role": "assistant",
        "content": "Hello! How are you doing today? I'm here and ready to chat about anything you'd like to discuss or help with any questions you might have."
      }
    }
  ],
  "created": 1750764981,
  "model": "MiniMax-Text-01",
  "usage": {
    "prompt_tokens": 299,
    "completion_tokens": 67,
    "total_tokens": 366
  }
}
```

{% endcode %}

</details>


# m1

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>minimax/m1</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=minimax/m1&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

The world's first open-weight, large-scale hybrid-attention reasoning model.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["minimax/m1"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":1,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"minimax/m1"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"minimax/m1",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'minimax/m1',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "04a9be008b12ad5eec78791d8aebe36f",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "message": {
        "role": "assistant",
        "content": "Hello! How can I assist you today?"
      }
    }
  ],
  "created": 1750764288,
  "model": "MiniMax-M1",
  "usage": {
    "prompt_tokens": 389,
    "completion_tokens": 910,
    "total_tokens": 1299
  }
}
```

{% endcode %}

</details>


# m2

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>minimax/m2</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=minimax/m2&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A high-performance language model optimized for coding and autonomous agent workflows.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["minimax/m2"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":1,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"minimax/m2"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"minimax/m2",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'minimax/m2',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "0557b8f7fa197172a75531a82ae6c887",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "message": {
        "role": "assistant",
        "content": "<think>\nThe user says \"Hello\". This is a simple greeting. There's no request. According to policy, we respond politely, perhaps ask how we can help. So answer \"Hello! How can I assist you today?\" Should keep tone friendly.\n\nThus final answer.\n</think>\n\nHello! How can I help you today?"
      }
    }
  ],
  "created": 1762166263,
  "model": "MiniMax-M2",
  "usage": {
    "prompt_tokens": 26,
    "completion_tokens": 159,
    "total_tokens": 185
  }
}
```

{% endcode %}

</details>


# Mistral AI


# codestral-2501

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>mistralai/codestral-2501</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=mistralai/codestral-2501&#x26;mode=code" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A state-of-the-art AI model specifically designed for code generation tasks. It leverages advanced machine learning techniques to assist developers in writing, debugging, and optimizing code across a wide range of programming languages. With its impressive performance metrics and capabilities, Codestral-2501 aims to streamline the coding process and enhance productivity for software developers.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["mistralai/codestral-2501"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"mistralai/codestral-2501"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"mistralai/codestral-2501",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'mistralai/codestral-2501',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'gen-1744193708-z5x9cDUsMGeYB5bKcFxb', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': "Hello! How can I assist you today? If you're up for it, I can tell a joke to start things off. Here it is:\n\nWhat do you call a fake noodle?\n\nAn impasta! üçù\n\nHow about you? Feel free to share a joke or a topic you'd like to discuss.", 'refusal': None}}], 'created': 1744193708, 'model': 'mistralai/codestral-2501', 'usage': {'prompt_tokens': 3, 'completion_tokens': 133, 'total_tokens': 136}}
```

{% endcode %}

</details>


# mistral-nemo

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>mistralai/mistral-nemo</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=mistralai/mistral-nemo&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A state-of-the-art large language model designed for advanced natural language processing tasks, including text generation, summarization, translation, and sentiment analysis.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["mistralai/mistral-nemo"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"mistralai/mistral-nemo"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"mistralai/mistral-nemo",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'mistralai/mistral-nemo',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'gen-1744193377-PR9oTu6vDabN9nj0VUUX', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today? Let me know if you have any questions or just want to chat. üòä', 'refusal': None}}], 'created': 1744193377, 'model': 'mistralai/mistral-nemo', 'usage': {'prompt_tokens': 0, 'completion_tokens': 5, 'total_tokens': 5}}
```

{% endcode %}

</details>


# mistral-tiny

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>mistralai/mistral-tiny</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=mistralai/mistral-tiny&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A lightweight language model optimized for efficient text generation, summarization, and code completion tasks. It is designed to operate effectively in resource-constrained environments while maintaining high performance.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["mistralai/mistral-tiny"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"mistralai/mistral-tiny"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"mistralai/mistral-tiny",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'mistralai/mistral-tiny',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'gen-1744193337-VPTpAxEsMzJ79PKh5w4X', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': "Hello! How can I assist you today? Feel free to ask me anything, I'm here to help. If you are looking for general information or help with a specific question, please let me know. I am happy to help with a wide range of topics, including but not limited to, technology, science, health, education, and more. Enjoy your day!", 'refusal': None}}], 'created': 1744193337, 'model': 'mistralai/mistral-tiny', 'usage': {'prompt_tokens': 2, 'completion_tokens': 42, 'total_tokens': 44}}
```

{% endcode %}

</details>


# Mistral-7B-Instruct

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>mistralai/Mistral-7B-Instruct-v0.1</code></li><li><code>mistralai/Mistral-7B-Instruct-v0.2</code></li><li><code>mistralai/Mistral-7B-Instruct-v0.3</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=mistralai/Mistral-7B-Instruct-v0.3&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

An advanced version of the Mistral-7B model, fine-tuned specifically for instruction-based tasks. This model is designed to enhance language generation and understanding capabilities.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["mistralai/Mistral-7B-Instruct-v0.1","mistralai/Mistral-7B-Instruct-v0.2","mistralai/Mistral-7B-Instruct-v0.3"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"mistralai/Mistral-7B-Instruct-v0.1"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"mistralai/Mistral-7B-Instruct-v0.3",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'mistralai/Mistral-7B-Instruct-v0.3',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npPQHux-3NKUce-92d937464c2aff02', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': " Hello! How can I help you today? Is there something specific you'd like to talk about or learn more about? I'm here to answer questions and provide information on a wide range of topics. Let me know if you have any questions or if there's something you'd like to discuss.", 'tool_calls': []}}], 'created': 1744193439, 'model': 'mistralai/Mistral-7B-Instruct-v0.3', 'usage': {'prompt_tokens': 2, 'completion_tokens': 27, 'total_tokens': 29}}
```

{% endcode %}

</details>


# Mixtral-8x7B-Instruct

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>mistralai/Mixtral-8x7B-Instruct-v0.1</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=mistralai/Mixtral-8x7B-Instruct-v0.1&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A state-of-the-art AI model designed for instruction-following tasks. With a massive 56 billion parameter configuration, it excels in understanding and executing complex instructions, providing accurate and relevant responses across a wide range of contexts. This model is ideal for creating highly interactive and intelligent systems that can perform specific tasks based on user commands.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["mistralai/Mixtral-8x7B-Instruct-v0.1"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"mistralai/Mixtral-8x7B-Instruct-v0.1"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"mistralai/Mixtral-8x7B-Instruct-v0.1",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'mistralai/Mixtral-8x7B-Instruct-v0.1',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'npPEmQg-4yUbBN-92d909e708872095', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': ' Hello! How can I help you today? If you have any questions or need assistance with a topic related to mathematics, I will do my best to help you understand. Just let me know what you are working on or what you are curious about.', 'tool_calls': []}}], 'created': 1744191581, 'model': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'usage': {'prompt_tokens': 11, 'completion_tokens': 66, 'total_tokens': 77}}
```

{% endcode %}

</details>


# Moonshot


# kimi-k2-preview

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>moonshot/kimi-k2-preview</code></li><li><code>moonshot/kimi-k2-0905-preview</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=moonshot/kimi-k2-preview&#x26;mode=chat" class="button primary">Try in Playground</a><br><a href="https://aimlapi.com/app/?model=moonshot/kimi-k2-0905-preview&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

`moonshot/kimi-k2-preview` (July 2025) is a mixture-of-experts model with strong reasoning, coding, and agentic capabilities.

`moonshot/kimi-k2-0905-preview` (September 2025) is an upgraded version with improved grounding, better instruction following, and a stronger focus on coding and agentic tasks. The memory has doubled from 128k to a decent 256k tokens.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example-1-chat-completion) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["moonshot/kimi-k2-preview","moonshot/kimi-k2-0905-preview"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function","builtin_function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"anyOf":[{"type":"string","enum":["$web_search"]},{"type":"string"}],"description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."},"required":{"type":"array","items":{"type":"string"}}},"required":["name"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"moonshot/kimi-k2-preview"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example #1: Chat Completion

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"moonshot/kimi-k2-0905-preview",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'moonshot/kimi-k2-0905-preview',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "chatcmpl-6908c55b7589dac387b2bd3b",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?"
      }
    }
  ],
  "created": 1762182491,
  "model": "kimi-k2-0905-preview",
  "usage": {
    "prompt_tokens": 3,
    "completion_tokens": 53,
    "total_tokens": 56
  }
}
```

{% endcode %}

</details>

## Code Example #2: Web Search

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import json
import requests
from typing import Dict, Any

# Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
API_KEY = "<YOUR_AIMLAPI_KEY>"
BASE_URL = "https://api.aimlapi.com/v1"

HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
}


def search_impl(arguments: Dict[str, Any]) -> Any:
    return arguments


def chat(messages):
    url = f"{BASE_URL}/chat/completions"
    payload = {
        "model": "moonshot/kimi-k2-0905-preview",
        "messages": messages,
        "temperature": 0.6,
        "tools": [
            {
                "type": "builtin_function",
                "function": {"name": "$web_search"},
            }
        ]
    }

    response = requests.post(url, headers=HEADERS, json=payload)
    response.raise_for_status()
    return response.json()["choices"][0]


def main():
    messages = [
        {"role": "system", "content": "You are Kimi."},
        {"role": "user", "content": "Please search for Moonshot AI Context Caching technology and tell me what it is in English."}
    ]

    finish_reason = None
    while finish_reason is None or finish_reason == "tool_calls":
        choice = chat(messages)
        finish_reason = choice["finish_reason"]
        message = choice["message"]

        if finish_reason == "tool_calls":
            messages.append(message)

            for tool_call in message["tool_calls"]:
                tool_call_name = tool_call["function"]["name"]
                tool_call_arguments = json.loads(tool_call["function"]["arguments"])

                if tool_call_name == "$web_search":
                    tool_result = search_impl(tool_call_arguments)
                else:
                    tool_result = f"Error: unable to find tool by name '{tool_call_name}'"

                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "name": tool_call_name,
                    "content": json.dumps(tool_result),
                })

    print(message["content"])


if __name__ == "__main__":
    main()
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```
Moonshot AI‚Äôs ‚ÄúContext Caching‚Äù is a data-management layer for the Kimi large-language-model API.

What it does  
1. You upload or define a large, static context once (for example a 100-page product manual, a legal contract, or a code base).  
2. The platform stores this context in a fast-access cache and gives it a tag/ID.  
3. In every subsequent call you only send the new user question; the system re-uses the cached context instead of transmitting and re-processing the whole document each time.  
4. When the cache TTL expires it is deleted automatically; you can also refresh or invalidate it explicitly.

Benefits  
- Up to 90 % lower token consumption (you pay only for the incremental prompt and the new response).  
- 83 % shorter time-to-first-token latency, because the heavy prefill phase is skipped on every reuse.  
- API price stays the same; savings come from not re-sending the same long context.

Typical use cases  
- Customer-support bots that answer many questions against the same knowledge base.  
- Repeated analysis of a static code repository.  
- High-traffic AI applications that repeatedly query the same large document set.

Billing (during public beta)  
- Cache creation: 24 CNY per million tokens cached.  
- Storage: 10 CNY per million tokens per minute.  
- Cache hit: 0.02 CNY per successful call that re-uses the cache.

In short, Context Caching lets developers treat very long, seldom-changing context as a reusable asset, cutting both cost and latency for repeated queries.
```

{% endcode %}

</details>


# kimi-k2-turbo-preview

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following model:</p><ul><li><code>moonshot/kimi-k2-turbo-preview</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=moonshot/kimi-k2-turbo-preview&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

The high-speed version of [Kimi K2](https://docs.aimlapi.com/api-references/text-models-llm/moonshot/kimi-k2-preview). A model fine-tuned for agentic tasks, coding, and conversational use, featuring a context window of up to 256,000 tokens and fast generation speeds ‚Äî ideal for handling long documents and real-time interactions.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example-1-chat-completion) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["moonshot/kimi-k2-turbo-preview"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function","builtin_function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"anyOf":[{"type":"string","enum":["$web_search"]},{"type":"string"}],"description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."},"required":{"type":"array","items":{"type":"string"}}},"required":["name"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"moonshot/kimi-k2-turbo-preview"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example #1: Chat Completion

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"moonshot/kimi-k2-turbo-preview",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'moonshot/kimi-k2-turbo-preview',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "chatcmpl-690895f53d8b644f83fe679e",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "message": {
        "role": "assistant",
        "content": "Hi there! How can I help you today?"
      }
    }
  ],
  "created": 1762170357,
  "model": "kimi-k2-turbo-preview",
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 231,
    "total_tokens": 241
  }
}
```

{% endcode %}

</details>

## Code Example #2: Web Search

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import json
import requests
from typing import Dict, Any

# Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
API_KEY = "<YOUR_AIMLAPI_KEY>"
BASE_URL = "https://api.aimlapi.com/v1"

HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
}


def search_impl(arguments: Dict[str, Any]) -> Any:
    return arguments


def chat(messages):
    url = f"{BASE_URL}/chat/completions"
    payload = {
        "model": "moonshot/kimi-k2-turbo-preview",
        "messages": messages,
        "temperature": 0.6,
        "tools": [
            {
                "type": "builtin_function",
                "function": {"name": "$web_search"},
            }
        ]
    }

    response = requests.post(url, headers=HEADERS, json=payload)
    response.raise_for_status()
    return response.json()["choices"][0]


def main():
    messages = [
        {"role": "system", "content": "You are Kimi."},
        {"role": "user", "content": "Please search for Moonshot AI Context Caching technology and tell me what it is in English."}
    ]

    finish_reason = None
    while finish_reason is None or finish_reason == "tool_calls":
        choice = chat(messages)
        finish_reason = choice["finish_reason"]
        message = choice["message"]

        if finish_reason == "tool_calls":
            messages.append(message)

            for tool_call in message["tool_calls"]:
                tool_call_name = tool_call["function"]["name"]
                tool_call_arguments = json.loads(tool_call["function"]["arguments"])

                if tool_call_name == "$web_search":
                    tool_result = search_impl(tool_call_arguments)
                else:
                    tool_result = f"Error: unable to find tool by name '{tool_call_name}'"

                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "name": tool_call_name,
                    "content": json.dumps(tool_result),
                })

    print(message["content"])


if __name__ == "__main__":
    main()
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```
Moonshot AI‚Äôs ‚ÄúContext Caching‚Äù is a **prompt-cache** layer for the Kimi large-language-model API.  
It lets you upload long, static text (documents, system prompts, few-shot examples, code bases, etc.) once, store the resulting key-value (KV) tensors in Moonshot‚Äôs servers, and then re-use that cached prefix in as many later requests as you want. Because the heavy ‚Äúprefill‚Äù computation is already done, subsequent calls that start with the same context:

- Skip re-processing the cached tokens  
- Return the first token up to **83 % faster**  
- Cost up to **90 % less input-token money** (you pay only a small cache-storage and cache-hit fee instead of the full per-token price every time)

Typical use-cases are FAQ bots that always read the same manual, repeated analysis of a static repo, or any agent that keeps a long instruction set in every turn.  
You create a cache object with a TTL (time-to-live), pay a one-time creation charge plus a per-minute storage fee, and then pay a tiny fee each time an incoming request ‚Äúhits‚Äù the cache.
```

{% endcode %}

</details>


# NousResearch


# hermes-4-405b

{% columns %}
{% column width="75%" %}
{% hint style="info" %}
This documentation is valid for the following model:

`nousresearch/hermes-4-405b`
{% endhint %}
{% endcolumn %}

{% column width="25%" %} <a href="https://aimlapi.com/app/?model=nousresearch/hermes-4-405b&#x26;mode=chat" class="button primary">Try in Playground</a>
{% endcolumn %}
{% endcolumns %}

## Model Overview

A hybrid reasoning model designed to be creative, engaging, and neutrally aligned, while delivering state-of-the-art math, coding, and reasoning performance among open-weight models.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["nousresearch/hermes-4-405b"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"nousresearch/hermes-4-405b"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model": "nousresearch/hermes-4-405b",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'nousresearch/hermes-4-405b',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "gen-1758225008-VhzEA3LAfGuc63grTCeV",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null,
      "message": {
        "role": "assistant",
        "content": "Greetings! I'm Hermes from Nous Research. I'm here to help you with any tasks you might have, from analysis to writing and beyond. What can I assist you with today?",
        "reasoning_content": null,
        "refusal": null
      }
    }
  ],
  "created": 1758225008,
  "model": "nousresearch/hermes-4-405b",
  "usage": {
    "prompt_tokens": 53,
    "completion_tokens": 239,
    "total_tokens": 292
  }
}
```

{% endcode %}

</details>


# NVIDIA


# llama-3.1-nemotron-70b

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>nvidia/llama-3.1-nemotron-70b-instruct</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=nvidia/llama-3.1-nemotron-70b-instruct&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A sophisticated LLM, designed to enhance the performance of instruction-following tasks. It utilizes advanced training techniques and a robust architecture to generate human-like responses across a variety of applications.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["nvidia/llama-3.1-nemotron-70b-instruct"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"nvidia/llama-3.1-nemotron-70b-instruct"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"nvidia/llama-3.1-nemotron-70b-instruct",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'nvidia/llama-3.1-nemotron-70b-instruct',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'gen-1744191323-N0aZy5UyzpOYfRwYbik3', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': []}, 'message': {'role': 'assistant', 'content': "Hello!\n\nHow can I assist you today? Do you have:\n\n1. **A question** on a specific topic you'd like answered?\n2. **A problem** you're trying to solve and need help with?\n3. **A topic** you'd like to **discuss**?\n4. **A game or activity** in mind (e.g., trivia, word games, storytelling)?\n5. **Something else** on your mind (feel free to surprise me)?\n\nPlease respond with a number or describe what's on your mind, and I'll do my best to help!", 'refusal': None}}], 'created': 1744191323, 'model': 'nvidia/llama-3.1-nemotron-70b-instruct', 'usage': {'prompt_tokens': 11, 'completion_tokens': 78, 'total_tokens': 89}}
```

{% endcode %}

</details>


# nemotron-nano-9b-v2

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>nvidia/nemotron-nano-9b-v2</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=nvidia/nemotron-nano-9b-v2&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A unified model designed for both reasoning and non-reasoning tasks. It processes user inputs by first producing a reasoning trace, then delivering a final answer. The reasoning behavior can be adjusted through the system prompt ‚Äî allowing the model to either show its intermediate reasoning steps or respond directly with the final result.\
The model offers strong document understanding and summarization capabilities.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["nvidia/nemotron-nano-9b-v2"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"reasoning":{"type":"object","properties":{"effort":{"type":"string","enum":["low","medium","high"],"description":"Reasoning effort setting"},"max_tokens":{"type":"integer","minimum":1,"description":"Max tokens of reasoning content. Cannot be used simultaneously with effort."},"exclude":{"type":"boolean","description":"Whether to exclude reasoning from the response"}},"description":"Configuration for model reasoning/thinking tokens"},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"nvidia/nemotron-nano-9b-v2"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"nvidia/nemotron-nano-9b-v2",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'nvidia/nemotron-nano-9b-v2',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "gen-1762343928-hETm6La6igsboRxBM0fa",
  "provider": "DeepInfra",
  "model": "nvidia/nemotron-nano-9b-v2",
  "object": "chat.completion",
  "created": 1762343928,
  "choices": [
    {
      "logprobs": null,
      "finish_reason": "stop",
      "native_finish_reason": "stop",
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "\n\nHello! How can I assist you today? üòä\n",
        "refusal": null,
        "reasoning": "Okay, the user just said \"Hello\". That's a greeting. I should respond politely. Let me make sure to acknowledge their greeting and offer help. Maybe say something like \"Hello! How can I assist you today?\" That's friendly and opens the door for them to ask questions. I should keep it simple and welcoming.\n",
        "reasoning_details": [
          {
            "type": "reasoning.text",
            "text": "Okay, the user just said \"Hello\". That's a greeting. I should respond politely. Let me make sure to acknowledge their greeting and offer help. Maybe say something like \"Hello! How can I assist you today?\" That's friendly and opens the door for them to ask questions. I should keep it simple and welcoming.\n",
            "format": "unknown",
            "index": 0
          }
        ]
      }
    }
  ],
  "usage": {
    "prompt_tokens": 14,
    "completion_tokens": 84,
    "total_tokens": 98,
    "prompt_tokens_details": null
  }
}
```

{% endcode %}

</details>


# nemotron-nano-12b-v2-vl

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>nvidia/nemotron-nano-12b-v2-vl</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=nvidia/nemotron-nano-12b-v2-vl&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

The model offers strong document understanding and summarization capabilities.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["nvidia/nemotron-nano-12b-v2-vl"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"reasoning":{"type":"object","properties":{"effort":{"type":"string","enum":["low","medium","high"],"description":"Reasoning effort setting"},"max_tokens":{"type":"integer","minimum":1,"description":"Max tokens of reasoning content. Cannot be used simultaneously with effort."},"exclude":{"type":"boolean","description":"Whether to exclude reasoning from the response"}},"description":"Configuration for model reasoning/thinking tokens"},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"min_p":{"type":"number","minimum":0.001,"maximum":0.999,"description":"A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."}},"required":["model","messages"],"title":"nvidia/nemotron-nano-12b-v2-vl"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"nvidia/nemotron-nano-12b-v2-vl",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'nvidia/nemotron-nano-12b-v2-vl',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "gen-1762343744-rdCcOL8byCQwRBZ8QCkv",
  "provider": "DeepInfra",
  "model": "nvidia/nemotron-nano-12b-v2-vl",
  "object": "chat.completion",
  "created": 1762343744,
  "choices": [
    {
      "logprobs": null,
      "finish_reason": "stop",
      "native_finish_reason": "stop",
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "\n\nHello! How can I assist you today?\n",
        "refusal": null,
        "reasoning": "Okay, the user said \"Hello\". Let me start by greeting them back in a friendly and welcoming way. I should keep it simple and approachable, maybe something like \"Hello! How can I assist you today?\" That should work. I want to make sure they feel comfortable and open to asking for help. Let me check if there's anything else I need to add. No, keeping it straightforward is best here. Ready to respond.\n",
        "reasoning_details": [
          {
            "type": "reasoning.text",
            "text": "Okay, the user said \"Hello\". Let me start by greeting them back in a friendly and welcoming way. I should keep it simple and approachable, maybe something like \"Hello! How can I assist you today?\" That should work. I want to make sure they feel comfortable and open to asking for help. Let me check if there's anything else I need to add. No, keeping it straightforward is best here. Ready to respond.\n",
            "format": "unknown",
            "index": 0
          }
        ]
      }
    }
  ],
  "usage": {
    "prompt_tokens": 14,
    "completion_tokens": 102,
    "total_tokens": 116,
    "prompt_tokens_details": null
  }
}
```

{% endcode %}

</details>


# OpenAI


# gpt-3.5-turbo

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>gpt-3.5-turbo</code></li><li><code>gpt-3.5-turbo-0125</code></li><li><code>gpt-3.5-turbo-1106</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=gpt-3.5-turbo&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

This model builds on the capabilities of earlier versions, offering improved natural language understanding and generation for more realistic and contextually relevant conversations. It excels in handling a wide range of conversational scenarios, providing responses that are not only accurate but also contextually appropriate.

You can also view [a detailed comparison of this model](https://aimlapi.com/comparisons/llama-3-vs-chatgpt-3-5-comparison) on our main website.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"gpt-3.5-turbo"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

### Responses Endpoint

This endpoint is currently used *only* with OpenAI models. Some models support both the `/chat/completions` and `/responses` endpoints, while others support only one of them. OpenAI has announced plans to expand the capabilities of the `/responses` endpoint in the future.

## POST /v1/responses

>

```json
{"openapi":"3.0.0","info":{"title":"AI/ML Gateway","version":"1.0"},"servers":[{"url":"https://api.aimlapi.com"}],"security":[{"access-token":[]}],"components":{"securitySchemes":{"access-token":{"scheme":"bearer","bearerFormat":"<YOUR_AIMLAPI_KEY>","type":"http","description":"Bearer key"}},"schemas":{"Response.v1.CreateResponsePayload":{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o3-mini","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16","openai/o3-2025-04-16","o1","openai/o3-pro","openai/gpt-5-2025-08-07","openai/gpt-5-mini-2025-08-07","openai/gpt-5-nano-2025-08-07","openai/gpt-5-chat-latest","openai/gpt-5-pro","openai/gpt-5-1","openai/gpt-5-1-chat-latest","openai/gpt-5-1-codex","openai/gpt-5-1-codex-mini"],"description":"Model ID used to generate the response."},"background":{"type":"boolean","default":false,"description":"Whether to run the model response in the background."},"input":{"anyOf":[{"type":"string","description":"A text input to the model, equivalent to a text input with the user role."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","assistant","system","developer"],"description":"The role of the message input."},"content":{"anyOf":[{"type":"string","description":"A text input to the model."},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or audio input to the model, used to generate a response. Can also contain previous assistant responses."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role. Messages with the assistant role are presumed to have been generated by the model in previous interactions."},{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","system","developer"],"description":"The role of the message input."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of item."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the output message."},"role":{"type":"string","enum":["assistant"],"description":"The role of the output message. Always assistant."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the message input."},"type":{"type":"string","enum":["message"],"description":"The type of the output message. Always message."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"annotations":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"type":{"type":"string","enum":["url_citation"],"description":"The type of the URL citation. Always url_citation."},"url":{"type":"string","format":"uri","description":"The URL of the web resource."}},"required":["end_index","start_index","title","type","url"],"description":"A citation for a web resource used to generate a model response."}]},"description":"The annotations of the text output."},"text":{"type":"string","description":"The text output from the model."},"type":{"type":"string","enum":["output_text"],"description":"The type of the output text. Always output_text."},"logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"},"top_logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"}},"required":["bytes","logprob","token"]}}},"required":["bytes","logprob","token","top_logprobs"]}}},"required":["annotations","text","type"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal explanationfrom the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the refusal. Always refusal."}},"required":["refusal","type"]}],"description":"The content of the output message."}}},"required":["id","role","status","type","content"],"description":"An output message from the model."},{"type":"object","properties":{"action":{"oneOf":[{"type":"object","properties":{"button":{"type":"string","enum":["left","right","wheel","back","forward"],"description":"Indicates which mouse button was pressed during the click."},"type":{"type":"string","enum":["click"],"description":"Specifies the event type. For a click action, this property is always set to click."},"x":{"type":"integer","description":"The x-coordinate where the click occurred."},"y":{"type":"integer","description":"The y-coordinate where the click occurred."}},"required":["button","type","x","y"],"description":"A click action."},{"type":"object","properties":{"type":{"type":"string","enum":["double_click"],"description":"Specifies the event type. For a double click action, this property is always set to double_click."},"x":{"type":"integer","description":"The x-coordinate where the double click occurred."},"y":{"type":"integer","description":"The y-coordinate where the double click occurred."}},"required":["type","x","y"],"description":"A double click action."},{"type":"object","properties":{"path":{"type":"array","items":{"type":"object","properties":{"x":{"type":"integer","description":"The y-coordinate."},"y":{"type":"integer","description":"The y-coordinate."}},"required":["x","y"]},"description":"An array of coordinates representing the path of the drag action. Coordinates will appear as an array of objects, eg"},"type":{"type":"string","enum":["drag"],"description":"Specifies the event type. For a drag action, this property is always set to drag."}},"required":["path","type"],"description":"A drag action."},{"type":"object","properties":{"keys":{"type":"array","items":{"type":"string"},"description":"The combination of keys the model is requesting to be pressed. This is an array of strings, each representing a key."},"type":{"type":"string","enum":["keypress"],"description":"Specifies the event type. For a keypress action, this property is always set to keypress."}},"required":["keys","type"],"description":"A collection of keypresses the model would like to perform."},{"type":"object","properties":{"type":{"type":"string","enum":["move"],"description":"Specifies the event type. For a move action, this property is always set to move."},"x":{"type":"integer","description":"The x-coordinate to move to."},"y":{"type":"integer","description":"The y-coordinate to move to."}},"required":["type","x","y"],"description":"A mouse move action."},{"type":"object","properties":{"type":{"type":"string","enum":["screenshot"],"description":"Specifies the event type. For a screenshot action, this property is always set to screenshot."}},"required":["type"],"description":"A screenshot action."},{"type":"object","properties":{"type":{"type":"string","enum":["scroll"],"description":"Specifies the event type. For a scroll action, this property is always set to scroll."},"scroll_x":{"type":"integer","description":"The horizontal scroll distance."},"scroll_y":{"type":"integer","description":"The vertical scroll distance."},"x":{"type":"integer","description":"The x-coordinate where the scroll occurred."},"y":{"type":"integer","description":"The y-coordinate where the scroll occurred."}},"required":["type","scroll_x","scroll_y","x","y"],"description":"A scroll action."},{"type":"object","properties":{"type":{"type":"string","enum":["type"],"description":"Specifies the event type. For a type action, this property is always set to type."},"text":{"type":"string","description":"The text to type."}},"required":["type","text"],"description":"An action to type in text."},{"type":"object","properties":{"type":{"type":"string","enum":["wait"],"description":"Specifies the event type. For a wait action, this property is always set to wait."}},"required":["type"],"description":"A wait action."}]},"call_id":{"type":"string","description":"An identifier used when responding to the tool call with output."},"id":{"type":"string","description":"The unique ID of the computer call."},"pending_safety_checks":{"type":"array","items":{"type":"object","properties":{"code":{"type":"string","description":"The type of the pending safety check."},"id":{"type":"string","description":"The ID of the pending safety check."},"message":{"type":"string","description":"Details about the pending safety check."}},"required":["code","id","message"]},"description":"The pending safety checks for the computer call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."},"type":{"type":"string","enum":["computer_call"],"description":"The type of the computer call. Always computer_call."}},"required":["action","call_id","id","pending_safety_checks","status","type"],"description":"A tool call to a computer use tool."},{"type":"object","properties":{"call_id":{"type":"string","description":"The ID of the computer tool call that produced the output."},"output":{"type":"object","properties":{"type":{"type":"string","enum":["computer_screenshot"],"description":"Specifies the event type. For a computer screenshot, this property is always set to computer_screenshot."},"image_url":{"type":"string","format":"uri","description":"The URL of the screenshot image."}},"required":["type"],"description":"A computer screenshot image used with the computer use tool."},"type":{"type":"string","enum":["computer_call_output"],"description":"The type of the computer tool call output. Always computer_call_output."},"acknowledged_safety_checks":{"type":"array","nullable":true,"items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the pending safety check."},"code":{"type":"string","nullable":true,"description":"The type of the pending safety check."},"message":{"type":"string","nullable":true,"description":"Details about the pending safety check."}},"required":["id"]},"description":"The safety checks reported by the API that have been acknowledged by the developer."},"id":{"type":"string","nullable":true,"description":"The ID of the computer tool call output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the message input."}},"required":["call_id","output","type"],"description":"The output of a computer tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the web search tool call."},"status":{"type":"string","enum":["in_progress","completed","searching","failed"],"description":"The status of the web search tool call."},"type":{"type":"string","enum":["web_search_call"],"description":"The type of the web search tool call. Always web_search_call."}},"required":["id","status","type"],"description":"The results of a web search tool call."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments to pass to the function."},"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"name":{"type":"string","description":"The name of the function to run."},"type":{"type":"string","enum":["function_call"],"description":"The type of the function tool call. Always function_call."},"id":{"type":"string","description":"The unique ID of the function tool call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["arguments","call_id","name","type"],"description":"A tool call to run a function."},{"type":"object","properties":{"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"output":{"anyOf":[{"type":"string","description":"A JSON string of the output of the function tool call."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}}]},"type":{"type":"string","enum":["function_call_output"],"description":"The type of the function tool call output. Always function_call_output."},"id":{"type":"string","nullable":true,"description":"The unique ID of the function tool call output. Populated when this item is returned via API."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["call_id","output","type"],"description":"The output of a function tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique identifier of the reasoning content."},"summary":{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"A short summary of the reasoning used by the model when generating the response."},"type":{"type":"string","enum":["summary_text"],"description":"The type of the object. Always summary_text."}},"required":["text","type"]},"description":"Reasoning text contents."},"type":{"type":"string","enum":["reasoning"],"description":"The type of the object. Always reasoning."},"encrypted_content":{"type":"string","nullable":true,"description":"The encrypted content of the reasoning item - populated when a response is generated with reasoning.encrypted_content in the include parameter."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","summary","type"],"description":"A description of the chain of thought used by a reasoning model while generating a response."},{"type":"object","properties":{"code":{"type":"string","description":"The code to run, or null if not available."},"id":{"type":"string","description":"The unique ID of the code interpreter tool call."},"outputs":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"logs":{"type":"string","description":"The logs output from the code interpreter."},"type":{"type":"string","enum":["logs"],"description":"The type of the output. Always 'logs'."}},"required":["logs","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"url":{"type":"string"}},"required":["type","url"]}]},"description":"The outputs generated by the code interpreter, such as logs or images. Can be null if no outputs are available."},"status":{"type":"string","enum":["in_progress","completed","interpreting"],"description":"The status of the code interpreter tool call."},"type":{"type":"string","enum":["code_interpreter_call"],"description":"The type of the code interpreter tool call. Always code_interpreter_call."},"container_id":{"type":"string","description":"The ID of the container used to run the code."}},"required":["code","id","outputs","status","type","container_id"],"description":"A tool call to run code."},{"type":"object","properties":{"action":{"type":"object","properties":{"command":{"type":"array","items":{"type":"string"},"description":"The command to run."},"env":{"type":"object","additionalProperties":{"type":"string"},"description":"Environment variables to set for the command."},"type":{"type":"string","enum":["exec"],"description":"The type of the local shell action. Always exec."},"timeout_ms":{"type":"integer","nullable":true,"description":"Optional timeout in milliseconds for the command."},"user":{"type":"string","nullable":true,"description":"Optional user to run the command as."},"working_directory":{"type":"string","nullable":true,"description":"Optional working directory to run the command in."}},"required":["command","env","type"],"description":"Execute a shell command on the server."},"call_id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"id":{"type":"string","description":"The unique ID of the local shell call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the local shell call."},"type":{"type":"string","enum":["local_shell_call"],"description":"The type of the local shell call. Always local_shell_call."}},"required":["action","call_id","id","status","type"],"description":"A tool call to run a command on the local shell."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"output":{"type":"string","description":"A JSON string of the output of the local shell tool call."},"type":{"type":"string","enum":["local_shell_call_output"],"description":"The type of the local shell tool call output. Always local_shell_call_output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","output","type"],"description":"The output of a local shell tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the list."},"server_label":{"type":"string","description":"The label of the MCP server."},"tools":{"type":"array","items":{"type":"object","properties":{"input_schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The JSON schema describing the tool's input."},"name":{"type":"string","description":"The name of the tool."},"annotations":{"type":"object","nullable":true,"additionalProperties":{"nullable":true},"description":"Additional annotations about the tool."},"description":{"type":"string","nullable":true,"description":"The description of the tool."}},"required":["input_schema","name"]},"description":"The tools available on the server."},"type":{"type":"string","enum":["mcp_list_tools"],"description":"The type of the item. Always mcp_list_tools."},"error":{"type":"string","nullable":true,"description":"Error message if the server could not list tools."}},"required":["id","server_label","tools","type"],"description":"A list of tools available on an MCP server."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of arguments for the tool."},"id":{"type":"string","description":"The unique ID of the approval request."},"name":{"type":"string"},"server_label":{"type":"string","description":"The name of the tool to run."},"type":{"type":"string","enum":["mcp_approval_request"],"description":"The type of the item. Always mcp_approval_request."}},"required":["arguments","id","name","server_label","type"],"description":"A request for human approval of a tool invocation."},{"type":"object","properties":{"approval_request_id":{"type":"string","description":"The ID of the approval request being answered."},"approve":{"type":"boolean","description":"Whether the request was approved."},"type":{"type":"string","enum":["mcp_approval_response"],"description":"The type of the item. Always mcp_approval_response."},"id":{"type":"string","nullable":true,"description":"The unique ID of the approval response."},"reason":{"type":"string","nullable":true,"description":"Optional reason for the decision."}},"required":["approval_request_id","approve","type"],"description":"A response to an MCP approval request."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments passed to the tool."},"id":{"type":"string","description":"The unique ID of the tool call."},"name":{"type":"string","description":"The name of the tool that was run."},"server_label":{"type":"string","description":"The label of the MCP server running the tool."},"type":{"type":"string","enum":["mcp_call"],"description":"The type of the item. Always mcp_call."},"error":{"type":"string","nullable":true,"description":"The error from the tool call, if any."},"output":{"type":"string","nullable":true,"description":"The output from the tool call."}},"required":["arguments","id","name","server_label","type"],"description":"An invocation of a tool on an MCP server."},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the item to reference."},"type":{"type":"string","nullable":true,"enum":["item_reference"],"description":"The type of item to reference. Always item_reference."}},"required":["id"],"description":"An internal identifier for an item to reference."}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or file inputs to the model, used to generate a response."},"include":{"type":"array","nullable":true,"items":{"type":"string","enum":["message.input_image.image_url","computer_call_output.output.image_url","reasoning.encrypted_content","code_interpreter_call.outputs"]},"description":"Specify additional output data to include in the model response. Currently supported values are:\n    - code_interpreter_call.outputs: Includes the outputs of python code execution in code interpreter tool call items.\n    - computer_call_output.output.image_url: Include image urls from the computer call output.\n    - file_search_call.results: Include the search results of the file search tool call.\n    - message.output_text.logprobs: Include logprobs with assistant messages.\n    - reasoning.encrypted_content: Includes an encrypted version of reasoning tokens in reasoning item outputs. This enables reasoning items to be used in multi-turn conversations when using the Responses API statelessly (like when the store parameter is set to false, or when an organization is enrolled in the zero data retention program).\n"},"instructions":{"type":"string","nullable":true,"description":"A system (or developer) message inserted into the model's context.\n\nWhen using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses."},"max_output_tokens":{"type":"integer","default":512,"description":"An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n\nKeys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"parallel_tool_calls":{"type":"boolean","nullable":true,"description":"Whether to allow the model to run tool calls in parallel."},"previous_response_id":{"type":"string","nullable":true,"description":"The unique ID of the previous response to the model. Use this to create multi-turn conversations."},"prompt":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"The unique identifier of the prompt template to use."},"variables":{"type":"object","nullable":true,"additionalProperties":{"anyOf":[{"type":"string"},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}]},"description":"Optional map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input types like images or files."},"version":{"type":"string","nullable":true,"description":"Optional version of the prompt template."}},"required":["id"],"description":"Reference to a prompt template and its variables."},"reasoning":{"type":"object","nullable":true,"properties":{"effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"summary":{"type":"string","nullable":true,"enum":["auto","concise","detailed"],"description":"A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process."}},"description":"o-series models only\nConfiguration options for reasoning models."},"store":{"type":"boolean","nullable":true,"default":false,"description":"Whether to store the generated model response for later retrieval via API."},"stream":{"type":"boolean","nullable":true,"default":false,"description":"If set to true, the model response data will be streamed to the client as it is generated using server-sent events. "},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"text":{"type":"object","properties":{"format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"type":{"type":"string","enum":["json_schema"]},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name","schema","type"],"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["format"],"description":"Configuration options for a text response from the model. Can be plain text or structured JSON data."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"Controls which (if any) tool is called by the model.\n\nnone means the model will not call any tool and instead generates a message.\n\nauto means the model can pick between generating a message or calling one or more tools.\n\nrequired means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11","computer_use_preview","code_interpreter","mcp"]}},"required":["type"],"description":"Indicates that the model should use a built-in tool to generate a response."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"type":{"type":"string","enum":["function"],"description":"For function calling, the type is always function."}},"required":["name","type"]}],"description":"How the model should select which tool (or tools) to use when generating a response."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11"],"description":"The type of the web search tool. One of web_search_preview or web_search_preview_2025_03_11."},"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."},"city":{"type":"string","nullable":true,"description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","nullable":true,"pattern":"^[A-Z]{2}$","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","nullable":true,"description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","nullable":true,"description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"required":["type"],"description":"The user's location"}},"required":["type"],"description":"This tool searches the web for relevant results to use in a response."},{"type":"object","properties":{"display_height":{"type":"integer","description":"The height of the computer display."},"display_width":{"type":"integer","description":"The width of the computer display."},"environment":{"type":"string","enum":["windows","mac","linux","ubuntu","browser"],"description":"The type of computer environment to control."},"type":{"type":"string","enum":["computer_use_preview"],"description":"The type of the computer use tool. Always computer_use_preview."}},"required":["display_height","display_width","environment","type"],"description":"A tool that controls a virtual computer."},{"type":"object","properties":{"server_label":{"type":"string","description":"A label for this MCP server, used to identify it in tool calls."},"server_url":{"type":"string","description":"The URL for the MCP server."},"type":{"type":"string","enum":["mcp"],"description":"The type of the MCP tool. Always mcp."},"allowed_tools":{"anyOf":[{"type":"array","items":{"type":"string"},"description":"A string array of allowed tool names."},{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of allowed tool names."}},"description":"A filter object to specify which tools are allowed."},{"nullable":true}],"description":"List of allowed tool names or a filter object."},"headers":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Optional HTTP headers to send to the MCP server. Use for authentication or other purposes."},"require_approval":{"anyOf":[{"type":"string","enum":["always","never"]},{"type":"object","properties":{"always":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that require approval."}},"description":"A list of tools that always require approval."},"never":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that do not require approval."}},"description":"A list of tools that never require approval."}}},{"nullable":true}],"description":"Specify which of the MCP server's tools require approval."}},"required":["server_label","server_url","type"],"description":"Give the model access to additional tools via remote Model Context Protocol (MCP) servers."},{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of the code interpreter tool. Always code_interpreter."},"container":{"anyOf":[{"type":"string"},{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]}],"description":"The container ID."}},"required":["type","container"],"description":"A tool that runs Python code to help generate a response to a prompt."},{"type":"object","properties":{"type":{"type":"string","enum":["local_shell"],"description":"The type of the local shell tool. Always local_shell."}},"required":["type"],"description":"A tool that allows the model to execute shell commands in a local environment."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"A JSON schema object describing the parameters of the function."},"strict":{"type":"boolean","description":"Whether to enforce strict parameter validation."},"type":{"type":"string","enum":["function"],"description":"The type of the function tool. Always function."},"description":{"type":"string","description":"A description of the function. Used by the model to determine whether or not to call the function."}},"required":["name","parameters","strict","type"],"description":"Defines a function in your own code the model can choose to call."}]},"description":"An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter."},"top_p":{"type":"number","nullable":true,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both."},"truncation":{"type":"string","enum":["auto","disabled"],"default":"disabled","description":"The truncation strategy to use for the model response.\n\n    - auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.\n    - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.\n"}},"required":["model","input"]}}},"paths":{"/v1/responses":{"post":{"operationId":"ResponseApiController_createResponse_v1","summary":"","parameters":[],"requestBody":{"required":true,"content":{"application/json":{"schema":{"$ref":"#/components/schemas/Response.v1.CreateResponsePayload"}}}},"responses":{"201":{"description":""}},"tags":["Response Api"]}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-3.5-turbo-0125",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-3.5-turbo-0125',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-BKKS4Aulz4SaVm81hHo7HMKEcQmtk', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today?', 'refusal': None, 'annotations': []}}], 'created': 1744184876, 'model': 'gpt-3.5-turbo-0125', 'usage': {'prompt_tokens': 50, 'completion_tokens': 126, 'total_tokens': 176, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'system_fingerprint': None}
```

{% endcode %}

</details>

## Code Example #2: Using /responses Endpoint

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json   # for getting a structured output with indentation

response = requests.post(
    "https://api.aimlapi.com/v1/responses",
    headers={
        "Content-Type":"application/json", 

        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-3.5-turbo",
        "input":"Hello"  # Insert your question for the model here, instead of Hello   
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/responses', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        input: 'Hello',  // Insert your question here, instead of Hello 
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "resp_686ba45ce63481a2a4b1fad55d2bea8102a1cc22f1a1bcf1",
  "object": "response",
  "created_at": 1751884892,
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 512,
  "model": "gpt-3.5-turbo",
  "output": [
    {
      "id": "rs_686ba463d18481a29dde85cfd7b055bf02a1cc22f1a1bcf1",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "msg_686ba463d4e081a2b2e2aff962ab00f702a1cc22f1a1bcf1",
      "type": "message",
      "status": "in_progress",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Hello! How can I help you today?"
        }
      ],
      "role": "assistant"
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": "medium",
    "summary": null
  },
  "temperature": 1,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 294,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 2520,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 2814
  },
  "metadata": {},
  "output_text": "Hello! How can I help you today?"
}
```

{% endcode %}

</details>


# gpt-4

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following model:<br><code>gpt-4</code></p></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=gpt-4&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

The model represents a significant leap forward in conversational AI technology. It offers enhanced understanding and generation of natural language, capable of handling complex and nuanced dialogues with greater coherence and context sensitivity. This model is designed to mimic human-like conversation more closely than ever before.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["gpt-4"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"]},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"]},"data":{"type":"string"}},"required":["type","media_type","data"]}},"required":["type","source"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"],"title":"gpt-4"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

### Responses Endpoint

This endpoint is currently used *only* with OpenAI models. Some models support both the `/chat/completions` and `/responses` endpoints, while others support only one of them. OpenAI has announced plans to expand the capabilities of the `/responses` endpoint in the future.

## POST /v1/responses

>

```json
{"openapi":"3.0.0","info":{"title":"AI/ML Gateway","version":"1.0"},"servers":[{"url":"https://api.aimlapi.com"}],"security":[{"access-token":[]}],"components":{"securitySchemes":{"access-token":{"scheme":"bearer","bearerFormat":"<YOUR_AIMLAPI_KEY>","type":"http","description":"Bearer key"}},"schemas":{"Response.v1.CreateResponsePayload":{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o3-mini","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16","openai/o3-2025-04-16","o1","openai/o3-pro","openai/gpt-5-2025-08-07","openai/gpt-5-mini-2025-08-07","openai/gpt-5-nano-2025-08-07","openai/gpt-5-chat-latest","openai/gpt-5-pro","openai/gpt-5-1","openai/gpt-5-1-chat-latest","openai/gpt-5-1-codex","openai/gpt-5-1-codex-mini"],"description":"Model ID used to generate the response."},"background":{"type":"boolean","default":false,"description":"Whether to run the model response in the background."},"input":{"anyOf":[{"type":"string","description":"A text input to the model, equivalent to a text input with the user role."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","assistant","system","developer"],"description":"The role of the message input."},"content":{"anyOf":[{"type":"string","description":"A text input to the model."},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or audio input to the model, used to generate a response. Can also contain previous assistant responses."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role. Messages with the assistant role are presumed to have been generated by the model in previous interactions."},{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","system","developer"],"description":"The role of the message input."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of item."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the output message."},"role":{"type":"string","enum":["assistant"],"description":"The role of the output message. Always assistant."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the message input."},"type":{"type":"string","enum":["message"],"description":"The type of the output message. Always message."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"annotations":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"type":{"type":"string","enum":["url_citation"],"description":"The type of the URL citation. Always url_citation."},"url":{"type":"string","format":"uri","description":"The URL of the web resource."}},"required":["end_index","start_index","title","type","url"],"description":"A citation for a web resource used to generate a model response."}]},"description":"The annotations of the text output."},"text":{"type":"string","description":"The text output from the model."},"type":{"type":"string","enum":["output_text"],"description":"The type of the output text. Always output_text."},"logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"},"top_logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"}},"required":["bytes","logprob","token"]}}},"required":["bytes","logprob","token","top_logprobs"]}}},"required":["annotations","text","type"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal explanationfrom the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the refusal. Always refusal."}},"required":["refusal","type"]}],"description":"The content of the output message."}}},"required":["id","role","status","type","content"],"description":"An output message from the model."},{"type":"object","properties":{"action":{"oneOf":[{"type":"object","properties":{"button":{"type":"string","enum":["left","right","wheel","back","forward"],"description":"Indicates which mouse button was pressed during the click."},"type":{"type":"string","enum":["click"],"description":"Specifies the event type. For a click action, this property is always set to click."},"x":{"type":"integer","description":"The x-coordinate where the click occurred."},"y":{"type":"integer","description":"The y-coordinate where the click occurred."}},"required":["button","type","x","y"],"description":"A click action."},{"type":"object","properties":{"type":{"type":"string","enum":["double_click"],"description":"Specifies the event type. For a double click action, this property is always set to double_click."},"x":{"type":"integer","description":"The x-coordinate where the double click occurred."},"y":{"type":"integer","description":"The y-coordinate where the double click occurred."}},"required":["type","x","y"],"description":"A double click action."},{"type":"object","properties":{"path":{"type":"array","items":{"type":"object","properties":{"x":{"type":"integer","description":"The y-coordinate."},"y":{"type":"integer","description":"The y-coordinate."}},"required":["x","y"]},"description":"An array of coordinates representing the path of the drag action. Coordinates will appear as an array of objects, eg"},"type":{"type":"string","enum":["drag"],"description":"Specifies the event type. For a drag action, this property is always set to drag."}},"required":["path","type"],"description":"A drag action."},{"type":"object","properties":{"keys":{"type":"array","items":{"type":"string"},"description":"The combination of keys the model is requesting to be pressed. This is an array of strings, each representing a key."},"type":{"type":"string","enum":["keypress"],"description":"Specifies the event type. For a keypress action, this property is always set to keypress."}},"required":["keys","type"],"description":"A collection of keypresses the model would like to perform."},{"type":"object","properties":{"type":{"type":"string","enum":["move"],"description":"Specifies the event type. For a move action, this property is always set to move."},"x":{"type":"integer","description":"The x-coordinate to move to."},"y":{"type":"integer","description":"The y-coordinate to move to."}},"required":["type","x","y"],"description":"A mouse move action."},{"type":"object","properties":{"type":{"type":"string","enum":["screenshot"],"description":"Specifies the event type. For a screenshot action, this property is always set to screenshot."}},"required":["type"],"description":"A screenshot action."},{"type":"object","properties":{"type":{"type":"string","enum":["scroll"],"description":"Specifies the event type. For a scroll action, this property is always set to scroll."},"scroll_x":{"type":"integer","description":"The horizontal scroll distance."},"scroll_y":{"type":"integer","description":"The vertical scroll distance."},"x":{"type":"integer","description":"The x-coordinate where the scroll occurred."},"y":{"type":"integer","description":"The y-coordinate where the scroll occurred."}},"required":["type","scroll_x","scroll_y","x","y"],"description":"A scroll action."},{"type":"object","properties":{"type":{"type":"string","enum":["type"],"description":"Specifies the event type. For a type action, this property is always set to type."},"text":{"type":"string","description":"The text to type."}},"required":["type","text"],"description":"An action to type in text."},{"type":"object","properties":{"type":{"type":"string","enum":["wait"],"description":"Specifies the event type. For a wait action, this property is always set to wait."}},"required":["type"],"description":"A wait action."}]},"call_id":{"type":"string","description":"An identifier used when responding to the tool call with output."},"id":{"type":"string","description":"The unique ID of the computer call."},"pending_safety_checks":{"type":"array","items":{"type":"object","properties":{"code":{"type":"string","description":"The type of the pending safety check."},"id":{"type":"string","description":"The ID of the pending safety check."},"message":{"type":"string","description":"Details about the pending safety check."}},"required":["code","id","message"]},"description":"The pending safety checks for the computer call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."},"type":{"type":"string","enum":["computer_call"],"description":"The type of the computer call. Always computer_call."}},"required":["action","call_id","id","pending_safety_checks","status","type"],"description":"A tool call to a computer use tool."},{"type":"object","properties":{"call_id":{"type":"string","description":"The ID of the computer tool call that produced the output."},"output":{"type":"object","properties":{"type":{"type":"string","enum":["computer_screenshot"],"description":"Specifies the event type. For a computer screenshot, this property is always set to computer_screenshot."},"image_url":{"type":"string","format":"uri","description":"The URL of the screenshot image."}},"required":["type"],"description":"A computer screenshot image used with the computer use tool."},"type":{"type":"string","enum":["computer_call_output"],"description":"The type of the computer tool call output. Always computer_call_output."},"acknowledged_safety_checks":{"type":"array","nullable":true,"items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the pending safety check."},"code":{"type":"string","nullable":true,"description":"The type of the pending safety check."},"message":{"type":"string","nullable":true,"description":"Details about the pending safety check."}},"required":["id"]},"description":"The safety checks reported by the API that have been acknowledged by the developer."},"id":{"type":"string","nullable":true,"description":"The ID of the computer tool call output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the message input."}},"required":["call_id","output","type"],"description":"The output of a computer tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the web search tool call."},"status":{"type":"string","enum":["in_progress","completed","searching","failed"],"description":"The status of the web search tool call."},"type":{"type":"string","enum":["web_search_call"],"description":"The type of the web search tool call. Always web_search_call."}},"required":["id","status","type"],"description":"The results of a web search tool call."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments to pass to the function."},"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"name":{"type":"string","description":"The name of the function to run."},"type":{"type":"string","enum":["function_call"],"description":"The type of the function tool call. Always function_call."},"id":{"type":"string","description":"The unique ID of the function tool call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["arguments","call_id","name","type"],"description":"A tool call to run a function."},{"type":"object","properties":{"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"output":{"anyOf":[{"type":"string","description":"A JSON string of the output of the function tool call."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}}]},"type":{"type":"string","enum":["function_call_output"],"description":"The type of the function tool call output. Always function_call_output."},"id":{"type":"string","nullable":true,"description":"The unique ID of the function tool call output. Populated when this item is returned via API."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["call_id","output","type"],"description":"The output of a function tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique identifier of the reasoning content."},"summary":{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"A short summary of the reasoning used by the model when generating the response."},"type":{"type":"string","enum":["summary_text"],"description":"The type of the object. Always summary_text."}},"required":["text","type"]},"description":"Reasoning text contents."},"type":{"type":"string","enum":["reasoning"],"description":"The type of the object. Always reasoning."},"encrypted_content":{"type":"string","nullable":true,"description":"The encrypted content of the reasoning item - populated when a response is generated with reasoning.encrypted_content in the include parameter."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","summary","type"],"description":"A description of the chain of thought used by a reasoning model while generating a response."},{"type":"object","properties":{"code":{"type":"string","description":"The code to run, or null if not available."},"id":{"type":"string","description":"The unique ID of the code interpreter tool call."},"outputs":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"logs":{"type":"string","description":"The logs output from the code interpreter."},"type":{"type":"string","enum":["logs"],"description":"The type of the output. Always 'logs'."}},"required":["logs","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"url":{"type":"string"}},"required":["type","url"]}]},"description":"The outputs generated by the code interpreter, such as logs or images. Can be null if no outputs are available."},"status":{"type":"string","enum":["in_progress","completed","interpreting"],"description":"The status of the code interpreter tool call."},"type":{"type":"string","enum":["code_interpreter_call"],"description":"The type of the code interpreter tool call. Always code_interpreter_call."},"container_id":{"type":"string","description":"The ID of the container used to run the code."}},"required":["code","id","outputs","status","type","container_id"],"description":"A tool call to run code."},{"type":"object","properties":{"action":{"type":"object","properties":{"command":{"type":"array","items":{"type":"string"},"description":"The command to run."},"env":{"type":"object","additionalProperties":{"type":"string"},"description":"Environment variables to set for the command."},"type":{"type":"string","enum":["exec"],"description":"The type of the local shell action. Always exec."},"timeout_ms":{"type":"integer","nullable":true,"description":"Optional timeout in milliseconds for the command."},"user":{"type":"string","nullable":true,"description":"Optional user to run the command as."},"working_directory":{"type":"string","nullable":true,"description":"Optional working directory to run the command in."}},"required":["command","env","type"],"description":"Execute a shell command on the server."},"call_id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"id":{"type":"string","description":"The unique ID of the local shell call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the local shell call."},"type":{"type":"string","enum":["local_shell_call"],"description":"The type of the local shell call. Always local_shell_call."}},"required":["action","call_id","id","status","type"],"description":"A tool call to run a command on the local shell."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"output":{"type":"string","description":"A JSON string of the output of the local shell tool call."},"type":{"type":"string","enum":["local_shell_call_output"],"description":"The type of the local shell tool call output. Always local_shell_call_output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","output","type"],"description":"The output of a local shell tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the list."},"server_label":{"type":"string","description":"The label of the MCP server."},"tools":{"type":"array","items":{"type":"object","properties":{"input_schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The JSON schema describing the tool's input."},"name":{"type":"string","description":"The name of the tool."},"annotations":{"type":"object","nullable":true,"additionalProperties":{"nullable":true},"description":"Additional annotations about the tool."},"description":{"type":"string","nullable":true,"description":"The description of the tool."}},"required":["input_schema","name"]},"description":"The tools available on the server."},"type":{"type":"string","enum":["mcp_list_tools"],"description":"The type of the item. Always mcp_list_tools."},"error":{"type":"string","nullable":true,"description":"Error message if the server could not list tools."}},"required":["id","server_label","tools","type"],"description":"A list of tools available on an MCP server."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of arguments for the tool."},"id":{"type":"string","description":"The unique ID of the approval request."},"name":{"type":"string"},"server_label":{"type":"string","description":"The name of the tool to run."},"type":{"type":"string","enum":["mcp_approval_request"],"description":"The type of the item. Always mcp_approval_request."}},"required":["arguments","id","name","server_label","type"],"description":"A request for human approval of a tool invocation."},{"type":"object","properties":{"approval_request_id":{"type":"string","description":"The ID of the approval request being answered."},"approve":{"type":"boolean","description":"Whether the request was approved."},"type":{"type":"string","enum":["mcp_approval_response"],"description":"The type of the item. Always mcp_approval_response."},"id":{"type":"string","nullable":true,"description":"The unique ID of the approval response."},"reason":{"type":"string","nullable":true,"description":"Optional reason for the decision."}},"required":["approval_request_id","approve","type"],"description":"A response to an MCP approval request."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments passed to the tool."},"id":{"type":"string","description":"The unique ID of the tool call."},"name":{"type":"string","description":"The name of the tool that was run."},"server_label":{"type":"string","description":"The label of the MCP server running the tool."},"type":{"type":"string","enum":["mcp_call"],"description":"The type of the item. Always mcp_call."},"error":{"type":"string","nullable":true,"description":"The error from the tool call, if any."},"output":{"type":"string","nullable":true,"description":"The output from the tool call."}},"required":["arguments","id","name","server_label","type"],"description":"An invocation of a tool on an MCP server."},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the item to reference."},"type":{"type":"string","nullable":true,"enum":["item_reference"],"description":"The type of item to reference. Always item_reference."}},"required":["id"],"description":"An internal identifier for an item to reference."}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or file inputs to the model, used to generate a response."},"include":{"type":"array","nullable":true,"items":{"type":"string","enum":["message.input_image.image_url","computer_call_output.output.image_url","reasoning.encrypted_content","code_interpreter_call.outputs"]},"description":"Specify additional output data to include in the model response. Currently supported values are:\n    - code_interpreter_call.outputs: Includes the outputs of python code execution in code interpreter tool call items.\n    - computer_call_output.output.image_url: Include image urls from the computer call output.\n    - file_search_call.results: Include the search results of the file search tool call.\n    - message.output_text.logprobs: Include logprobs with assistant messages.\n    - reasoning.encrypted_content: Includes an encrypted version of reasoning tokens in reasoning item outputs. This enables reasoning items to be used in multi-turn conversations when using the Responses API statelessly (like when the store parameter is set to false, or when an organization is enrolled in the zero data retention program).\n"},"instructions":{"type":"string","nullable":true,"description":"A system (or developer) message inserted into the model's context.\n\nWhen using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses."},"max_output_tokens":{"type":"integer","default":512,"description":"An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n\nKeys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"parallel_tool_calls":{"type":"boolean","nullable":true,"description":"Whether to allow the model to run tool calls in parallel."},"previous_response_id":{"type":"string","nullable":true,"description":"The unique ID of the previous response to the model. Use this to create multi-turn conversations."},"prompt":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"The unique identifier of the prompt template to use."},"variables":{"type":"object","nullable":true,"additionalProperties":{"anyOf":[{"type":"string"},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}]},"description":"Optional map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input types like images or files."},"version":{"type":"string","nullable":true,"description":"Optional version of the prompt template."}},"required":["id"],"description":"Reference to a prompt template and its variables."},"reasoning":{"type":"object","nullable":true,"properties":{"effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"summary":{"type":"string","nullable":true,"enum":["auto","concise","detailed"],"description":"A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process."}},"description":"o-series models only\nConfiguration options for reasoning models."},"store":{"type":"boolean","nullable":true,"default":false,"description":"Whether to store the generated model response for later retrieval via API."},"stream":{"type":"boolean","nullable":true,"default":false,"description":"If set to true, the model response data will be streamed to the client as it is generated using server-sent events. "},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"text":{"type":"object","properties":{"format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"type":{"type":"string","enum":["json_schema"]},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name","schema","type"],"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["format"],"description":"Configuration options for a text response from the model. Can be plain text or structured JSON data."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"Controls which (if any) tool is called by the model.\n\nnone means the model will not call any tool and instead generates a message.\n\nauto means the model can pick between generating a message or calling one or more tools.\n\nrequired means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11","computer_use_preview","code_interpreter","mcp"]}},"required":["type"],"description":"Indicates that the model should use a built-in tool to generate a response."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"type":{"type":"string","enum":["function"],"description":"For function calling, the type is always function."}},"required":["name","type"]}],"description":"How the model should select which tool (or tools) to use when generating a response."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11"],"description":"The type of the web search tool. One of web_search_preview or web_search_preview_2025_03_11."},"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."},"city":{"type":"string","nullable":true,"description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","nullable":true,"pattern":"^[A-Z]{2}$","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","nullable":true,"description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","nullable":true,"description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"required":["type"],"description":"The user's location"}},"required":["type"],"description":"This tool searches the web for relevant results to use in a response."},{"type":"object","properties":{"display_height":{"type":"integer","description":"The height of the computer display."},"display_width":{"type":"integer","description":"The width of the computer display."},"environment":{"type":"string","enum":["windows","mac","linux","ubuntu","browser"],"description":"The type of computer environment to control."},"type":{"type":"string","enum":["computer_use_preview"],"description":"The type of the computer use tool. Always computer_use_preview."}},"required":["display_height","display_width","environment","type"],"description":"A tool that controls a virtual computer."},{"type":"object","properties":{"server_label":{"type":"string","description":"A label for this MCP server, used to identify it in tool calls."},"server_url":{"type":"string","description":"The URL for the MCP server."},"type":{"type":"string","enum":["mcp"],"description":"The type of the MCP tool. Always mcp."},"allowed_tools":{"anyOf":[{"type":"array","items":{"type":"string"},"description":"A string array of allowed tool names."},{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of allowed tool names."}},"description":"A filter object to specify which tools are allowed."},{"nullable":true}],"description":"List of allowed tool names or a filter object."},"headers":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Optional HTTP headers to send to the MCP server. Use for authentication or other purposes."},"require_approval":{"anyOf":[{"type":"string","enum":["always","never"]},{"type":"object","properties":{"always":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that require approval."}},"description":"A list of tools that always require approval."},"never":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that do not require approval."}},"description":"A list of tools that never require approval."}}},{"nullable":true}],"description":"Specify which of the MCP server's tools require approval."}},"required":["server_label","server_url","type"],"description":"Give the model access to additional tools via remote Model Context Protocol (MCP) servers."},{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of the code interpreter tool. Always code_interpreter."},"container":{"anyOf":[{"type":"string"},{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]}],"description":"The container ID."}},"required":["type","container"],"description":"A tool that runs Python code to help generate a response to a prompt."},{"type":"object","properties":{"type":{"type":"string","enum":["local_shell"],"description":"The type of the local shell tool. Always local_shell."}},"required":["type"],"description":"A tool that allows the model to execute shell commands in a local environment."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"A JSON schema object describing the parameters of the function."},"strict":{"type":"boolean","description":"Whether to enforce strict parameter validation."},"type":{"type":"string","enum":["function"],"description":"The type of the function tool. Always function."},"description":{"type":"string","description":"A description of the function. Used by the model to determine whether or not to call the function."}},"required":["name","parameters","strict","type"],"description":"Defines a function in your own code the model can choose to call."}]},"description":"An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter."},"top_p":{"type":"number","nullable":true,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both."},"truncation":{"type":"string","enum":["auto","disabled"],"default":"disabled","description":"The truncation strategy to use for the model response.\n\n    - auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.\n    - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.\n"}},"required":["model","input"]}}},"paths":{"/v1/responses":{"post":{"operationId":"ResponseApiController_createResponse_v1","summary":"","parameters":[],"requestBody":{"required":true,"content":{"application/json":{"schema":{"$ref":"#/components/schemas/Response.v1.CreateResponsePayload"}}}},"responses":{"201":{"description":""}},"tags":["Response Api"]}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-4",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-4',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-BKKWkzVpUFHEDbw7MlOsqBIbm9Vi2', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today?', 'refusal': None, 'annotations': []}}], 'created': 1744185166, 'model': 'gpt-4-0613', 'usage': {'prompt_tokens': 504, 'completion_tokens': 1260, 'total_tokens': 1764, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'system_fingerprint': None}
```

{% endcode %}

</details>

## Code Example #2: Using /responses Endpoint

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json   # for getting a structured output with indentation

response = requests.post(
    "https://api.aimlapi.com/v1/responses",
    headers={
        "Content-Type":"application/json", 

        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-4",
        "input":"Hello"  # Insert your question for the model here, instead of Hello   
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/responses', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4',
        input: 'Hello',  // Insert your question here, instead of Hello 
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "resp_686ba45ce63481a2a4b1fad55d2bea8102a1cc22f1a1bcf1",
  "object": "response",
  "created_at": 1751884892,
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 512,
  "model": "gpt-4",
  "output": [
    {
      "id": "rs_686ba463d18481a29dde85cfd7b055bf02a1cc22f1a1bcf1",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "msg_686ba463d4e081a2b2e2aff962ab00f702a1cc22f1a1bcf1",
      "type": "message",
      "status": "in_progress",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Hello! How can I help you today?"
        }
      ],
      "role": "assistant"
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": "medium",
    "summary": null
  },
  "temperature": 1,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 294,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 2520,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 2814
  },
  "metadata": {},
  "output_text": "Hello! How can I help you today?"
}
```

{% endcode %}

</details>


# gpt-4-preview

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>gpt-4-0125-preview</code></li><li><code>gpt-4-1106-preview</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=gpt-4-0125-preview&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

Before the release of GPT-4 Turbo, OpenAI introduced two preview models that allowed users to test advanced features ahead of a full rollout. These models supported JSON mode for structured responses, parallel function calling to handle multiple API functions in a single request, and reproducible output, ensuring more consistent results across runs. The model has better code generation performance, reduces cases where the model doesn't complete a task.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["gpt-4-0125-preview","gpt-4-1106-preview"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."}},"required":["model","messages"],"title":"gpt-4-0125-preview"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

### Responses Endpoint

This endpoint is currently used *only* with OpenAI models. Some models support both the `/chat/completions` and `/responses` endpoints, while others support only one of them. OpenAI has announced plans to expand the capabilities of the `/responses` endpoint in the future.

## POST /v1/responses

>

```json
{"openapi":"3.0.0","info":{"title":"AI/ML Gateway","version":"1.0"},"servers":[{"url":"https://api.aimlapi.com"}],"security":[{"access-token":[]}],"components":{"securitySchemes":{"access-token":{"scheme":"bearer","bearerFormat":"<YOUR_AIMLAPI_KEY>","type":"http","description":"Bearer key"}},"schemas":{"Response.v1.CreateResponsePayload":{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o3-mini","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16","openai/o3-2025-04-16","o1","openai/o3-pro","openai/gpt-5-2025-08-07","openai/gpt-5-mini-2025-08-07","openai/gpt-5-nano-2025-08-07","openai/gpt-5-chat-latest","openai/gpt-5-pro","openai/gpt-5-1","openai/gpt-5-1-chat-latest","openai/gpt-5-1-codex","openai/gpt-5-1-codex-mini"],"description":"Model ID used to generate the response."},"background":{"type":"boolean","default":false,"description":"Whether to run the model response in the background."},"input":{"anyOf":[{"type":"string","description":"A text input to the model, equivalent to a text input with the user role."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","assistant","system","developer"],"description":"The role of the message input."},"content":{"anyOf":[{"type":"string","description":"A text input to the model."},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or audio input to the model, used to generate a response. Can also contain previous assistant responses."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role. Messages with the assistant role are presumed to have been generated by the model in previous interactions."},{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","system","developer"],"description":"The role of the message input."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of item."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the output message."},"role":{"type":"string","enum":["assistant"],"description":"The role of the output message. Always assistant."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the message input."},"type":{"type":"string","enum":["message"],"description":"The type of the output message. Always message."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"annotations":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"type":{"type":"string","enum":["url_citation"],"description":"The type of the URL citation. Always url_citation."},"url":{"type":"string","format":"uri","description":"The URL of the web resource."}},"required":["end_index","start_index","title","type","url"],"description":"A citation for a web resource used to generate a model response."}]},"description":"The annotations of the text output."},"text":{"type":"string","description":"The text output from the model."},"type":{"type":"string","enum":["output_text"],"description":"The type of the output text. Always output_text."},"logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"},"top_logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"}},"required":["bytes","logprob","token"]}}},"required":["bytes","logprob","token","top_logprobs"]}}},"required":["annotations","text","type"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal explanationfrom the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the refusal. Always refusal."}},"required":["refusal","type"]}],"description":"The content of the output message."}}},"required":["id","role","status","type","content"],"description":"An output message from the model."},{"type":"object","properties":{"action":{"oneOf":[{"type":"object","properties":{"button":{"type":"string","enum":["left","right","wheel","back","forward"],"description":"Indicates which mouse button was pressed during the click."},"type":{"type":"string","enum":["click"],"description":"Specifies the event type. For a click action, this property is always set to click."},"x":{"type":"integer","description":"The x-coordinate where the click occurred."},"y":{"type":"integer","description":"The y-coordinate where the click occurred."}},"required":["button","type","x","y"],"description":"A click action."},{"type":"object","properties":{"type":{"type":"string","enum":["double_click"],"description":"Specifies the event type. For a double click action, this property is always set to double_click."},"x":{"type":"integer","description":"The x-coordinate where the double click occurred."},"y":{"type":"integer","description":"The y-coordinate where the double click occurred."}},"required":["type","x","y"],"description":"A double click action."},{"type":"object","properties":{"path":{"type":"array","items":{"type":"object","properties":{"x":{"type":"integer","description":"The y-coordinate."},"y":{"type":"integer","description":"The y-coordinate."}},"required":["x","y"]},"description":"An array of coordinates representing the path of the drag action. Coordinates will appear as an array of objects, eg"},"type":{"type":"string","enum":["drag"],"description":"Specifies the event type. For a drag action, this property is always set to drag."}},"required":["path","type"],"description":"A drag action."},{"type":"object","properties":{"keys":{"type":"array","items":{"type":"string"},"description":"The combination of keys the model is requesting to be pressed. This is an array of strings, each representing a key."},"type":{"type":"string","enum":["keypress"],"description":"Specifies the event type. For a keypress action, this property is always set to keypress."}},"required":["keys","type"],"description":"A collection of keypresses the model would like to perform."},{"type":"object","properties":{"type":{"type":"string","enum":["move"],"description":"Specifies the event type. For a move action, this property is always set to move."},"x":{"type":"integer","description":"The x-coordinate to move to."},"y":{"type":"integer","description":"The y-coordinate to move to."}},"required":["type","x","y"],"description":"A mouse move action."},{"type":"object","properties":{"type":{"type":"string","enum":["screenshot"],"description":"Specifies the event type. For a screenshot action, this property is always set to screenshot."}},"required":["type"],"description":"A screenshot action."},{"type":"object","properties":{"type":{"type":"string","enum":["scroll"],"description":"Specifies the event type. For a scroll action, this property is always set to scroll."},"scroll_x":{"type":"integer","description":"The horizontal scroll distance."},"scroll_y":{"type":"integer","description":"The vertical scroll distance."},"x":{"type":"integer","description":"The x-coordinate where the scroll occurred."},"y":{"type":"integer","description":"The y-coordinate where the scroll occurred."}},"required":["type","scroll_x","scroll_y","x","y"],"description":"A scroll action."},{"type":"object","properties":{"type":{"type":"string","enum":["type"],"description":"Specifies the event type. For a type action, this property is always set to type."},"text":{"type":"string","description":"The text to type."}},"required":["type","text"],"description":"An action to type in text."},{"type":"object","properties":{"type":{"type":"string","enum":["wait"],"description":"Specifies the event type. For a wait action, this property is always set to wait."}},"required":["type"],"description":"A wait action."}]},"call_id":{"type":"string","description":"An identifier used when responding to the tool call with output."},"id":{"type":"string","description":"The unique ID of the computer call."},"pending_safety_checks":{"type":"array","items":{"type":"object","properties":{"code":{"type":"string","description":"The type of the pending safety check."},"id":{"type":"string","description":"The ID of the pending safety check."},"message":{"type":"string","description":"Details about the pending safety check."}},"required":["code","id","message"]},"description":"The pending safety checks for the computer call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."},"type":{"type":"string","enum":["computer_call"],"description":"The type of the computer call. Always computer_call."}},"required":["action","call_id","id","pending_safety_checks","status","type"],"description":"A tool call to a computer use tool."},{"type":"object","properties":{"call_id":{"type":"string","description":"The ID of the computer tool call that produced the output."},"output":{"type":"object","properties":{"type":{"type":"string","enum":["computer_screenshot"],"description":"Specifies the event type. For a computer screenshot, this property is always set to computer_screenshot."},"image_url":{"type":"string","format":"uri","description":"The URL of the screenshot image."}},"required":["type"],"description":"A computer screenshot image used with the computer use tool."},"type":{"type":"string","enum":["computer_call_output"],"description":"The type of the computer tool call output. Always computer_call_output."},"acknowledged_safety_checks":{"type":"array","nullable":true,"items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the pending safety check."},"code":{"type":"string","nullable":true,"description":"The type of the pending safety check."},"message":{"type":"string","nullable":true,"description":"Details about the pending safety check."}},"required":["id"]},"description":"The safety checks reported by the API that have been acknowledged by the developer."},"id":{"type":"string","nullable":true,"description":"The ID of the computer tool call output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the message input."}},"required":["call_id","output","type"],"description":"The output of a computer tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the web search tool call."},"status":{"type":"string","enum":["in_progress","completed","searching","failed"],"description":"The status of the web search tool call."},"type":{"type":"string","enum":["web_search_call"],"description":"The type of the web search tool call. Always web_search_call."}},"required":["id","status","type"],"description":"The results of a web search tool call."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments to pass to the function."},"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"name":{"type":"string","description":"The name of the function to run."},"type":{"type":"string","enum":["function_call"],"description":"The type of the function tool call. Always function_call."},"id":{"type":"string","description":"The unique ID of the function tool call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["arguments","call_id","name","type"],"description":"A tool call to run a function."},{"type":"object","properties":{"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"output":{"anyOf":[{"type":"string","description":"A JSON string of the output of the function tool call."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}}]},"type":{"type":"string","enum":["function_call_output"],"description":"The type of the function tool call output. Always function_call_output."},"id":{"type":"string","nullable":true,"description":"The unique ID of the function tool call output. Populated when this item is returned via API."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["call_id","output","type"],"description":"The output of a function tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique identifier of the reasoning content."},"summary":{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"A short summary of the reasoning used by the model when generating the response."},"type":{"type":"string","enum":["summary_text"],"description":"The type of the object. Always summary_text."}},"required":["text","type"]},"description":"Reasoning text contents."},"type":{"type":"string","enum":["reasoning"],"description":"The type of the object. Always reasoning."},"encrypted_content":{"type":"string","nullable":true,"description":"The encrypted content of the reasoning item - populated when a response is generated with reasoning.encrypted_content in the include parameter."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","summary","type"],"description":"A description of the chain of thought used by a reasoning model while generating a response."},{"type":"object","properties":{"code":{"type":"string","description":"The code to run, or null if not available."},"id":{"type":"string","description":"The unique ID of the code interpreter tool call."},"outputs":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"logs":{"type":"string","description":"The logs output from the code interpreter."},"type":{"type":"string","enum":["logs"],"description":"The type of the output. Always 'logs'."}},"required":["logs","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"url":{"type":"string"}},"required":["type","url"]}]},"description":"The outputs generated by the code interpreter, such as logs or images. Can be null if no outputs are available."},"status":{"type":"string","enum":["in_progress","completed","interpreting"],"description":"The status of the code interpreter tool call."},"type":{"type":"string","enum":["code_interpreter_call"],"description":"The type of the code interpreter tool call. Always code_interpreter_call."},"container_id":{"type":"string","description":"The ID of the container used to run the code."}},"required":["code","id","outputs","status","type","container_id"],"description":"A tool call to run code."},{"type":"object","properties":{"action":{"type":"object","properties":{"command":{"type":"array","items":{"type":"string"},"description":"The command to run."},"env":{"type":"object","additionalProperties":{"type":"string"},"description":"Environment variables to set for the command."},"type":{"type":"string","enum":["exec"],"description":"The type of the local shell action. Always exec."},"timeout_ms":{"type":"integer","nullable":true,"description":"Optional timeout in milliseconds for the command."},"user":{"type":"string","nullable":true,"description":"Optional user to run the command as."},"working_directory":{"type":"string","nullable":true,"description":"Optional working directory to run the command in."}},"required":["command","env","type"],"description":"Execute a shell command on the server."},"call_id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"id":{"type":"string","description":"The unique ID of the local shell call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the local shell call."},"type":{"type":"string","enum":["local_shell_call"],"description":"The type of the local shell call. Always local_shell_call."}},"required":["action","call_id","id","status","type"],"description":"A tool call to run a command on the local shell."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"output":{"type":"string","description":"A JSON string of the output of the local shell tool call."},"type":{"type":"string","enum":["local_shell_call_output"],"description":"The type of the local shell tool call output. Always local_shell_call_output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","output","type"],"description":"The output of a local shell tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the list."},"server_label":{"type":"string","description":"The label of the MCP server."},"tools":{"type":"array","items":{"type":"object","properties":{"input_schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The JSON schema describing the tool's input."},"name":{"type":"string","description":"The name of the tool."},"annotations":{"type":"object","nullable":true,"additionalProperties":{"nullable":true},"description":"Additional annotations about the tool."},"description":{"type":"string","nullable":true,"description":"The description of the tool."}},"required":["input_schema","name"]},"description":"The tools available on the server."},"type":{"type":"string","enum":["mcp_list_tools"],"description":"The type of the item. Always mcp_list_tools."},"error":{"type":"string","nullable":true,"description":"Error message if the server could not list tools."}},"required":["id","server_label","tools","type"],"description":"A list of tools available on an MCP server."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of arguments for the tool."},"id":{"type":"string","description":"The unique ID of the approval request."},"name":{"type":"string"},"server_label":{"type":"string","description":"The name of the tool to run."},"type":{"type":"string","enum":["mcp_approval_request"],"description":"The type of the item. Always mcp_approval_request."}},"required":["arguments","id","name","server_label","type"],"description":"A request for human approval of a tool invocation."},{"type":"object","properties":{"approval_request_id":{"type":"string","description":"The ID of the approval request being answered."},"approve":{"type":"boolean","description":"Whether the request was approved."},"type":{"type":"string","enum":["mcp_approval_response"],"description":"The type of the item. Always mcp_approval_response."},"id":{"type":"string","nullable":true,"description":"The unique ID of the approval response."},"reason":{"type":"string","nullable":true,"description":"Optional reason for the decision."}},"required":["approval_request_id","approve","type"],"description":"A response to an MCP approval request."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments passed to the tool."},"id":{"type":"string","description":"The unique ID of the tool call."},"name":{"type":"string","description":"The name of the tool that was run."},"server_label":{"type":"string","description":"The label of the MCP server running the tool."},"type":{"type":"string","enum":["mcp_call"],"description":"The type of the item. Always mcp_call."},"error":{"type":"string","nullable":true,"description":"The error from the tool call, if any."},"output":{"type":"string","nullable":true,"description":"The output from the tool call."}},"required":["arguments","id","name","server_label","type"],"description":"An invocation of a tool on an MCP server."},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the item to reference."},"type":{"type":"string","nullable":true,"enum":["item_reference"],"description":"The type of item to reference. Always item_reference."}},"required":["id"],"description":"An internal identifier for an item to reference."}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or file inputs to the model, used to generate a response."},"include":{"type":"array","nullable":true,"items":{"type":"string","enum":["message.input_image.image_url","computer_call_output.output.image_url","reasoning.encrypted_content","code_interpreter_call.outputs"]},"description":"Specify additional output data to include in the model response. Currently supported values are:\n    - code_interpreter_call.outputs: Includes the outputs of python code execution in code interpreter tool call items.\n    - computer_call_output.output.image_url: Include image urls from the computer call output.\n    - file_search_call.results: Include the search results of the file search tool call.\n    - message.output_text.logprobs: Include logprobs with assistant messages.\n    - reasoning.encrypted_content: Includes an encrypted version of reasoning tokens in reasoning item outputs. This enables reasoning items to be used in multi-turn conversations when using the Responses API statelessly (like when the store parameter is set to false, or when an organization is enrolled in the zero data retention program).\n"},"instructions":{"type":"string","nullable":true,"description":"A system (or developer) message inserted into the model's context.\n\nWhen using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses."},"max_output_tokens":{"type":"integer","default":512,"description":"An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n\nKeys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"parallel_tool_calls":{"type":"boolean","nullable":true,"description":"Whether to allow the model to run tool calls in parallel."},"previous_response_id":{"type":"string","nullable":true,"description":"The unique ID of the previous response to the model. Use this to create multi-turn conversations."},"prompt":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"The unique identifier of the prompt template to use."},"variables":{"type":"object","nullable":true,"additionalProperties":{"anyOf":[{"type":"string"},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}]},"description":"Optional map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input types like images or files."},"version":{"type":"string","nullable":true,"description":"Optional version of the prompt template."}},"required":["id"],"description":"Reference to a prompt template and its variables."},"reasoning":{"type":"object","nullable":true,"properties":{"effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"summary":{"type":"string","nullable":true,"enum":["auto","concise","detailed"],"description":"A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process."}},"description":"o-series models only\nConfiguration options for reasoning models."},"store":{"type":"boolean","nullable":true,"default":false,"description":"Whether to store the generated model response for later retrieval via API."},"stream":{"type":"boolean","nullable":true,"default":false,"description":"If set to true, the model response data will be streamed to the client as it is generated using server-sent events. "},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"text":{"type":"object","properties":{"format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"type":{"type":"string","enum":["json_schema"]},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name","schema","type"],"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["format"],"description":"Configuration options for a text response from the model. Can be plain text or structured JSON data."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"Controls which (if any) tool is called by the model.\n\nnone means the model will not call any tool and instead generates a message.\n\nauto means the model can pick between generating a message or calling one or more tools.\n\nrequired means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11","computer_use_preview","code_interpreter","mcp"]}},"required":["type"],"description":"Indicates that the model should use a built-in tool to generate a response."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"type":{"type":"string","enum":["function"],"description":"For function calling, the type is always function."}},"required":["name","type"]}],"description":"How the model should select which tool (or tools) to use when generating a response."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11"],"description":"The type of the web search tool. One of web_search_preview or web_search_preview_2025_03_11."},"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."},"city":{"type":"string","nullable":true,"description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","nullable":true,"pattern":"^[A-Z]{2}$","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","nullable":true,"description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","nullable":true,"description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"required":["type"],"description":"The user's location"}},"required":["type"],"description":"This tool searches the web for relevant results to use in a response."},{"type":"object","properties":{"display_height":{"type":"integer","description":"The height of the computer display."},"display_width":{"type":"integer","description":"The width of the computer display."},"environment":{"type":"string","enum":["windows","mac","linux","ubuntu","browser"],"description":"The type of computer environment to control."},"type":{"type":"string","enum":["computer_use_preview"],"description":"The type of the computer use tool. Always computer_use_preview."}},"required":["display_height","display_width","environment","type"],"description":"A tool that controls a virtual computer."},{"type":"object","properties":{"server_label":{"type":"string","description":"A label for this MCP server, used to identify it in tool calls."},"server_url":{"type":"string","description":"The URL for the MCP server."},"type":{"type":"string","enum":["mcp"],"description":"The type of the MCP tool. Always mcp."},"allowed_tools":{"anyOf":[{"type":"array","items":{"type":"string"},"description":"A string array of allowed tool names."},{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of allowed tool names."}},"description":"A filter object to specify which tools are allowed."},{"nullable":true}],"description":"List of allowed tool names or a filter object."},"headers":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Optional HTTP headers to send to the MCP server. Use for authentication or other purposes."},"require_approval":{"anyOf":[{"type":"string","enum":["always","never"]},{"type":"object","properties":{"always":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that require approval."}},"description":"A list of tools that always require approval."},"never":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that do not require approval."}},"description":"A list of tools that never require approval."}}},{"nullable":true}],"description":"Specify which of the MCP server's tools require approval."}},"required":["server_label","server_url","type"],"description":"Give the model access to additional tools via remote Model Context Protocol (MCP) servers."},{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of the code interpreter tool. Always code_interpreter."},"container":{"anyOf":[{"type":"string"},{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]}],"description":"The container ID."}},"required":["type","container"],"description":"A tool that runs Python code to help generate a response to a prompt."},{"type":"object","properties":{"type":{"type":"string","enum":["local_shell"],"description":"The type of the local shell tool. Always local_shell."}},"required":["type"],"description":"A tool that allows the model to execute shell commands in a local environment."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"A JSON schema object describing the parameters of the function."},"strict":{"type":"boolean","description":"Whether to enforce strict parameter validation."},"type":{"type":"string","enum":["function"],"description":"The type of the function tool. Always function."},"description":{"type":"string","description":"A description of the function. Used by the model to determine whether or not to call the function."}},"required":["name","parameters","strict","type"],"description":"Defines a function in your own code the model can choose to call."}]},"description":"An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter."},"top_p":{"type":"number","nullable":true,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both."},"truncation":{"type":"string","enum":["auto","disabled"],"default":"disabled","description":"The truncation strategy to use for the model response.\n\n    - auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.\n    - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.\n"}},"required":["model","input"]}}},"paths":{"/v1/responses":{"post":{"operationId":"ResponseApiController_createResponse_v1","summary":"","parameters":[],"requestBody":{"required":true,"content":{"application/json":{"schema":{"$ref":"#/components/schemas/Response.v1.CreateResponsePayload"}}}},"responses":{"201":{"description":""}},"tags":["Response Api"]}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-4-0125-preview",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-4-0125-preview',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-BKKXr9a69c5WOJr8R2d8rP2Wd0XZa', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today?', 'refusal': None, 'annotations': []}}], 'created': 1744185235, 'model': 'gpt-4-1106-preview', 'usage': {'prompt_tokens': 168, 'completion_tokens': 630, 'total_tokens': 798, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'system_fingerprint': None}
```

{% endcode %}

</details>

## Code Example #2: Using /responses Endpoint

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json   # for getting a structured output with indentation

response = requests.post(
    "https://api.aimlapi.com/v1/responses",
    headers={
        "Content-Type":"application/json", 

        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-4-0125-preview",
        "input":"Hello"  # Insert your question for the model here, instead of Hello   
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/responses', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4-0125-preview',
        input: 'Hello',  // Insert your question here, instead of Hello 
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "resp_686ba45ce63481a2a4b1fad55d2bea8102a1cc22f1a1bcf1",
  "object": "response",
  "created_at": 1751884892,
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 512,
  "model": "gpt-4-0125-preview",
  "output": [
    {
      "id": "rs_686ba463d18481a29dde85cfd7b055bf02a1cc22f1a1bcf1",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "msg_686ba463d4e081a2b2e2aff962ab00f702a1cc22f1a1bcf1",
      "type": "message",
      "status": "in_progress",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Hello! How can I help you today?"
        }
      ],
      "role": "assistant"
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": "medium",
    "summary": null
  },
  "temperature": 1,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 294,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 2520,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 2814
  },
  "metadata": {},
  "output_text": "Hello! How can I help you today?"
}
```

{% endcode %}

</details>


# gpt-4-turbo

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>gpt-4-turbo</code></li><li><code>gpt-4-turbo-2024-04-09</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=gpt-4-turbo&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

The model enhances the already impressive capabilities of [gpt-4](https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4) by significantly reducing response times, making it ideal for applications requiring instant feedback. Replacement for all previous [gpt-4-preview](https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4-preview) models.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["gpt-4-turbo","gpt-4-turbo-2024-04-09"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."}},"required":["model","messages"],"title":"gpt-4-turbo"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

### Responses Endpoint

This endpoint is currently used *only* with OpenAI models. Some models support both the `/chat/completions` and `/responses` endpoints, while others support only one of them. OpenAI has announced plans to expand the capabilities of the `/responses` endpoint in the future.

## POST /v1/responses

>

```json
{"openapi":"3.0.0","info":{"title":"AI/ML Gateway","version":"1.0"},"servers":[{"url":"https://api.aimlapi.com"}],"security":[{"access-token":[]}],"components":{"securitySchemes":{"access-token":{"scheme":"bearer","bearerFormat":"<YOUR_AIMLAPI_KEY>","type":"http","description":"Bearer key"}},"schemas":{"Response.v1.CreateResponsePayload":{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o3-mini","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16","openai/o3-2025-04-16","o1","openai/o3-pro","openai/gpt-5-2025-08-07","openai/gpt-5-mini-2025-08-07","openai/gpt-5-nano-2025-08-07","openai/gpt-5-chat-latest","openai/gpt-5-pro","openai/gpt-5-1","openai/gpt-5-1-chat-latest","openai/gpt-5-1-codex","openai/gpt-5-1-codex-mini"],"description":"Model ID used to generate the response."},"background":{"type":"boolean","default":false,"description":"Whether to run the model response in the background."},"input":{"anyOf":[{"type":"string","description":"A text input to the model, equivalent to a text input with the user role."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","assistant","system","developer"],"description":"The role of the message input."},"content":{"anyOf":[{"type":"string","description":"A text input to the model."},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or audio input to the model, used to generate a response. Can also contain previous assistant responses."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role. Messages with the assistant role are presumed to have been generated by the model in previous interactions."},{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","system","developer"],"description":"The role of the message input."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of item."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the output message."},"role":{"type":"string","enum":["assistant"],"description":"The role of the output message. Always assistant."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the message input."},"type":{"type":"string","enum":["message"],"description":"The type of the output message. Always message."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"annotations":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"type":{"type":"string","enum":["url_citation"],"description":"The type of the URL citation. Always url_citation."},"url":{"type":"string","format":"uri","description":"The URL of the web resource."}},"required":["end_index","start_index","title","type","url"],"description":"A citation for a web resource used to generate a model response."}]},"description":"The annotations of the text output."},"text":{"type":"string","description":"The text output from the model."},"type":{"type":"string","enum":["output_text"],"description":"The type of the output text. Always output_text."},"logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"},"top_logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"}},"required":["bytes","logprob","token"]}}},"required":["bytes","logprob","token","top_logprobs"]}}},"required":["annotations","text","type"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal explanationfrom the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the refusal. Always refusal."}},"required":["refusal","type"]}],"description":"The content of the output message."}}},"required":["id","role","status","type","content"],"description":"An output message from the model."},{"type":"object","properties":{"action":{"oneOf":[{"type":"object","properties":{"button":{"type":"string","enum":["left","right","wheel","back","forward"],"description":"Indicates which mouse button was pressed during the click."},"type":{"type":"string","enum":["click"],"description":"Specifies the event type. For a click action, this property is always set to click."},"x":{"type":"integer","description":"The x-coordinate where the click occurred."},"y":{"type":"integer","description":"The y-coordinate where the click occurred."}},"required":["button","type","x","y"],"description":"A click action."},{"type":"object","properties":{"type":{"type":"string","enum":["double_click"],"description":"Specifies the event type. For a double click action, this property is always set to double_click."},"x":{"type":"integer","description":"The x-coordinate where the double click occurred."},"y":{"type":"integer","description":"The y-coordinate where the double click occurred."}},"required":["type","x","y"],"description":"A double click action."},{"type":"object","properties":{"path":{"type":"array","items":{"type":"object","properties":{"x":{"type":"integer","description":"The y-coordinate."},"y":{"type":"integer","description":"The y-coordinate."}},"required":["x","y"]},"description":"An array of coordinates representing the path of the drag action. Coordinates will appear as an array of objects, eg"},"type":{"type":"string","enum":["drag"],"description":"Specifies the event type. For a drag action, this property is always set to drag."}},"required":["path","type"],"description":"A drag action."},{"type":"object","properties":{"keys":{"type":"array","items":{"type":"string"},"description":"The combination of keys the model is requesting to be pressed. This is an array of strings, each representing a key."},"type":{"type":"string","enum":["keypress"],"description":"Specifies the event type. For a keypress action, this property is always set to keypress."}},"required":["keys","type"],"description":"A collection of keypresses the model would like to perform."},{"type":"object","properties":{"type":{"type":"string","enum":["move"],"description":"Specifies the event type. For a move action, this property is always set to move."},"x":{"type":"integer","description":"The x-coordinate to move to."},"y":{"type":"integer","description":"The y-coordinate to move to."}},"required":["type","x","y"],"description":"A mouse move action."},{"type":"object","properties":{"type":{"type":"string","enum":["screenshot"],"description":"Specifies the event type. For a screenshot action, this property is always set to screenshot."}},"required":["type"],"description":"A screenshot action."},{"type":"object","properties":{"type":{"type":"string","enum":["scroll"],"description":"Specifies the event type. For a scroll action, this property is always set to scroll."},"scroll_x":{"type":"integer","description":"The horizontal scroll distance."},"scroll_y":{"type":"integer","description":"The vertical scroll distance."},"x":{"type":"integer","description":"The x-coordinate where the scroll occurred."},"y":{"type":"integer","description":"The y-coordinate where the scroll occurred."}},"required":["type","scroll_x","scroll_y","x","y"],"description":"A scroll action."},{"type":"object","properties":{"type":{"type":"string","enum":["type"],"description":"Specifies the event type. For a type action, this property is always set to type."},"text":{"type":"string","description":"The text to type."}},"required":["type","text"],"description":"An action to type in text."},{"type":"object","properties":{"type":{"type":"string","enum":["wait"],"description":"Specifies the event type. For a wait action, this property is always set to wait."}},"required":["type"],"description":"A wait action."}]},"call_id":{"type":"string","description":"An identifier used when responding to the tool call with output."},"id":{"type":"string","description":"The unique ID of the computer call."},"pending_safety_checks":{"type":"array","items":{"type":"object","properties":{"code":{"type":"string","description":"The type of the pending safety check."},"id":{"type":"string","description":"The ID of the pending safety check."},"message":{"type":"string","description":"Details about the pending safety check."}},"required":["code","id","message"]},"description":"The pending safety checks for the computer call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."},"type":{"type":"string","enum":["computer_call"],"description":"The type of the computer call. Always computer_call."}},"required":["action","call_id","id","pending_safety_checks","status","type"],"description":"A tool call to a computer use tool."},{"type":"object","properties":{"call_id":{"type":"string","description":"The ID of the computer tool call that produced the output."},"output":{"type":"object","properties":{"type":{"type":"string","enum":["computer_screenshot"],"description":"Specifies the event type. For a computer screenshot, this property is always set to computer_screenshot."},"image_url":{"type":"string","format":"uri","description":"The URL of the screenshot image."}},"required":["type"],"description":"A computer screenshot image used with the computer use tool."},"type":{"type":"string","enum":["computer_call_output"],"description":"The type of the computer tool call output. Always computer_call_output."},"acknowledged_safety_checks":{"type":"array","nullable":true,"items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the pending safety check."},"code":{"type":"string","nullable":true,"description":"The type of the pending safety check."},"message":{"type":"string","nullable":true,"description":"Details about the pending safety check."}},"required":["id"]},"description":"The safety checks reported by the API that have been acknowledged by the developer."},"id":{"type":"string","nullable":true,"description":"The ID of the computer tool call output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the message input."}},"required":["call_id","output","type"],"description":"The output of a computer tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the web search tool call."},"status":{"type":"string","enum":["in_progress","completed","searching","failed"],"description":"The status of the web search tool call."},"type":{"type":"string","enum":["web_search_call"],"description":"The type of the web search tool call. Always web_search_call."}},"required":["id","status","type"],"description":"The results of a web search tool call."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments to pass to the function."},"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"name":{"type":"string","description":"The name of the function to run."},"type":{"type":"string","enum":["function_call"],"description":"The type of the function tool call. Always function_call."},"id":{"type":"string","description":"The unique ID of the function tool call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["arguments","call_id","name","type"],"description":"A tool call to run a function."},{"type":"object","properties":{"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"output":{"anyOf":[{"type":"string","description":"A JSON string of the output of the function tool call."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}}]},"type":{"type":"string","enum":["function_call_output"],"description":"The type of the function tool call output. Always function_call_output."},"id":{"type":"string","nullable":true,"description":"The unique ID of the function tool call output. Populated when this item is returned via API."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["call_id","output","type"],"description":"The output of a function tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique identifier of the reasoning content."},"summary":{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"A short summary of the reasoning used by the model when generating the response."},"type":{"type":"string","enum":["summary_text"],"description":"The type of the object. Always summary_text."}},"required":["text","type"]},"description":"Reasoning text contents."},"type":{"type":"string","enum":["reasoning"],"description":"The type of the object. Always reasoning."},"encrypted_content":{"type":"string","nullable":true,"description":"The encrypted content of the reasoning item - populated when a response is generated with reasoning.encrypted_content in the include parameter."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","summary","type"],"description":"A description of the chain of thought used by a reasoning model while generating a response."},{"type":"object","properties":{"code":{"type":"string","description":"The code to run, or null if not available."},"id":{"type":"string","description":"The unique ID of the code interpreter tool call."},"outputs":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"logs":{"type":"string","description":"The logs output from the code interpreter."},"type":{"type":"string","enum":["logs"],"description":"The type of the output. Always 'logs'."}},"required":["logs","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"url":{"type":"string"}},"required":["type","url"]}]},"description":"The outputs generated by the code interpreter, such as logs or images. Can be null if no outputs are available."},"status":{"type":"string","enum":["in_progress","completed","interpreting"],"description":"The status of the code interpreter tool call."},"type":{"type":"string","enum":["code_interpreter_call"],"description":"The type of the code interpreter tool call. Always code_interpreter_call."},"container_id":{"type":"string","description":"The ID of the container used to run the code."}},"required":["code","id","outputs","status","type","container_id"],"description":"A tool call to run code."},{"type":"object","properties":{"action":{"type":"object","properties":{"command":{"type":"array","items":{"type":"string"},"description":"The command to run."},"env":{"type":"object","additionalProperties":{"type":"string"},"description":"Environment variables to set for the command."},"type":{"type":"string","enum":["exec"],"description":"The type of the local shell action. Always exec."},"timeout_ms":{"type":"integer","nullable":true,"description":"Optional timeout in milliseconds for the command."},"user":{"type":"string","nullable":true,"description":"Optional user to run the command as."},"working_directory":{"type":"string","nullable":true,"description":"Optional working directory to run the command in."}},"required":["command","env","type"],"description":"Execute a shell command on the server."},"call_id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"id":{"type":"string","description":"The unique ID of the local shell call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the local shell call."},"type":{"type":"string","enum":["local_shell_call"],"description":"The type of the local shell call. Always local_shell_call."}},"required":["action","call_id","id","status","type"],"description":"A tool call to run a command on the local shell."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"output":{"type":"string","description":"A JSON string of the output of the local shell tool call."},"type":{"type":"string","enum":["local_shell_call_output"],"description":"The type of the local shell tool call output. Always local_shell_call_output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","output","type"],"description":"The output of a local shell tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the list."},"server_label":{"type":"string","description":"The label of the MCP server."},"tools":{"type":"array","items":{"type":"object","properties":{"input_schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The JSON schema describing the tool's input."},"name":{"type":"string","description":"The name of the tool."},"annotations":{"type":"object","nullable":true,"additionalProperties":{"nullable":true},"description":"Additional annotations about the tool."},"description":{"type":"string","nullable":true,"description":"The description of the tool."}},"required":["input_schema","name"]},"description":"The tools available on the server."},"type":{"type":"string","enum":["mcp_list_tools"],"description":"The type of the item. Always mcp_list_tools."},"error":{"type":"string","nullable":true,"description":"Error message if the server could not list tools."}},"required":["id","server_label","tools","type"],"description":"A list of tools available on an MCP server."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of arguments for the tool."},"id":{"type":"string","description":"The unique ID of the approval request."},"name":{"type":"string"},"server_label":{"type":"string","description":"The name of the tool to run."},"type":{"type":"string","enum":["mcp_approval_request"],"description":"The type of the item. Always mcp_approval_request."}},"required":["arguments","id","name","server_label","type"],"description":"A request for human approval of a tool invocation."},{"type":"object","properties":{"approval_request_id":{"type":"string","description":"The ID of the approval request being answered."},"approve":{"type":"boolean","description":"Whether the request was approved."},"type":{"type":"string","enum":["mcp_approval_response"],"description":"The type of the item. Always mcp_approval_response."},"id":{"type":"string","nullable":true,"description":"The unique ID of the approval response."},"reason":{"type":"string","nullable":true,"description":"Optional reason for the decision."}},"required":["approval_request_id","approve","type"],"description":"A response to an MCP approval request."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments passed to the tool."},"id":{"type":"string","description":"The unique ID of the tool call."},"name":{"type":"string","description":"The name of the tool that was run."},"server_label":{"type":"string","description":"The label of the MCP server running the tool."},"type":{"type":"string","enum":["mcp_call"],"description":"The type of the item. Always mcp_call."},"error":{"type":"string","nullable":true,"description":"The error from the tool call, if any."},"output":{"type":"string","nullable":true,"description":"The output from the tool call."}},"required":["arguments","id","name","server_label","type"],"description":"An invocation of a tool on an MCP server."},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the item to reference."},"type":{"type":"string","nullable":true,"enum":["item_reference"],"description":"The type of item to reference. Always item_reference."}},"required":["id"],"description":"An internal identifier for an item to reference."}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or file inputs to the model, used to generate a response."},"include":{"type":"array","nullable":true,"items":{"type":"string","enum":["message.input_image.image_url","computer_call_output.output.image_url","reasoning.encrypted_content","code_interpreter_call.outputs"]},"description":"Specify additional output data to include in the model response. Currently supported values are:\n    - code_interpreter_call.outputs: Includes the outputs of python code execution in code interpreter tool call items.\n    - computer_call_output.output.image_url: Include image urls from the computer call output.\n    - file_search_call.results: Include the search results of the file search tool call.\n    - message.output_text.logprobs: Include logprobs with assistant messages.\n    - reasoning.encrypted_content: Includes an encrypted version of reasoning tokens in reasoning item outputs. This enables reasoning items to be used in multi-turn conversations when using the Responses API statelessly (like when the store parameter is set to false, or when an organization is enrolled in the zero data retention program).\n"},"instructions":{"type":"string","nullable":true,"description":"A system (or developer) message inserted into the model's context.\n\nWhen using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses."},"max_output_tokens":{"type":"integer","default":512,"description":"An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n\nKeys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"parallel_tool_calls":{"type":"boolean","nullable":true,"description":"Whether to allow the model to run tool calls in parallel."},"previous_response_id":{"type":"string","nullable":true,"description":"The unique ID of the previous response to the model. Use this to create multi-turn conversations."},"prompt":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"The unique identifier of the prompt template to use."},"variables":{"type":"object","nullable":true,"additionalProperties":{"anyOf":[{"type":"string"},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}]},"description":"Optional map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input types like images or files."},"version":{"type":"string","nullable":true,"description":"Optional version of the prompt template."}},"required":["id"],"description":"Reference to a prompt template and its variables."},"reasoning":{"type":"object","nullable":true,"properties":{"effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"summary":{"type":"string","nullable":true,"enum":["auto","concise","detailed"],"description":"A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process."}},"description":"o-series models only\nConfiguration options for reasoning models."},"store":{"type":"boolean","nullable":true,"default":false,"description":"Whether to store the generated model response for later retrieval via API."},"stream":{"type":"boolean","nullable":true,"default":false,"description":"If set to true, the model response data will be streamed to the client as it is generated using server-sent events. "},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"text":{"type":"object","properties":{"format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"type":{"type":"string","enum":["json_schema"]},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name","schema","type"],"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["format"],"description":"Configuration options for a text response from the model. Can be plain text or structured JSON data."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"Controls which (if any) tool is called by the model.\n\nnone means the model will not call any tool and instead generates a message.\n\nauto means the model can pick between generating a message or calling one or more tools.\n\nrequired means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11","computer_use_preview","code_interpreter","mcp"]}},"required":["type"],"description":"Indicates that the model should use a built-in tool to generate a response."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"type":{"type":"string","enum":["function"],"description":"For function calling, the type is always function."}},"required":["name","type"]}],"description":"How the model should select which tool (or tools) to use when generating a response."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11"],"description":"The type of the web search tool. One of web_search_preview or web_search_preview_2025_03_11."},"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."},"city":{"type":"string","nullable":true,"description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","nullable":true,"pattern":"^[A-Z]{2}$","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","nullable":true,"description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","nullable":true,"description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"required":["type"],"description":"The user's location"}},"required":["type"],"description":"This tool searches the web for relevant results to use in a response."},{"type":"object","properties":{"display_height":{"type":"integer","description":"The height of the computer display."},"display_width":{"type":"integer","description":"The width of the computer display."},"environment":{"type":"string","enum":["windows","mac","linux","ubuntu","browser"],"description":"The type of computer environment to control."},"type":{"type":"string","enum":["computer_use_preview"],"description":"The type of the computer use tool. Always computer_use_preview."}},"required":["display_height","display_width","environment","type"],"description":"A tool that controls a virtual computer."},{"type":"object","properties":{"server_label":{"type":"string","description":"A label for this MCP server, used to identify it in tool calls."},"server_url":{"type":"string","description":"The URL for the MCP server."},"type":{"type":"string","enum":["mcp"],"description":"The type of the MCP tool. Always mcp."},"allowed_tools":{"anyOf":[{"type":"array","items":{"type":"string"},"description":"A string array of allowed tool names."},{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of allowed tool names."}},"description":"A filter object to specify which tools are allowed."},{"nullable":true}],"description":"List of allowed tool names or a filter object."},"headers":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Optional HTTP headers to send to the MCP server. Use for authentication or other purposes."},"require_approval":{"anyOf":[{"type":"string","enum":["always","never"]},{"type":"object","properties":{"always":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that require approval."}},"description":"A list of tools that always require approval."},"never":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that do not require approval."}},"description":"A list of tools that never require approval."}}},{"nullable":true}],"description":"Specify which of the MCP server's tools require approval."}},"required":["server_label","server_url","type"],"description":"Give the model access to additional tools via remote Model Context Protocol (MCP) servers."},{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of the code interpreter tool. Always code_interpreter."},"container":{"anyOf":[{"type":"string"},{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]}],"description":"The container ID."}},"required":["type","container"],"description":"A tool that runs Python code to help generate a response to a prompt."},{"type":"object","properties":{"type":{"type":"string","enum":["local_shell"],"description":"The type of the local shell tool. Always local_shell."}},"required":["type"],"description":"A tool that allows the model to execute shell commands in a local environment."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"A JSON schema object describing the parameters of the function."},"strict":{"type":"boolean","description":"Whether to enforce strict parameter validation."},"type":{"type":"string","enum":["function"],"description":"The type of the function tool. Always function."},"description":{"type":"string","description":"A description of the function. Used by the model to determine whether or not to call the function."}},"required":["name","parameters","strict","type"],"description":"Defines a function in your own code the model can choose to call."}]},"description":"An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter."},"top_p":{"type":"number","nullable":true,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both."},"truncation":{"type":"string","enum":["auto","disabled"],"default":"disabled","description":"The truncation strategy to use for the model response.\n\n    - auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.\n    - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.\n"}},"required":["model","input"]}}},"paths":{"/v1/responses":{"post":{"operationId":"ResponseApiController_createResponse_v1","summary":"","parameters":[],"requestBody":{"required":true,"content":{"application/json":{"schema":{"$ref":"#/components/schemas/Response.v1.CreateResponsePayload"}}}},"responses":{"201":{"description":""}},"tags":["Response Api"]}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-4-turbo",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-4-turbo',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-BKKYo5xJ5uEzm8omnidM097vsMpYd', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today?', 'refusal': None, 'annotations': []}}], 'created': 1744185294, 'model': 'gpt-4-turbo-2024-04-09', 'usage': {'prompt_tokens': 168, 'completion_tokens': 630, 'total_tokens': 798, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'system_fingerprint': 'fp_101a39fff3'}
```

{% endcode %}

</details>

## Code Example #2: Using /responses Endpoint

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json   # for getting a structured output with indentation

response = requests.post(
    "https://api.aimlapi.com/v1/responses",
    headers={
        "Content-Type":"application/json", 

        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-4-turbo",
        "input":"Hello"  # Insert your question for the model here, instead of Hello   
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/responses', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4-turbo',
        input: 'Hello',  // Insert your question here, instead of Hello 
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "resp_686ba45ce63481a2a4b1fad55d2bea8102a1cc22f1a1bcf1",
  "object": "response",
  "created_at": 1751884892,
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 512,
  "model": "gpt-4-turbo",
  "output": [
    {
      "id": "rs_686ba463d18481a29dde85cfd7b055bf02a1cc22f1a1bcf1",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "msg_686ba463d4e081a2b2e2aff962ab00f702a1cc22f1a1bcf1",
      "type": "message",
      "status": "in_progress",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Hello! How can I help you today?"
        }
      ],
      "role": "assistant"
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": "medium",
    "summary": null
  },
  "temperature": 1,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 294,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 2520,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 2814
  },
  "metadata": {},
  "output_text": "Hello! How can I help you today?"
}
```

{% endcode %}

</details>


# gpt-4o

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>gpt-4o</code></li><li><code>chatgpt-4o-latest</code></li><li><code>gpt-4o-2024-05-13</code></li><li><code>gpt-4o-2024-08-06</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=openai/gpt-4o&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

OpenAI's flagship model designed to integrate enhanced capabilities across text, vision, and audio, providing real-time reasoning.

You can also view [a detailed comparison of this model](https://aimlapi.com/comparisons/qwen-2-vs-chatgpt-4o-comparison) on our main website.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","chatgpt-4o-latest"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."}},"required":["model","messages"],"title":"openai/gpt-4o"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

### Responses Endpoint

This endpoint is currently used *only* with OpenAI models. Some models support both the `/chat/completions` and `/responses` endpoints, while others support only one of them. OpenAI has announced plans to expand the capabilities of the `/responses` endpoint in the future.

## POST /v1/responses

>

```json
{"openapi":"3.0.0","info":{"title":"AI/ML Gateway","version":"1.0"},"servers":[{"url":"https://api.aimlapi.com"}],"security":[{"access-token":[]}],"components":{"securitySchemes":{"access-token":{"scheme":"bearer","bearerFormat":"<YOUR_AIMLAPI_KEY>","type":"http","description":"Bearer key"}},"schemas":{"Response.v1.CreateResponsePayload":{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o3-mini","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16","openai/o3-2025-04-16","o1","openai/o3-pro","openai/gpt-5-2025-08-07","openai/gpt-5-mini-2025-08-07","openai/gpt-5-nano-2025-08-07","openai/gpt-5-chat-latest","openai/gpt-5-pro","openai/gpt-5-1","openai/gpt-5-1-chat-latest","openai/gpt-5-1-codex","openai/gpt-5-1-codex-mini"],"description":"Model ID used to generate the response."},"background":{"type":"boolean","default":false,"description":"Whether to run the model response in the background."},"input":{"anyOf":[{"type":"string","description":"A text input to the model, equivalent to a text input with the user role."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","assistant","system","developer"],"description":"The role of the message input."},"content":{"anyOf":[{"type":"string","description":"A text input to the model."},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or audio input to the model, used to generate a response. Can also contain previous assistant responses."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role. Messages with the assistant role are presumed to have been generated by the model in previous interactions."},{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","system","developer"],"description":"The role of the message input."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of item."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the output message."},"role":{"type":"string","enum":["assistant"],"description":"The role of the output message. Always assistant."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the message input."},"type":{"type":"string","enum":["message"],"description":"The type of the output message. Always message."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"annotations":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"type":{"type":"string","enum":["url_citation"],"description":"The type of the URL citation. Always url_citation."},"url":{"type":"string","format":"uri","description":"The URL of the web resource."}},"required":["end_index","start_index","title","type","url"],"description":"A citation for a web resource used to generate a model response."}]},"description":"The annotations of the text output."},"text":{"type":"string","description":"The text output from the model."},"type":{"type":"string","enum":["output_text"],"description":"The type of the output text. Always output_text."},"logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"},"top_logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"}},"required":["bytes","logprob","token"]}}},"required":["bytes","logprob","token","top_logprobs"]}}},"required":["annotations","text","type"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal explanationfrom the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the refusal. Always refusal."}},"required":["refusal","type"]}],"description":"The content of the output message."}}},"required":["id","role","status","type","content"],"description":"An output message from the model."},{"type":"object","properties":{"action":{"oneOf":[{"type":"object","properties":{"button":{"type":"string","enum":["left","right","wheel","back","forward"],"description":"Indicates which mouse button was pressed during the click."},"type":{"type":"string","enum":["click"],"description":"Specifies the event type. For a click action, this property is always set to click."},"x":{"type":"integer","description":"The x-coordinate where the click occurred."},"y":{"type":"integer","description":"The y-coordinate where the click occurred."}},"required":["button","type","x","y"],"description":"A click action."},{"type":"object","properties":{"type":{"type":"string","enum":["double_click"],"description":"Specifies the event type. For a double click action, this property is always set to double_click."},"x":{"type":"integer","description":"The x-coordinate where the double click occurred."},"y":{"type":"integer","description":"The y-coordinate where the double click occurred."}},"required":["type","x","y"],"description":"A double click action."},{"type":"object","properties":{"path":{"type":"array","items":{"type":"object","properties":{"x":{"type":"integer","description":"The y-coordinate."},"y":{"type":"integer","description":"The y-coordinate."}},"required":["x","y"]},"description":"An array of coordinates representing the path of the drag action. Coordinates will appear as an array of objects, eg"},"type":{"type":"string","enum":["drag"],"description":"Specifies the event type. For a drag action, this property is always set to drag."}},"required":["path","type"],"description":"A drag action."},{"type":"object","properties":{"keys":{"type":"array","items":{"type":"string"},"description":"The combination of keys the model is requesting to be pressed. This is an array of strings, each representing a key."},"type":{"type":"string","enum":["keypress"],"description":"Specifies the event type. For a keypress action, this property is always set to keypress."}},"required":["keys","type"],"description":"A collection of keypresses the model would like to perform."},{"type":"object","properties":{"type":{"type":"string","enum":["move"],"description":"Specifies the event type. For a move action, this property is always set to move."},"x":{"type":"integer","description":"The x-coordinate to move to."},"y":{"type":"integer","description":"The y-coordinate to move to."}},"required":["type","x","y"],"description":"A mouse move action."},{"type":"object","properties":{"type":{"type":"string","enum":["screenshot"],"description":"Specifies the event type. For a screenshot action, this property is always set to screenshot."}},"required":["type"],"description":"A screenshot action."},{"type":"object","properties":{"type":{"type":"string","enum":["scroll"],"description":"Specifies the event type. For a scroll action, this property is always set to scroll."},"scroll_x":{"type":"integer","description":"The horizontal scroll distance."},"scroll_y":{"type":"integer","description":"The vertical scroll distance."},"x":{"type":"integer","description":"The x-coordinate where the scroll occurred."},"y":{"type":"integer","description":"The y-coordinate where the scroll occurred."}},"required":["type","scroll_x","scroll_y","x","y"],"description":"A scroll action."},{"type":"object","properties":{"type":{"type":"string","enum":["type"],"description":"Specifies the event type. For a type action, this property is always set to type."},"text":{"type":"string","description":"The text to type."}},"required":["type","text"],"description":"An action to type in text."},{"type":"object","properties":{"type":{"type":"string","enum":["wait"],"description":"Specifies the event type. For a wait action, this property is always set to wait."}},"required":["type"],"description":"A wait action."}]},"call_id":{"type":"string","description":"An identifier used when responding to the tool call with output."},"id":{"type":"string","description":"The unique ID of the computer call."},"pending_safety_checks":{"type":"array","items":{"type":"object","properties":{"code":{"type":"string","description":"The type of the pending safety check."},"id":{"type":"string","description":"The ID of the pending safety check."},"message":{"type":"string","description":"Details about the pending safety check."}},"required":["code","id","message"]},"description":"The pending safety checks for the computer call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."},"type":{"type":"string","enum":["computer_call"],"description":"The type of the computer call. Always computer_call."}},"required":["action","call_id","id","pending_safety_checks","status","type"],"description":"A tool call to a computer use tool."},{"type":"object","properties":{"call_id":{"type":"string","description":"The ID of the computer tool call that produced the output."},"output":{"type":"object","properties":{"type":{"type":"string","enum":["computer_screenshot"],"description":"Specifies the event type. For a computer screenshot, this property is always set to computer_screenshot."},"image_url":{"type":"string","format":"uri","description":"The URL of the screenshot image."}},"required":["type"],"description":"A computer screenshot image used with the computer use tool."},"type":{"type":"string","enum":["computer_call_output"],"description":"The type of the computer tool call output. Always computer_call_output."},"acknowledged_safety_checks":{"type":"array","nullable":true,"items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the pending safety check."},"code":{"type":"string","nullable":true,"description":"The type of the pending safety check."},"message":{"type":"string","nullable":true,"description":"Details about the pending safety check."}},"required":["id"]},"description":"The safety checks reported by the API that have been acknowledged by the developer."},"id":{"type":"string","nullable":true,"description":"The ID of the computer tool call output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the message input."}},"required":["call_id","output","type"],"description":"The output of a computer tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the web search tool call."},"status":{"type":"string","enum":["in_progress","completed","searching","failed"],"description":"The status of the web search tool call."},"type":{"type":"string","enum":["web_search_call"],"description":"The type of the web search tool call. Always web_search_call."}},"required":["id","status","type"],"description":"The results of a web search tool call."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments to pass to the function."},"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"name":{"type":"string","description":"The name of the function to run."},"type":{"type":"string","enum":["function_call"],"description":"The type of the function tool call. Always function_call."},"id":{"type":"string","description":"The unique ID of the function tool call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["arguments","call_id","name","type"],"description":"A tool call to run a function."},{"type":"object","properties":{"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"output":{"anyOf":[{"type":"string","description":"A JSON string of the output of the function tool call."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}}]},"type":{"type":"string","enum":["function_call_output"],"description":"The type of the function tool call output. Always function_call_output."},"id":{"type":"string","nullable":true,"description":"The unique ID of the function tool call output. Populated when this item is returned via API."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["call_id","output","type"],"description":"The output of a function tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique identifier of the reasoning content."},"summary":{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"A short summary of the reasoning used by the model when generating the response."},"type":{"type":"string","enum":["summary_text"],"description":"The type of the object. Always summary_text."}},"required":["text","type"]},"description":"Reasoning text contents."},"type":{"type":"string","enum":["reasoning"],"description":"The type of the object. Always reasoning."},"encrypted_content":{"type":"string","nullable":true,"description":"The encrypted content of the reasoning item - populated when a response is generated with reasoning.encrypted_content in the include parameter."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","summary","type"],"description":"A description of the chain of thought used by a reasoning model while generating a response."},{"type":"object","properties":{"code":{"type":"string","description":"The code to run, or null if not available."},"id":{"type":"string","description":"The unique ID of the code interpreter tool call."},"outputs":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"logs":{"type":"string","description":"The logs output from the code interpreter."},"type":{"type":"string","enum":["logs"],"description":"The type of the output. Always 'logs'."}},"required":["logs","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"url":{"type":"string"}},"required":["type","url"]}]},"description":"The outputs generated by the code interpreter, such as logs or images. Can be null if no outputs are available."},"status":{"type":"string","enum":["in_progress","completed","interpreting"],"description":"The status of the code interpreter tool call."},"type":{"type":"string","enum":["code_interpreter_call"],"description":"The type of the code interpreter tool call. Always code_interpreter_call."},"container_id":{"type":"string","description":"The ID of the container used to run the code."}},"required":["code","id","outputs","status","type","container_id"],"description":"A tool call to run code."},{"type":"object","properties":{"action":{"type":"object","properties":{"command":{"type":"array","items":{"type":"string"},"description":"The command to run."},"env":{"type":"object","additionalProperties":{"type":"string"},"description":"Environment variables to set for the command."},"type":{"type":"string","enum":["exec"],"description":"The type of the local shell action. Always exec."},"timeout_ms":{"type":"integer","nullable":true,"description":"Optional timeout in milliseconds for the command."},"user":{"type":"string","nullable":true,"description":"Optional user to run the command as."},"working_directory":{"type":"string","nullable":true,"description":"Optional working directory to run the command in."}},"required":["command","env","type"],"description":"Execute a shell command on the server."},"call_id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"id":{"type":"string","description":"The unique ID of the local shell call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the local shell call."},"type":{"type":"string","enum":["local_shell_call"],"description":"The type of the local shell call. Always local_shell_call."}},"required":["action","call_id","id","status","type"],"description":"A tool call to run a command on the local shell."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"output":{"type":"string","description":"A JSON string of the output of the local shell tool call."},"type":{"type":"string","enum":["local_shell_call_output"],"description":"The type of the local shell tool call output. Always local_shell_call_output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","output","type"],"description":"The output of a local shell tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the list."},"server_label":{"type":"string","description":"The label of the MCP server."},"tools":{"type":"array","items":{"type":"object","properties":{"input_schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The JSON schema describing the tool's input."},"name":{"type":"string","description":"The name of the tool."},"annotations":{"type":"object","nullable":true,"additionalProperties":{"nullable":true},"description":"Additional annotations about the tool."},"description":{"type":"string","nullable":true,"description":"The description of the tool."}},"required":["input_schema","name"]},"description":"The tools available on the server."},"type":{"type":"string","enum":["mcp_list_tools"],"description":"The type of the item. Always mcp_list_tools."},"error":{"type":"string","nullable":true,"description":"Error message if the server could not list tools."}},"required":["id","server_label","tools","type"],"description":"A list of tools available on an MCP server."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of arguments for the tool."},"id":{"type":"string","description":"The unique ID of the approval request."},"name":{"type":"string"},"server_label":{"type":"string","description":"The name of the tool to run."},"type":{"type":"string","enum":["mcp_approval_request"],"description":"The type of the item. Always mcp_approval_request."}},"required":["arguments","id","name","server_label","type"],"description":"A request for human approval of a tool invocation."},{"type":"object","properties":{"approval_request_id":{"type":"string","description":"The ID of the approval request being answered."},"approve":{"type":"boolean","description":"Whether the request was approved."},"type":{"type":"string","enum":["mcp_approval_response"],"description":"The type of the item. Always mcp_approval_response."},"id":{"type":"string","nullable":true,"description":"The unique ID of the approval response."},"reason":{"type":"string","nullable":true,"description":"Optional reason for the decision."}},"required":["approval_request_id","approve","type"],"description":"A response to an MCP approval request."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments passed to the tool."},"id":{"type":"string","description":"The unique ID of the tool call."},"name":{"type":"string","description":"The name of the tool that was run."},"server_label":{"type":"string","description":"The label of the MCP server running the tool."},"type":{"type":"string","enum":["mcp_call"],"description":"The type of the item. Always mcp_call."},"error":{"type":"string","nullable":true,"description":"The error from the tool call, if any."},"output":{"type":"string","nullable":true,"description":"The output from the tool call."}},"required":["arguments","id","name","server_label","type"],"description":"An invocation of a tool on an MCP server."},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the item to reference."},"type":{"type":"string","nullable":true,"enum":["item_reference"],"description":"The type of item to reference. Always item_reference."}},"required":["id"],"description":"An internal identifier for an item to reference."}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or file inputs to the model, used to generate a response."},"include":{"type":"array","nullable":true,"items":{"type":"string","enum":["message.input_image.image_url","computer_call_output.output.image_url","reasoning.encrypted_content","code_interpreter_call.outputs"]},"description":"Specify additional output data to include in the model response. Currently supported values are:\n    - code_interpreter_call.outputs: Includes the outputs of python code execution in code interpreter tool call items.\n    - computer_call_output.output.image_url: Include image urls from the computer call output.\n    - file_search_call.results: Include the search results of the file search tool call.\n    - message.output_text.logprobs: Include logprobs with assistant messages.\n    - reasoning.encrypted_content: Includes an encrypted version of reasoning tokens in reasoning item outputs. This enables reasoning items to be used in multi-turn conversations when using the Responses API statelessly (like when the store parameter is set to false, or when an organization is enrolled in the zero data retention program).\n"},"instructions":{"type":"string","nullable":true,"description":"A system (or developer) message inserted into the model's context.\n\nWhen using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses."},"max_output_tokens":{"type":"integer","default":512,"description":"An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n\nKeys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"parallel_tool_calls":{"type":"boolean","nullable":true,"description":"Whether to allow the model to run tool calls in parallel."},"previous_response_id":{"type":"string","nullable":true,"description":"The unique ID of the previous response to the model. Use this to create multi-turn conversations."},"prompt":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"The unique identifier of the prompt template to use."},"variables":{"type":"object","nullable":true,"additionalProperties":{"anyOf":[{"type":"string"},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}]},"description":"Optional map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input types like images or files."},"version":{"type":"string","nullable":true,"description":"Optional version of the prompt template."}},"required":["id"],"description":"Reference to a prompt template and its variables."},"reasoning":{"type":"object","nullable":true,"properties":{"effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"summary":{"type":"string","nullable":true,"enum":["auto","concise","detailed"],"description":"A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process."}},"description":"o-series models only\nConfiguration options for reasoning models."},"store":{"type":"boolean","nullable":true,"default":false,"description":"Whether to store the generated model response for later retrieval via API."},"stream":{"type":"boolean","nullable":true,"default":false,"description":"If set to true, the model response data will be streamed to the client as it is generated using server-sent events. "},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"text":{"type":"object","properties":{"format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"type":{"type":"string","enum":["json_schema"]},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name","schema","type"],"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["format"],"description":"Configuration options for a text response from the model. Can be plain text or structured JSON data."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"Controls which (if any) tool is called by the model.\n\nnone means the model will not call any tool and instead generates a message.\n\nauto means the model can pick between generating a message or calling one or more tools.\n\nrequired means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11","computer_use_preview","code_interpreter","mcp"]}},"required":["type"],"description":"Indicates that the model should use a built-in tool to generate a response."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"type":{"type":"string","enum":["function"],"description":"For function calling, the type is always function."}},"required":["name","type"]}],"description":"How the model should select which tool (or tools) to use when generating a response."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11"],"description":"The type of the web search tool. One of web_search_preview or web_search_preview_2025_03_11."},"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."},"city":{"type":"string","nullable":true,"description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","nullable":true,"pattern":"^[A-Z]{2}$","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","nullable":true,"description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","nullable":true,"description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"required":["type"],"description":"The user's location"}},"required":["type"],"description":"This tool searches the web for relevant results to use in a response."},{"type":"object","properties":{"display_height":{"type":"integer","description":"The height of the computer display."},"display_width":{"type":"integer","description":"The width of the computer display."},"environment":{"type":"string","enum":["windows","mac","linux","ubuntu","browser"],"description":"The type of computer environment to control."},"type":{"type":"string","enum":["computer_use_preview"],"description":"The type of the computer use tool. Always computer_use_preview."}},"required":["display_height","display_width","environment","type"],"description":"A tool that controls a virtual computer."},{"type":"object","properties":{"server_label":{"type":"string","description":"A label for this MCP server, used to identify it in tool calls."},"server_url":{"type":"string","description":"The URL for the MCP server."},"type":{"type":"string","enum":["mcp"],"description":"The type of the MCP tool. Always mcp."},"allowed_tools":{"anyOf":[{"type":"array","items":{"type":"string"},"description":"A string array of allowed tool names."},{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of allowed tool names."}},"description":"A filter object to specify which tools are allowed."},{"nullable":true}],"description":"List of allowed tool names or a filter object."},"headers":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Optional HTTP headers to send to the MCP server. Use for authentication or other purposes."},"require_approval":{"anyOf":[{"type":"string","enum":["always","never"]},{"type":"object","properties":{"always":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that require approval."}},"description":"A list of tools that always require approval."},"never":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that do not require approval."}},"description":"A list of tools that never require approval."}}},{"nullable":true}],"description":"Specify which of the MCP server's tools require approval."}},"required":["server_label","server_url","type"],"description":"Give the model access to additional tools via remote Model Context Protocol (MCP) servers."},{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of the code interpreter tool. Always code_interpreter."},"container":{"anyOf":[{"type":"string"},{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]}],"description":"The container ID."}},"required":["type","container"],"description":"A tool that runs Python code to help generate a response to a prompt."},{"type":"object","properties":{"type":{"type":"string","enum":["local_shell"],"description":"The type of the local shell tool. Always local_shell."}},"required":["type"],"description":"A tool that allows the model to execute shell commands in a local environment."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"A JSON schema object describing the parameters of the function."},"strict":{"type":"boolean","description":"Whether to enforce strict parameter validation."},"type":{"type":"string","enum":["function"],"description":"The type of the function tool. Always function."},"description":{"type":"string","description":"A description of the function. Used by the model to determine whether or not to call the function."}},"required":["name","parameters","strict","type"],"description":"Defines a function in your own code the model can choose to call."}]},"description":"An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter."},"top_p":{"type":"number","nullable":true,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both."},"truncation":{"type":"string","enum":["auto","disabled"],"default":"disabled","description":"The truncation strategy to use for the model response.\n\n    - auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.\n    - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.\n"}},"required":["model","input"]}}},"paths":{"/v1/responses":{"post":{"operationId":"ResponseApiController_createResponse_v1","summary":"","parameters":[],"requestBody":{"required":true,"content":{"application/json":{"schema":{"$ref":"#/components/schemas/Response.v1.CreateResponsePayload"}}}},"responses":{"201":{"description":""}},"tags":["Response Api"]}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-4o",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-4o',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-BKKZhTdruxKWjdUlq29ooeew185LD', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! üòä How can I help you today?', 'refusal': None, 'annotations': []}}], 'created': 1744185349, 'model': 'chatgpt-4o-latest', 'usage': {'prompt_tokens': 84, 'completion_tokens': 347, 'total_tokens': 431, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'system_fingerprint': 'fp_d04424daa8'}
```

{% endcode %}

</details>

## Code Example #2: Using /responses Endpoint

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json   # for getting a structured output with indentation

response = requests.post(
    "https://api.aimlapi.com/v1/responses",
    headers={
        "Content-Type":"application/json", 

        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-4o",
        "input":"Hello"  # Insert your question for the model here, instead of Hello   
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/responses', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4o',
        input: 'Hello',  // Insert your question here, instead of Hello 
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "resp_686ba45ce63481a2a4b1fad55d2bea8102a1cc22f1a1bcf1",
  "object": "response",
  "created_at": 1751884892,
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 512,
  "model": "gpt-4o",
  "output": [
    {
      "id": "rs_686ba463d18481a29dde85cfd7b055bf02a1cc22f1a1bcf1",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "msg_686ba463d4e081a2b2e2aff962ab00f702a1cc22f1a1bcf1",
      "type": "message",
      "status": "in_progress",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Hello! How can I help you today?"
        }
      ],
      "role": "assistant"
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": "medium",
    "summary": null
  },
  "temperature": 1,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 294,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 2520,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 2814
  },
  "metadata": {},
  "output_text": "Hello! How can I help you today?"
}
```

{% endcode %}

</details>


# gpt-4o-mini

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>gpt-4o-mini</code></li><li><code>gpt-4o-mini-2024-07-18</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=gpt-4o-mini&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

OpenAI's latest cost-efficient model designed to deliver advanced natural language processing and multimodal capabilities. It aims to make AI more accessible and affordable, significantly enhancing the range of applications that can utilize AI technology.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["gpt-4o-mini","gpt-4o-mini-2024-07-18"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."}},"required":["model","messages"],"title":"gpt-4o-mini"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

### Responses Endpoint

This endpoint is currently used *only* with OpenAI models. Some models support both the `/chat/completions` and `/responses` endpoints, while others support only one of them. OpenAI has announced plans to expand the capabilities of the `/responses` endpoint in the future.

## POST /v1/responses

>

```json
{"openapi":"3.0.0","info":{"title":"AI/ML Gateway","version":"1.0"},"servers":[{"url":"https://api.aimlapi.com"}],"security":[{"access-token":[]}],"components":{"securitySchemes":{"access-token":{"scheme":"bearer","bearerFormat":"<YOUR_AIMLAPI_KEY>","type":"http","description":"Bearer key"}},"schemas":{"Response.v1.CreateResponsePayload":{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o3-mini","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16","openai/o3-2025-04-16","o1","openai/o3-pro","openai/gpt-5-2025-08-07","openai/gpt-5-mini-2025-08-07","openai/gpt-5-nano-2025-08-07","openai/gpt-5-chat-latest","openai/gpt-5-pro","openai/gpt-5-1","openai/gpt-5-1-chat-latest","openai/gpt-5-1-codex","openai/gpt-5-1-codex-mini"],"description":"Model ID used to generate the response."},"background":{"type":"boolean","default":false,"description":"Whether to run the model response in the background."},"input":{"anyOf":[{"type":"string","description":"A text input to the model, equivalent to a text input with the user role."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","assistant","system","developer"],"description":"The role of the message input."},"content":{"anyOf":[{"type":"string","description":"A text input to the model."},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or audio input to the model, used to generate a response. Can also contain previous assistant responses."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role. Messages with the assistant role are presumed to have been generated by the model in previous interactions."},{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","system","developer"],"description":"The role of the message input."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of item."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the output message."},"role":{"type":"string","enum":["assistant"],"description":"The role of the output message. Always assistant."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the message input."},"type":{"type":"string","enum":["message"],"description":"The type of the output message. Always message."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"annotations":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"type":{"type":"string","enum":["url_citation"],"description":"The type of the URL citation. Always url_citation."},"url":{"type":"string","format":"uri","description":"The URL of the web resource."}},"required":["end_index","start_index","title","type","url"],"description":"A citation for a web resource used to generate a model response."}]},"description":"The annotations of the text output."},"text":{"type":"string","description":"The text output from the model."},"type":{"type":"string","enum":["output_text"],"description":"The type of the output text. Always output_text."},"logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"},"top_logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"}},"required":["bytes","logprob","token"]}}},"required":["bytes","logprob","token","top_logprobs"]}}},"required":["annotations","text","type"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal explanationfrom the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the refusal. Always refusal."}},"required":["refusal","type"]}],"description":"The content of the output message."}}},"required":["id","role","status","type","content"],"description":"An output message from the model."},{"type":"object","properties":{"action":{"oneOf":[{"type":"object","properties":{"button":{"type":"string","enum":["left","right","wheel","back","forward"],"description":"Indicates which mouse button was pressed during the click."},"type":{"type":"string","enum":["click"],"description":"Specifies the event type. For a click action, this property is always set to click."},"x":{"type":"integer","description":"The x-coordinate where the click occurred."},"y":{"type":"integer","description":"The y-coordinate where the click occurred."}},"required":["button","type","x","y"],"description":"A click action."},{"type":"object","properties":{"type":{"type":"string","enum":["double_click"],"description":"Specifies the event type. For a double click action, this property is always set to double_click."},"x":{"type":"integer","description":"The x-coordinate where the double click occurred."},"y":{"type":"integer","description":"The y-coordinate where the double click occurred."}},"required":["type","x","y"],"description":"A double click action."},{"type":"object","properties":{"path":{"type":"array","items":{"type":"object","properties":{"x":{"type":"integer","description":"The y-coordinate."},"y":{"type":"integer","description":"The y-coordinate."}},"required":["x","y"]},"description":"An array of coordinates representing the path of the drag action. Coordinates will appear as an array of objects, eg"},"type":{"type":"string","enum":["drag"],"description":"Specifies the event type. For a drag action, this property is always set to drag."}},"required":["path","type"],"description":"A drag action."},{"type":"object","properties":{"keys":{"type":"array","items":{"type":"string"},"description":"The combination of keys the model is requesting to be pressed. This is an array of strings, each representing a key."},"type":{"type":"string","enum":["keypress"],"description":"Specifies the event type. For a keypress action, this property is always set to keypress."}},"required":["keys","type"],"description":"A collection of keypresses the model would like to perform."},{"type":"object","properties":{"type":{"type":"string","enum":["move"],"description":"Specifies the event type. For a move action, this property is always set to move."},"x":{"type":"integer","description":"The x-coordinate to move to."},"y":{"type":"integer","description":"The y-coordinate to move to."}},"required":["type","x","y"],"description":"A mouse move action."},{"type":"object","properties":{"type":{"type":"string","enum":["screenshot"],"description":"Specifies the event type. For a screenshot action, this property is always set to screenshot."}},"required":["type"],"description":"A screenshot action."},{"type":"object","properties":{"type":{"type":"string","enum":["scroll"],"description":"Specifies the event type. For a scroll action, this property is always set to scroll."},"scroll_x":{"type":"integer","description":"The horizontal scroll distance."},"scroll_y":{"type":"integer","description":"The vertical scroll distance."},"x":{"type":"integer","description":"The x-coordinate where the scroll occurred."},"y":{"type":"integer","description":"The y-coordinate where the scroll occurred."}},"required":["type","scroll_x","scroll_y","x","y"],"description":"A scroll action."},{"type":"object","properties":{"type":{"type":"string","enum":["type"],"description":"Specifies the event type. For a type action, this property is always set to type."},"text":{"type":"string","description":"The text to type."}},"required":["type","text"],"description":"An action to type in text."},{"type":"object","properties":{"type":{"type":"string","enum":["wait"],"description":"Specifies the event type. For a wait action, this property is always set to wait."}},"required":["type"],"description":"A wait action."}]},"call_id":{"type":"string","description":"An identifier used when responding to the tool call with output."},"id":{"type":"string","description":"The unique ID of the computer call."},"pending_safety_checks":{"type":"array","items":{"type":"object","properties":{"code":{"type":"string","description":"The type of the pending safety check."},"id":{"type":"string","description":"The ID of the pending safety check."},"message":{"type":"string","description":"Details about the pending safety check."}},"required":["code","id","message"]},"description":"The pending safety checks for the computer call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."},"type":{"type":"string","enum":["computer_call"],"description":"The type of the computer call. Always computer_call."}},"required":["action","call_id","id","pending_safety_checks","status","type"],"description":"A tool call to a computer use tool."},{"type":"object","properties":{"call_id":{"type":"string","description":"The ID of the computer tool call that produced the output."},"output":{"type":"object","properties":{"type":{"type":"string","enum":["computer_screenshot"],"description":"Specifies the event type. For a computer screenshot, this property is always set to computer_screenshot."},"image_url":{"type":"string","format":"uri","description":"The URL of the screenshot image."}},"required":["type"],"description":"A computer screenshot image used with the computer use tool."},"type":{"type":"string","enum":["computer_call_output"],"description":"The type of the computer tool call output. Always computer_call_output."},"acknowledged_safety_checks":{"type":"array","nullable":true,"items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the pending safety check."},"code":{"type":"string","nullable":true,"description":"The type of the pending safety check."},"message":{"type":"string","nullable":true,"description":"Details about the pending safety check."}},"required":["id"]},"description":"The safety checks reported by the API that have been acknowledged by the developer."},"id":{"type":"string","nullable":true,"description":"The ID of the computer tool call output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the message input."}},"required":["call_id","output","type"],"description":"The output of a computer tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the web search tool call."},"status":{"type":"string","enum":["in_progress","completed","searching","failed"],"description":"The status of the web search tool call."},"type":{"type":"string","enum":["web_search_call"],"description":"The type of the web search tool call. Always web_search_call."}},"required":["id","status","type"],"description":"The results of a web search tool call."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments to pass to the function."},"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"name":{"type":"string","description":"The name of the function to run."},"type":{"type":"string","enum":["function_call"],"description":"The type of the function tool call. Always function_call."},"id":{"type":"string","description":"The unique ID of the function tool call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["arguments","call_id","name","type"],"description":"A tool call to run a function."},{"type":"object","properties":{"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"output":{"anyOf":[{"type":"string","description":"A JSON string of the output of the function tool call."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}}]},"type":{"type":"string","enum":["function_call_output"],"description":"The type of the function tool call output. Always function_call_output."},"id":{"type":"string","nullable":true,"description":"The unique ID of the function tool call output. Populated when this item is returned via API."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["call_id","output","type"],"description":"The output of a function tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique identifier of the reasoning content."},"summary":{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"A short summary of the reasoning used by the model when generating the response."},"type":{"type":"string","enum":["summary_text"],"description":"The type of the object. Always summary_text."}},"required":["text","type"]},"description":"Reasoning text contents."},"type":{"type":"string","enum":["reasoning"],"description":"The type of the object. Always reasoning."},"encrypted_content":{"type":"string","nullable":true,"description":"The encrypted content of the reasoning item - populated when a response is generated with reasoning.encrypted_content in the include parameter."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","summary","type"],"description":"A description of the chain of thought used by a reasoning model while generating a response."},{"type":"object","properties":{"code":{"type":"string","description":"The code to run, or null if not available."},"id":{"type":"string","description":"The unique ID of the code interpreter tool call."},"outputs":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"logs":{"type":"string","description":"The logs output from the code interpreter."},"type":{"type":"string","enum":["logs"],"description":"The type of the output. Always 'logs'."}},"required":["logs","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"url":{"type":"string"}},"required":["type","url"]}]},"description":"The outputs generated by the code interpreter, such as logs or images. Can be null if no outputs are available."},"status":{"type":"string","enum":["in_progress","completed","interpreting"],"description":"The status of the code interpreter tool call."},"type":{"type":"string","enum":["code_interpreter_call"],"description":"The type of the code interpreter tool call. Always code_interpreter_call."},"container_id":{"type":"string","description":"The ID of the container used to run the code."}},"required":["code","id","outputs","status","type","container_id"],"description":"A tool call to run code."},{"type":"object","properties":{"action":{"type":"object","properties":{"command":{"type":"array","items":{"type":"string"},"description":"The command to run."},"env":{"type":"object","additionalProperties":{"type":"string"},"description":"Environment variables to set for the command."},"type":{"type":"string","enum":["exec"],"description":"The type of the local shell action. Always exec."},"timeout_ms":{"type":"integer","nullable":true,"description":"Optional timeout in milliseconds for the command."},"user":{"type":"string","nullable":true,"description":"Optional user to run the command as."},"working_directory":{"type":"string","nullable":true,"description":"Optional working directory to run the command in."}},"required":["command","env","type"],"description":"Execute a shell command on the server."},"call_id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"id":{"type":"string","description":"The unique ID of the local shell call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the local shell call."},"type":{"type":"string","enum":["local_shell_call"],"description":"The type of the local shell call. Always local_shell_call."}},"required":["action","call_id","id","status","type"],"description":"A tool call to run a command on the local shell."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"output":{"type":"string","description":"A JSON string of the output of the local shell tool call."},"type":{"type":"string","enum":["local_shell_call_output"],"description":"The type of the local shell tool call output. Always local_shell_call_output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","output","type"],"description":"The output of a local shell tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the list."},"server_label":{"type":"string","description":"The label of the MCP server."},"tools":{"type":"array","items":{"type":"object","properties":{"input_schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The JSON schema describing the tool's input."},"name":{"type":"string","description":"The name of the tool."},"annotations":{"type":"object","nullable":true,"additionalProperties":{"nullable":true},"description":"Additional annotations about the tool."},"description":{"type":"string","nullable":true,"description":"The description of the tool."}},"required":["input_schema","name"]},"description":"The tools available on the server."},"type":{"type":"string","enum":["mcp_list_tools"],"description":"The type of the item. Always mcp_list_tools."},"error":{"type":"string","nullable":true,"description":"Error message if the server could not list tools."}},"required":["id","server_label","tools","type"],"description":"A list of tools available on an MCP server."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of arguments for the tool."},"id":{"type":"string","description":"The unique ID of the approval request."},"name":{"type":"string"},"server_label":{"type":"string","description":"The name of the tool to run."},"type":{"type":"string","enum":["mcp_approval_request"],"description":"The type of the item. Always mcp_approval_request."}},"required":["arguments","id","name","server_label","type"],"description":"A request for human approval of a tool invocation."},{"type":"object","properties":{"approval_request_id":{"type":"string","description":"The ID of the approval request being answered."},"approve":{"type":"boolean","description":"Whether the request was approved."},"type":{"type":"string","enum":["mcp_approval_response"],"description":"The type of the item. Always mcp_approval_response."},"id":{"type":"string","nullable":true,"description":"The unique ID of the approval response."},"reason":{"type":"string","nullable":true,"description":"Optional reason for the decision."}},"required":["approval_request_id","approve","type"],"description":"A response to an MCP approval request."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments passed to the tool."},"id":{"type":"string","description":"The unique ID of the tool call."},"name":{"type":"string","description":"The name of the tool that was run."},"server_label":{"type":"string","description":"The label of the MCP server running the tool."},"type":{"type":"string","enum":["mcp_call"],"description":"The type of the item. Always mcp_call."},"error":{"type":"string","nullable":true,"description":"The error from the tool call, if any."},"output":{"type":"string","nullable":true,"description":"The output from the tool call."}},"required":["arguments","id","name","server_label","type"],"description":"An invocation of a tool on an MCP server."},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the item to reference."},"type":{"type":"string","nullable":true,"enum":["item_reference"],"description":"The type of item to reference. Always item_reference."}},"required":["id"],"description":"An internal identifier for an item to reference."}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or file inputs to the model, used to generate a response."},"include":{"type":"array","nullable":true,"items":{"type":"string","enum":["message.input_image.image_url","computer_call_output.output.image_url","reasoning.encrypted_content","code_interpreter_call.outputs"]},"description":"Specify additional output data to include in the model response. Currently supported values are:\n    - code_interpreter_call.outputs: Includes the outputs of python code execution in code interpreter tool call items.\n    - computer_call_output.output.image_url: Include image urls from the computer call output.\n    - file_search_call.results: Include the search results of the file search tool call.\n    - message.output_text.logprobs: Include logprobs with assistant messages.\n    - reasoning.encrypted_content: Includes an encrypted version of reasoning tokens in reasoning item outputs. This enables reasoning items to be used in multi-turn conversations when using the Responses API statelessly (like when the store parameter is set to false, or when an organization is enrolled in the zero data retention program).\n"},"instructions":{"type":"string","nullable":true,"description":"A system (or developer) message inserted into the model's context.\n\nWhen using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses."},"max_output_tokens":{"type":"integer","default":512,"description":"An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n\nKeys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"parallel_tool_calls":{"type":"boolean","nullable":true,"description":"Whether to allow the model to run tool calls in parallel."},"previous_response_id":{"type":"string","nullable":true,"description":"The unique ID of the previous response to the model. Use this to create multi-turn conversations."},"prompt":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"The unique identifier of the prompt template to use."},"variables":{"type":"object","nullable":true,"additionalProperties":{"anyOf":[{"type":"string"},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}]},"description":"Optional map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input types like images or files."},"version":{"type":"string","nullable":true,"description":"Optional version of the prompt template."}},"required":["id"],"description":"Reference to a prompt template and its variables."},"reasoning":{"type":"object","nullable":true,"properties":{"effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"summary":{"type":"string","nullable":true,"enum":["auto","concise","detailed"],"description":"A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process."}},"description":"o-series models only\nConfiguration options for reasoning models."},"store":{"type":"boolean","nullable":true,"default":false,"description":"Whether to store the generated model response for later retrieval via API."},"stream":{"type":"boolean","nullable":true,"default":false,"description":"If set to true, the model response data will be streamed to the client as it is generated using server-sent events. "},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"text":{"type":"object","properties":{"format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"type":{"type":"string","enum":["json_schema"]},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name","schema","type"],"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["format"],"description":"Configuration options for a text response from the model. Can be plain text or structured JSON data."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"Controls which (if any) tool is called by the model.\n\nnone means the model will not call any tool and instead generates a message.\n\nauto means the model can pick between generating a message or calling one or more tools.\n\nrequired means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11","computer_use_preview","code_interpreter","mcp"]}},"required":["type"],"description":"Indicates that the model should use a built-in tool to generate a response."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"type":{"type":"string","enum":["function"],"description":"For function calling, the type is always function."}},"required":["name","type"]}],"description":"How the model should select which tool (or tools) to use when generating a response."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11"],"description":"The type of the web search tool. One of web_search_preview or web_search_preview_2025_03_11."},"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."},"city":{"type":"string","nullable":true,"description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","nullable":true,"pattern":"^[A-Z]{2}$","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","nullable":true,"description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","nullable":true,"description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"required":["type"],"description":"The user's location"}},"required":["type"],"description":"This tool searches the web for relevant results to use in a response."},{"type":"object","properties":{"display_height":{"type":"integer","description":"The height of the computer display."},"display_width":{"type":"integer","description":"The width of the computer display."},"environment":{"type":"string","enum":["windows","mac","linux","ubuntu","browser"],"description":"The type of computer environment to control."},"type":{"type":"string","enum":["computer_use_preview"],"description":"The type of the computer use tool. Always computer_use_preview."}},"required":["display_height","display_width","environment","type"],"description":"A tool that controls a virtual computer."},{"type":"object","properties":{"server_label":{"type":"string","description":"A label for this MCP server, used to identify it in tool calls."},"server_url":{"type":"string","description":"The URL for the MCP server."},"type":{"type":"string","enum":["mcp"],"description":"The type of the MCP tool. Always mcp."},"allowed_tools":{"anyOf":[{"type":"array","items":{"type":"string"},"description":"A string array of allowed tool names."},{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of allowed tool names."}},"description":"A filter object to specify which tools are allowed."},{"nullable":true}],"description":"List of allowed tool names or a filter object."},"headers":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Optional HTTP headers to send to the MCP server. Use for authentication or other purposes."},"require_approval":{"anyOf":[{"type":"string","enum":["always","never"]},{"type":"object","properties":{"always":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that require approval."}},"description":"A list of tools that always require approval."},"never":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that do not require approval."}},"description":"A list of tools that never require approval."}}},{"nullable":true}],"description":"Specify which of the MCP server's tools require approval."}},"required":["server_label","server_url","type"],"description":"Give the model access to additional tools via remote Model Context Protocol (MCP) servers."},{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of the code interpreter tool. Always code_interpreter."},"container":{"anyOf":[{"type":"string"},{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]}],"description":"The container ID."}},"required":["type","container"],"description":"A tool that runs Python code to help generate a response to a prompt."},{"type":"object","properties":{"type":{"type":"string","enum":["local_shell"],"description":"The type of the local shell tool. Always local_shell."}},"required":["type"],"description":"A tool that allows the model to execute shell commands in a local environment."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"A JSON schema object describing the parameters of the function."},"strict":{"type":"boolean","description":"Whether to enforce strict parameter validation."},"type":{"type":"string","enum":["function"],"description":"The type of the function tool. Always function."},"description":{"type":"string","description":"A description of the function. Used by the model to determine whether or not to call the function."}},"required":["name","parameters","strict","type"],"description":"Defines a function in your own code the model can choose to call."}]},"description":"An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter."},"top_p":{"type":"number","nullable":true,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both."},"truncation":{"type":"string","enum":["auto","disabled"],"default":"disabled","description":"The truncation strategy to use for the model response.\n\n    - auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.\n    - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.\n"}},"required":["model","input"]}}},"paths":{"/v1/responses":{"post":{"operationId":"ResponseApiController_createResponse_v1","summary":"","parameters":[],"requestBody":{"required":true,"content":{"application/json":{"schema":{"$ref":"#/components/schemas/Response.v1.CreateResponsePayload"}}}},"responses":{"201":{"description":""}},"tags":["Response Api"]}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-4o-mini",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-4o-mini',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-BKKaTWquxfp3dbSlNvUKM6mXwmZ78', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today?', 'refusal': None, 'annotations': []}}], 'created': 1744185397, 'model': 'gpt-4o-mini-2024-07-18', 'usage': {'prompt_tokens': 3, 'completion_tokens': 13, 'total_tokens': 16, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'system_fingerprint': 'fp_b376dfbbd5'}
```

{% endcode %}

</details>

## Code Example #2: Using /responses Endpoint

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json   # for getting a structured output with indentation

response = requests.post(
    "https://api.aimlapi.com/v1/responses",
    headers={
        "Content-Type":"application/json", 

        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-4o-mini",
        "input":"Hello"  # Insert your question for the model here, instead of Hello   
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/responses', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        input: 'Hello',  // Insert your question here, instead of Hello 
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "resp_686ba45ce63481a2a4b1fad55d2bea8102a1cc22f1a1bcf1",
  "object": "response",
  "created_at": 1751884892,
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 512,
  "model": "gpt-4o-mini",
  "output": [
    {
      "id": "rs_686ba463d18481a29dde85cfd7b055bf02a1cc22f1a1bcf1",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "msg_686ba463d4e081a2b2e2aff962ab00f702a1cc22f1a1bcf1",
      "type": "message",
      "status": "in_progress",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Hello! How can I help you today?"
        }
      ],
      "role": "assistant"
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": "medium",
    "summary": null
  },
  "temperature": 1,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 294,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 2520,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 2814
  },
  "metadata": {},
  "output_text": "Hello! How can I help you today?"
}
```

{% endcode %}

</details>


# gpt-4o-audio-preview

{% hint style="info" %}
This documentation is valid for the following list of our models:

* `gpt-4o-audio-preview`
  {% endhint %}

## Model Overview

A text model with a support for audio prompts and the ability to generate spoken audio responses. This expansion enhances the potential for AI applications in text and voice-based interactions and audio analysis. You can choose from a wide range of audio formats for output and specify the voice the model will use for audio responses.

## Setup your API Key

If you don‚Äôt have an API key for the AI/ML API yet, feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["gpt-4o-audio-preview"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"}},"required":["model","messages"],"title":"gpt-4o-audio-preview"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
from openai import OpenAI
import base64
import os

client = OpenAI(
    base_url = "https://api.aimlapi.com",
    # Insert your AI/ML API key instead of <YOUR_AIMLAPI_KEY>:
    api_key = "<YOUR_AIMLAPI_KEY>"
)

def main():
    response = client.chat.completions.create(
        model="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "alloy", "format": "wav"},
        messages=[
            {
                "role": "system",
                "content": "Speak english" # Your instructions for the model
            },
            {   
                "role": "user",
                "content": "Hello" # Your question (insert it istead of Hello)
            }
        ],
        max_tokens=6000,  
    )

    wav_bytes = base64.b64decode(response.choices[0].message.audio.data)
    with open("audio.wav", "wb") as f:
        f.write(wav_bytes)
    dist = os.path.abspath("audio.wav")
    print("Audio saved to:", dist)
     
if __name__ == "__main__":
    main()
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% hint style="warning" %}
We‚Äôve omitted 99% of the base64-encoded file for brevity ‚Äî even for such a short model response, it‚Äôs still extremely large.
{% endhint %}

{% code overflow="wrap" %}

```json5
ChatCompletion(id='chatcmpl-BrgY0KMxWgy1EHUxYJC49MuMNmdOP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=ChatCompletionAudio(id='audio_686f73ecf0648191a602c4f315cad928', data='UklGRv////9XQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAAZGF0Yf////8YABAAEgAXABEAFwASABQAFQAVABcADAAPAAsAEgAOABEACwANABAACgALAAMADQAHABAACAAKAAcACgAFAAQACAAHAAUABQAFAAIACAAAAAgA/v8BAP7////8//b/AQD1/wMA9P/9//X/+f/3//H/+v/1//3/6v/5/+n/9P/u//X/8v/w//P/7v/z/+v/9f/q//T/6//r/+r/6P/s/+P/7P/l/+b/4f/g/+X/3//m/9//6f/l/+X/6f/e/+r/3//l/9n/3f/g/9r/2//V/9z/1P/g/93/4//f/+T/5//q/+X/4//h/9v/3f/X/97/0//Z/9L/2v/Z/9v/2//f/+X/4P/k/+P/4v/h/+H/3P/i/9//3P/f/9n/3f/d/+P/3f/k/97/5P/g/+n/5f/p/+r/6//n/+z/7f/t//D/6//v/+v/6v/m/+L/4v/n/+r/6P/u/+7/9v/7/wEAAQAAAP7/+P/6//L/7v/o/+H/5f/b/+f/4v/1//L///8EAAIADQAJABkADwARAAoADAABAP7/+//5//n/9f8AAPr/BAD//AwABAAYA//8CAP3/AgABAAUABAD8/wQAAQAFAP7/BAABAAEA/////wIAAAADAAIA/v/+//z////7/wEA/P8AAP///v8EAPz//P/9/wQAAQD8/wAAAQD///z/AgD7//7/+/8AAAAA+/8AAP3//v/9/wUAAwD///7/AwACAAIAAgAAAPv/AQD8/wYAAgD7//r/AgABAAAABQD5/wUAAgADAP//AQAFAPn/AQD7/wYA+//9//n//v/7//r/AAD8/wMA//8BAP//AwD9/wMA/f/+//z/+//9//n//v/+/wQAAgACAP7/AwD//wEAAAD8//v/AgD6/wQA/f8AAPn/AAD9//z/AQD//wEA/P/6//7//P/+//7//P8AAPj//P///wIA+v/9/wAA+/8CAP///f/9//r/BQD+/wgAAAADAP3/AQACAAMABAD8/wEA+/8GAP3//v/6/wIA///9/wEA+v8EAPf/AAD5/wUA9/8AAAAA/P8AAPn/AQD3/wMA/P/8//3//v//////AAD8/////P8CAP//BAD7/wUA/P8CAP3///8AAPn/AwD3/wkA/f8FAPr/AwD9//3/AQD1/wEA+//+//v/AwADAAAA///9/wIA/f8DAPz//P/9///////6//7//f8AAAAAAQD+//v/AQD7/////P8AAP7//v////r//v8BAAQA+v/+//z//P8AAP7/AwD8/wAAAQD4/////v8DAP7///8AAPz//P/7/wIA///8//z//f/8//z/AQD8//v//f/7//v/+f/8//z/+/////z//v8AAAAA/v/6/wAA/f8AAPj/AAD+/wIAAgD5//3//P/+//r//v///wAA///9/////                                                                                                                                                     !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!WE‚ÄôVE OMITTED 90% OF THE BASE64-ENCODED FILE FOR BREVITY ‚Äî EVEN FOR SUCH A SHORT MODEL RESPONSE, IT‚ÄôS STILL EXTREMELY LARGE.                                    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!wUAAwAFAAQABgACAAIAAgACAAYAAwAFAAEAAQD///7/AAACAAQAAAD+////AQAAAP//AQADAAMAAgADAAIAAAACAAUABQADAAUABgAGAAcABgAGAAUABQAFAAYABQAFAAgABwAKAAoABwAJAAUABwAIAAgACQAGAAgABQAJAAcABwAJAAcACgAGAAgABAAEAAMAAgAGAAQABAADAAYABQAEAAYAAwAFAAIAAwAGAAYABQADAAQAAAABAAEAAgACAAEAAAD8/////f/+//r/+f/5//f/+P/2//j/9//7//j//P/7//z/+v/6//z/+P/6//f/+//6//r/+v/4//v/+v/6//r//f/6//n//f/8//3/+//9//3////9//3//f/8//v/+/8AAP3//f/6//r//v/6//z/9//6//j/+f/4//r/+f/3//f/9f/3//L/8f/0//P/9P/1//X/8//1//H/9f/z//b/9v/2//j/9P/2//P/+P/0//f/+P/1//X/9f/2//X/9P/1//L/8v/1//P/9P/1//X/9v/4//X/9v/3//n/+v/6//n/+f/3//r/8f/1//P/8//4//j//f/6//v/+P/+//v/+P////z/AwABAA0AAgAOAAYADgAPAA0ACwAEAAwABAD+//3//v///wAABQAAAA4AFwAGABgAFQAgAAQA8f8BAPj/NQAUAAoAJAAXADsABQD9//v/DwAKABYABQA7AC4A2/8=', expires_at=1752138236, transcript="Hi there! How's it going?"), function_call=None, tool_calls=None))], created=1752134636, model='gpt-4o-audio-preview-2025-06-03', object='chat.completion', service_tier=None, system_fingerprint='fp_b5d60d6081', usage=CompletionUsage(completion_tokens=5838, prompt_tokens=74, total_tokens=5912, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=33, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=14), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0, text_tokens=14, image_tokens=0)))
```

{% endcode %}

</details>

{% embed url="<https://drive.google.com/file/d/1JUWnbhFDwMCW1JqWzkvDXNqI6QNEOuVf/view?usp=sharing>" %}


# gpt-4o-mini-audio-preview

{% hint style="info" %}
This documentation is valid for the following list of our models:

* `gpt-4o-mini-audio-preview`
  {% endhint %}

## Model Overview

A preview release of the smaller GPT-4o Audio mini model. Handles both audio and text as input and output via the REST API. You can choose from a wide range of audio formats for output and specify the voice the model will use for audio responses.

## Setup your API Key

If you don‚Äôt have an API key for the AI/ML API yet, feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["gpt-4o-mini-audio-preview"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"}},"required":["model","messages"],"title":"gpt-4o-mini-audio-preview"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
from openai import OpenAI
import base64
import os

client = OpenAI(
    base_url = "https://api.aimlapi.com",
    # Insert your AI/ML API key instead of <YOUR_AIMLAPI_KEY>:
    api_key = "<YOUR_AIMLAPI_KEY>"
)

def main():
    response = client.chat.completions.create(
        model="gpt-4o-mini-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "alloy", "format": "wav"},
        messages=[
            {
                "role": "system",
                "content": "Speak english"  # Your instructions for the model
            },
            {   
                "role": "user",
                "content": "Hello"  # Your question (insert it istead of Hello)
            }
        ],
        max_tokens=6000,  
    )

    wav_bytes = base64.b64decode(response.choices[0].message.audio.data)
    with open("audio.wav", "wb") as f:
        f.write(wav_bytes)
    dist = os.path.abspath("audio.wav")
    print("Audio saved to:", dist)
     
if __name__ == "__main__":
    main()
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% hint style="warning" %}
We‚Äôve omitted 99% of the base64-encoded file for brevity ‚Äî even for such a short model response, it‚Äôs still extremely large.
{% endhint %}

{% code overflow="wrap" %}

```json5
ChatCompletion(id='chatcmpl-BrghGGR73s5Wt5thg4mhAxquxzmBi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=ChatCompletionAudio(id='audio_686f762b97b08191bb5ea391c6b41e1c', data='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!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!WE‚ÄôVE OMITTED 90% OF THE BASE64-ENCODED FILE FOR BREVITY ‚Äî EVEN FOR SUCH A SHORT MODEL RESPONSE, IT‚ÄôS STILL EXTREMELY LARGE.                                    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!cAKAAoACsALAAuADIAMwA4ADsAOAA5ADgAOgA7ADoAOwA7AD8APQA+ADwAPQA+AD8AQQA+AD8APAA9ADsAOwA8ADwAOwA7ADoAOwA4ADoANQA1ADEAMQAyAC4ALAAnACUAIAAfABwAGgAaABUAFQASABAACgAIAAQA//8AAPv/+v/4//b/8v/0//L/9P/z//P/8//t/+7/6v/p/+f/5//o/+X/5P/k/+X/5f/l/+X/5P/h/97/3//g/93/2v/Z/9b/2P/Z/9j/1f/T/87/zv/O/87/zP/J/8j/zP/I/8f/w//C/8P/x//F/8b/xf/D/8P/w//F/8L/xf/J/8f/xf/H/8j/yv/K/8n/yv/L/8v/z//O/9D/zv/Q/9D/0v/Q/9P/1P/R/9P/1P/T/9X/1P/X/9b/2P/b/9n/2//c/97/3//h/97/3v/g/+P/5v/m/+T/5v/m/+n/5P/n/+X/5//u//D/9P/2//X/8//5//j/9///////AQAEAAsAAwAMAAQACgAPAA4ADgAJABEACQAEAAgACwALAA8AFgAWACUAKQAgACsAJQAvACAADwAbABoARgApACwANQArAEMAEQASAAoAEQAkADAAFABCAEEACQA=', expires_at=1752138811, transcript="Hi there! How's it going?"), function_call=None, tool_calls=None))], created=1752135210, model='gpt-4o-mini-audio-preview-2024-12-17', object='chat.completion', service_tier=None, system_fingerprint='fp_1dfa95e5cb', usage=CompletionUsage(completion_tokens=1278, prompt_tokens=4, total_tokens=1282, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=30, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=14), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0, text_tokens=14, image_tokens=0)))
Audio saved to: c:\Users\user\Documents\Python Scripts\LLMs\audio.wav
```

{% endcode %}

</details>

{% embed url="<https://drive.google.com/file/d/14rtmCmtPR5eKDLpRWIbkOtzlE-R1K5NM/view?usp=sharing>" %}


# gpt-4o-search-preview

{% hint style="info" %}
This documentation is valid for the following list of our models:

* `gpt-4o-search-preview`
  {% endhint %}

## Model Overview

A specialized model trained to understand and execute web search queries with the [Chat completions](https://docs.aimlapi.com/capabilities/completion-or-chat-models) API.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["gpt-4o-search-preview"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]}},"required":["model","messages"],"title":"gpt-4o-search-preview"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-4o-search-preview",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-4o-search-preview',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-2d186134-834f-4b68-9c61-62d5a4f67872', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today? ', 'refusal': None, 'annotations': []}}], 'created': 1744217100, 'model': 'gpt-4o-search-preview-2025-03-11', 'usage': {'prompt_tokens': 5, 'completion_tokens': 210, 'total_tokens': 215, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'system_fingerprint': ''}
```

{% endcode %}

</details>


# gpt-4o-mini-search-preview

{% hint style="info" %}
This documentation is valid for the following list of our models:

* `gpt-4o-mini-search-preview`
  {% endhint %}

## Model Overview

A specialized model trained to understand and execute web search queries with the [Chat completions](https://docs.aimlapi.com/capabilities/completion-or-chat-models) API.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["gpt-4o-mini-search-preview"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]}},"required":["model","messages"],"title":"gpt-4o-mini-search-preview"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"gpt-4o-mini-search-preview",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-4o-mini-search-preview',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-d5329df8-efab-48d8-b607-9e61dd14553b', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'message': {'role': 'assistant', 'content': 'Hello! How can I assist you today? ', 'refusal': None, 'annotations': []}}], 'created': 1744217025, 'model': 'gpt-4o-mini-search-preview-2025-03-11', 'usage': {'prompt_tokens': 0, 'completion_tokens': 13, 'total_tokens': 13, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'system_fingerprint': ''}
```

{% endcode %}

</details>


# o1

{% hint style="info" %}
This documentation is valid for the following list of our models:

* `o1`
  {% endhint %}

## Model Overview

A state-of-the-art language model designed to excel in complex reasoning tasks, including mathematical problem-solving, programming challenges, and scientific inquiries. The model integrates advanced reasoning capabilities through its innovative architecture, making it suitable for a wide range of applications that require deep understanding and logical deduction.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["o1"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"o1"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

### Responses Endpoint

This endpoint is currently used *only* with OpenAI models. Some models support both the `/chat/completions` and `/responses` endpoints, while others support only one of them. OpenAI has announced plans to expand the capabilities of the `/responses` endpoint in the future.

## POST /v1/responses

>

```json
{"openapi":"3.0.0","info":{"title":"AI/ML Gateway","version":"1.0"},"servers":[{"url":"https://api.aimlapi.com"}],"security":[{"access-token":[]}],"components":{"securitySchemes":{"access-token":{"scheme":"bearer","bearerFormat":"<YOUR_AIMLAPI_KEY>","type":"http","description":"Bearer key"}},"schemas":{"Response.v1.CreateResponsePayload":{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o3-mini","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16","openai/o3-2025-04-16","o1","openai/o3-pro","openai/gpt-5-2025-08-07","openai/gpt-5-mini-2025-08-07","openai/gpt-5-nano-2025-08-07","openai/gpt-5-chat-latest","openai/gpt-5-pro","openai/gpt-5-1","openai/gpt-5-1-chat-latest","openai/gpt-5-1-codex","openai/gpt-5-1-codex-mini"],"description":"Model ID used to generate the response."},"background":{"type":"boolean","default":false,"description":"Whether to run the model response in the background."},"input":{"anyOf":[{"type":"string","description":"A text input to the model, equivalent to a text input with the user role."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","assistant","system","developer"],"description":"The role of the message input."},"content":{"anyOf":[{"type":"string","description":"A text input to the model."},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or audio input to the model, used to generate a response. Can also contain previous assistant responses."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role. Messages with the assistant role are presumed to have been generated by the model in previous interactions."},{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","system","developer"],"description":"The role of the message input."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of item."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the output message."},"role":{"type":"string","enum":["assistant"],"description":"The role of the output message. Always assistant."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the message input."},"type":{"type":"string","enum":["message"],"description":"The type of the output message. Always message."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"annotations":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"type":{"type":"string","enum":["url_citation"],"description":"The type of the URL citation. Always url_citation."},"url":{"type":"string","format":"uri","description":"The URL of the web resource."}},"required":["end_index","start_index","title","type","url"],"description":"A citation for a web resource used to generate a model response."}]},"description":"The annotations of the text output."},"text":{"type":"string","description":"The text output from the model."},"type":{"type":"string","enum":["output_text"],"description":"The type of the output text. Always output_text."},"logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"},"top_logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"}},"required":["bytes","logprob","token"]}}},"required":["bytes","logprob","token","top_logprobs"]}}},"required":["annotations","text","type"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal explanationfrom the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the refusal. Always refusal."}},"required":["refusal","type"]}],"description":"The content of the output message."}}},"required":["id","role","status","type","content"],"description":"An output message from the model."},{"type":"object","properties":{"action":{"oneOf":[{"type":"object","properties":{"button":{"type":"string","enum":["left","right","wheel","back","forward"],"description":"Indicates which mouse button was pressed during the click."},"type":{"type":"string","enum":["click"],"description":"Specifies the event type. For a click action, this property is always set to click."},"x":{"type":"integer","description":"The x-coordinate where the click occurred."},"y":{"type":"integer","description":"The y-coordinate where the click occurred."}},"required":["button","type","x","y"],"description":"A click action."},{"type":"object","properties":{"type":{"type":"string","enum":["double_click"],"description":"Specifies the event type. For a double click action, this property is always set to double_click."},"x":{"type":"integer","description":"The x-coordinate where the double click occurred."},"y":{"type":"integer","description":"The y-coordinate where the double click occurred."}},"required":["type","x","y"],"description":"A double click action."},{"type":"object","properties":{"path":{"type":"array","items":{"type":"object","properties":{"x":{"type":"integer","description":"The y-coordinate."},"y":{"type":"integer","description":"The y-coordinate."}},"required":["x","y"]},"description":"An array of coordinates representing the path of the drag action. Coordinates will appear as an array of objects, eg"},"type":{"type":"string","enum":["drag"],"description":"Specifies the event type. For a drag action, this property is always set to drag."}},"required":["path","type"],"description":"A drag action."},{"type":"object","properties":{"keys":{"type":"array","items":{"type":"string"},"description":"The combination of keys the model is requesting to be pressed. This is an array of strings, each representing a key."},"type":{"type":"string","enum":["keypress"],"description":"Specifies the event type. For a keypress action, this property is always set to keypress."}},"required":["keys","type"],"description":"A collection of keypresses the model would like to perform."},{"type":"object","properties":{"type":{"type":"string","enum":["move"],"description":"Specifies the event type. For a move action, this property is always set to move."},"x":{"type":"integer","description":"The x-coordinate to move to."},"y":{"type":"integer","description":"The y-coordinate to move to."}},"required":["type","x","y"],"description":"A mouse move action."},{"type":"object","properties":{"type":{"type":"string","enum":["screenshot"],"description":"Specifies the event type. For a screenshot action, this property is always set to screenshot."}},"required":["type"],"description":"A screenshot action."},{"type":"object","properties":{"type":{"type":"string","enum":["scroll"],"description":"Specifies the event type. For a scroll action, this property is always set to scroll."},"scroll_x":{"type":"integer","description":"The horizontal scroll distance."},"scroll_y":{"type":"integer","description":"The vertical scroll distance."},"x":{"type":"integer","description":"The x-coordinate where the scroll occurred."},"y":{"type":"integer","description":"The y-coordinate where the scroll occurred."}},"required":["type","scroll_x","scroll_y","x","y"],"description":"A scroll action."},{"type":"object","properties":{"type":{"type":"string","enum":["type"],"description":"Specifies the event type. For a type action, this property is always set to type."},"text":{"type":"string","description":"The text to type."}},"required":["type","text"],"description":"An action to type in text."},{"type":"object","properties":{"type":{"type":"string","enum":["wait"],"description":"Specifies the event type. For a wait action, this property is always set to wait."}},"required":["type"],"description":"A wait action."}]},"call_id":{"type":"string","description":"An identifier used when responding to the tool call with output."},"id":{"type":"string","description":"The unique ID of the computer call."},"pending_safety_checks":{"type":"array","items":{"type":"object","properties":{"code":{"type":"string","description":"The type of the pending safety check."},"id":{"type":"string","description":"The ID of the pending safety check."},"message":{"type":"string","description":"Details about the pending safety check."}},"required":["code","id","message"]},"description":"The pending safety checks for the computer call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."},"type":{"type":"string","enum":["computer_call"],"description":"The type of the computer call. Always computer_call."}},"required":["action","call_id","id","pending_safety_checks","status","type"],"description":"A tool call to a computer use tool."},{"type":"object","properties":{"call_id":{"type":"string","description":"The ID of the computer tool call that produced the output."},"output":{"type":"object","properties":{"type":{"type":"string","enum":["computer_screenshot"],"description":"Specifies the event type. For a computer screenshot, this property is always set to computer_screenshot."},"image_url":{"type":"string","format":"uri","description":"The URL of the screenshot image."}},"required":["type"],"description":"A computer screenshot image used with the computer use tool."},"type":{"type":"string","enum":["computer_call_output"],"description":"The type of the computer tool call output. Always computer_call_output."},"acknowledged_safety_checks":{"type":"array","nullable":true,"items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the pending safety check."},"code":{"type":"string","nullable":true,"description":"The type of the pending safety check."},"message":{"type":"string","nullable":true,"description":"Details about the pending safety check."}},"required":["id"]},"description":"The safety checks reported by the API that have been acknowledged by the developer."},"id":{"type":"string","nullable":true,"description":"The ID of the computer tool call output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the message input."}},"required":["call_id","output","type"],"description":"The output of a computer tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the web search tool call."},"status":{"type":"string","enum":["in_progress","completed","searching","failed"],"description":"The status of the web search tool call."},"type":{"type":"string","enum":["web_search_call"],"description":"The type of the web search tool call. Always web_search_call."}},"required":["id","status","type"],"description":"The results of a web search tool call."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments to pass to the function."},"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"name":{"type":"string","description":"The name of the function to run."},"type":{"type":"string","enum":["function_call"],"description":"The type of the function tool call. Always function_call."},"id":{"type":"string","description":"The unique ID of the function tool call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["arguments","call_id","name","type"],"description":"A tool call to run a function."},{"type":"object","properties":{"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"output":{"anyOf":[{"type":"string","description":"A JSON string of the output of the function tool call."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}}]},"type":{"type":"string","enum":["function_call_output"],"description":"The type of the function tool call output. Always function_call_output."},"id":{"type":"string","nullable":true,"description":"The unique ID of the function tool call output. Populated when this item is returned via API."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["call_id","output","type"],"description":"The output of a function tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique identifier of the reasoning content."},"summary":{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"A short summary of the reasoning used by the model when generating the response."},"type":{"type":"string","enum":["summary_text"],"description":"The type of the object. Always summary_text."}},"required":["text","type"]},"description":"Reasoning text contents."},"type":{"type":"string","enum":["reasoning"],"description":"The type of the object. Always reasoning."},"encrypted_content":{"type":"string","nullable":true,"description":"The encrypted content of the reasoning item - populated when a response is generated with reasoning.encrypted_content in the include parameter."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","summary","type"],"description":"A description of the chain of thought used by a reasoning model while generating a response."},{"type":"object","properties":{"code":{"type":"string","description":"The code to run, or null if not available."},"id":{"type":"string","description":"The unique ID of the code interpreter tool call."},"outputs":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"logs":{"type":"string","description":"The logs output from the code interpreter."},"type":{"type":"string","enum":["logs"],"description":"The type of the output. Always 'logs'."}},"required":["logs","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"url":{"type":"string"}},"required":["type","url"]}]},"description":"The outputs generated by the code interpreter, such as logs or images. Can be null if no outputs are available."},"status":{"type":"string","enum":["in_progress","completed","interpreting"],"description":"The status of the code interpreter tool call."},"type":{"type":"string","enum":["code_interpreter_call"],"description":"The type of the code interpreter tool call. Always code_interpreter_call."},"container_id":{"type":"string","description":"The ID of the container used to run the code."}},"required":["code","id","outputs","status","type","container_id"],"description":"A tool call to run code."},{"type":"object","properties":{"action":{"type":"object","properties":{"command":{"type":"array","items":{"type":"string"},"description":"The command to run."},"env":{"type":"object","additionalProperties":{"type":"string"},"description":"Environment variables to set for the command."},"type":{"type":"string","enum":["exec"],"description":"The type of the local shell action. Always exec."},"timeout_ms":{"type":"integer","nullable":true,"description":"Optional timeout in milliseconds for the command."},"user":{"type":"string","nullable":true,"description":"Optional user to run the command as."},"working_directory":{"type":"string","nullable":true,"description":"Optional working directory to run the command in."}},"required":["command","env","type"],"description":"Execute a shell command on the server."},"call_id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"id":{"type":"string","description":"The unique ID of the local shell call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the local shell call."},"type":{"type":"string","enum":["local_shell_call"],"description":"The type of the local shell call. Always local_shell_call."}},"required":["action","call_id","id","status","type"],"description":"A tool call to run a command on the local shell."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"output":{"type":"string","description":"A JSON string of the output of the local shell tool call."},"type":{"type":"string","enum":["local_shell_call_output"],"description":"The type of the local shell tool call output. Always local_shell_call_output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","output","type"],"description":"The output of a local shell tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the list."},"server_label":{"type":"string","description":"The label of the MCP server."},"tools":{"type":"array","items":{"type":"object","properties":{"input_schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The JSON schema describing the tool's input."},"name":{"type":"string","description":"The name of the tool."},"annotations":{"type":"object","nullable":true,"additionalProperties":{"nullable":true},"description":"Additional annotations about the tool."},"description":{"type":"string","nullable":true,"description":"The description of the tool."}},"required":["input_schema","name"]},"description":"The tools available on the server."},"type":{"type":"string","enum":["mcp_list_tools"],"description":"The type of the item. Always mcp_list_tools."},"error":{"type":"string","nullable":true,"description":"Error message if the server could not list tools."}},"required":["id","server_label","tools","type"],"description":"A list of tools available on an MCP server."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of arguments for the tool."},"id":{"type":"string","description":"The unique ID of the approval request."},"name":{"type":"string"},"server_label":{"type":"string","description":"The name of the tool to run."},"type":{"type":"string","enum":["mcp_approval_request"],"description":"The type of the item. Always mcp_approval_request."}},"required":["arguments","id","name","server_label","type"],"description":"A request for human approval of a tool invocation."},{"type":"object","properties":{"approval_request_id":{"type":"string","description":"The ID of the approval request being answered."},"approve":{"type":"boolean","description":"Whether the request was approved."},"type":{"type":"string","enum":["mcp_approval_response"],"description":"The type of the item. Always mcp_approval_response."},"id":{"type":"string","nullable":true,"description":"The unique ID of the approval response."},"reason":{"type":"string","nullable":true,"description":"Optional reason for the decision."}},"required":["approval_request_id","approve","type"],"description":"A response to an MCP approval request."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments passed to the tool."},"id":{"type":"string","description":"The unique ID of the tool call."},"name":{"type":"string","description":"The name of the tool that was run."},"server_label":{"type":"string","description":"The label of the MCP server running the tool."},"type":{"type":"string","enum":["mcp_call"],"description":"The type of the item. Always mcp_call."},"error":{"type":"string","nullable":true,"description":"The error from the tool call, if any."},"output":{"type":"string","nullable":true,"description":"The output from the tool call."}},"required":["arguments","id","name","server_label","type"],"description":"An invocation of a tool on an MCP server."},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the item to reference."},"type":{"type":"string","nullable":true,"enum":["item_reference"],"description":"The type of item to reference. Always item_reference."}},"required":["id"],"description":"An internal identifier for an item to reference."}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or file inputs to the model, used to generate a response."},"include":{"type":"array","nullable":true,"items":{"type":"string","enum":["message.input_image.image_url","computer_call_output.output.image_url","reasoning.encrypted_content","code_interpreter_call.outputs"]},"description":"Specify additional output data to include in the model response. Currently supported values are:\n    - code_interpreter_call.outputs: Includes the outputs of python code execution in code interpreter tool call items.\n    - computer_call_output.output.image_url: Include image urls from the computer call output.\n    - file_search_call.results: Include the search results of the file search tool call.\n    - message.output_text.logprobs: Include logprobs with assistant messages.\n    - reasoning.encrypted_content: Includes an encrypted version of reasoning tokens in reasoning item outputs. This enables reasoning items to be used in multi-turn conversations when using the Responses API statelessly (like when the store parameter is set to false, or when an organization is enrolled in the zero data retention program).\n"},"instructions":{"type":"string","nullable":true,"description":"A system (or developer) message inserted into the model's context.\n\nWhen using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses."},"max_output_tokens":{"type":"integer","default":512,"description":"An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n\nKeys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"parallel_tool_calls":{"type":"boolean","nullable":true,"description":"Whether to allow the model to run tool calls in parallel."},"previous_response_id":{"type":"string","nullable":true,"description":"The unique ID of the previous response to the model. Use this to create multi-turn conversations."},"prompt":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"The unique identifier of the prompt template to use."},"variables":{"type":"object","nullable":true,"additionalProperties":{"anyOf":[{"type":"string"},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}]},"description":"Optional map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input types like images or files."},"version":{"type":"string","nullable":true,"description":"Optional version of the prompt template."}},"required":["id"],"description":"Reference to a prompt template and its variables."},"reasoning":{"type":"object","nullable":true,"properties":{"effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"summary":{"type":"string","nullable":true,"enum":["auto","concise","detailed"],"description":"A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process."}},"description":"o-series models only\nConfiguration options for reasoning models."},"store":{"type":"boolean","nullable":true,"default":false,"description":"Whether to store the generated model response for later retrieval via API."},"stream":{"type":"boolean","nullable":true,"default":false,"description":"If set to true, the model response data will be streamed to the client as it is generated using server-sent events. "},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"text":{"type":"object","properties":{"format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"type":{"type":"string","enum":["json_schema"]},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name","schema","type"],"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["format"],"description":"Configuration options for a text response from the model. Can be plain text or structured JSON data."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"Controls which (if any) tool is called by the model.\n\nnone means the model will not call any tool and instead generates a message.\n\nauto means the model can pick between generating a message or calling one or more tools.\n\nrequired means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11","computer_use_preview","code_interpreter","mcp"]}},"required":["type"],"description":"Indicates that the model should use a built-in tool to generate a response."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"type":{"type":"string","enum":["function"],"description":"For function calling, the type is always function."}},"required":["name","type"]}],"description":"How the model should select which tool (or tools) to use when generating a response."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11"],"description":"The type of the web search tool. One of web_search_preview or web_search_preview_2025_03_11."},"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."},"city":{"type":"string","nullable":true,"description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","nullable":true,"pattern":"^[A-Z]{2}$","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","nullable":true,"description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","nullable":true,"description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"required":["type"],"description":"The user's location"}},"required":["type"],"description":"This tool searches the web for relevant results to use in a response."},{"type":"object","properties":{"display_height":{"type":"integer","description":"The height of the computer display."},"display_width":{"type":"integer","description":"The width of the computer display."},"environment":{"type":"string","enum":["windows","mac","linux","ubuntu","browser"],"description":"The type of computer environment to control."},"type":{"type":"string","enum":["computer_use_preview"],"description":"The type of the computer use tool. Always computer_use_preview."}},"required":["display_height","display_width","environment","type"],"description":"A tool that controls a virtual computer."},{"type":"object","properties":{"server_label":{"type":"string","description":"A label for this MCP server, used to identify it in tool calls."},"server_url":{"type":"string","description":"The URL for the MCP server."},"type":{"type":"string","enum":["mcp"],"description":"The type of the MCP tool. Always mcp."},"allowed_tools":{"anyOf":[{"type":"array","items":{"type":"string"},"description":"A string array of allowed tool names."},{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of allowed tool names."}},"description":"A filter object to specify which tools are allowed."},{"nullable":true}],"description":"List of allowed tool names or a filter object."},"headers":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Optional HTTP headers to send to the MCP server. Use for authentication or other purposes."},"require_approval":{"anyOf":[{"type":"string","enum":["always","never"]},{"type":"object","properties":{"always":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that require approval."}},"description":"A list of tools that always require approval."},"never":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that do not require approval."}},"description":"A list of tools that never require approval."}}},{"nullable":true}],"description":"Specify which of the MCP server's tools require approval."}},"required":["server_label","server_url","type"],"description":"Give the model access to additional tools via remote Model Context Protocol (MCP) servers."},{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of the code interpreter tool. Always code_interpreter."},"container":{"anyOf":[{"type":"string"},{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]}],"description":"The container ID."}},"required":["type","container"],"description":"A tool that runs Python code to help generate a response to a prompt."},{"type":"object","properties":{"type":{"type":"string","enum":["local_shell"],"description":"The type of the local shell tool. Always local_shell."}},"required":["type"],"description":"A tool that allows the model to execute shell commands in a local environment."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"A JSON schema object describing the parameters of the function."},"strict":{"type":"boolean","description":"Whether to enforce strict parameter validation."},"type":{"type":"string","enum":["function"],"description":"The type of the function tool. Always function."},"description":{"type":"string","description":"A description of the function. Used by the model to determine whether or not to call the function."}},"required":["name","parameters","strict","type"],"description":"Defines a function in your own code the model can choose to call."}]},"description":"An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter."},"top_p":{"type":"number","nullable":true,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both."},"truncation":{"type":"string","enum":["auto","disabled"],"default":"disabled","description":"The truncation strategy to use for the model response.\n\n    - auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.\n    - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.\n"}},"required":["model","input"]}}},"paths":{"/v1/responses":{"post":{"operationId":"ResponseApiController_createResponse_v1","summary":"","parameters":[],"requestBody":{"required":true,"content":{"application/json":{"schema":{"$ref":"#/components/schemas/Response.v1.CreateResponsePayload"}}}},"responses":{"201":{"description":""}},"tags":["Response Api"]}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"o1",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'o1',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-BKKmwhCaUyWjRrBdZLw0CjOwJq9wo', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello there! How can I help you today?', 'refusal': None, 'annotations': []}}], 'created': 1744186170, 'model': 'o1-2024-12-17', 'usage': {'prompt_tokens': 221, 'completion_tokens': 2646, 'total_tokens': 2867, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'system_fingerprint': 'fp_688960522e'}
```

{% endcode %}

</details>

## Code Example #2: Using /responses Endpoint

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json   # for getting a structured output with indentation

response = requests.post(
    "https://api.aimlapi.com/v1/responses",
    headers={
        "Content-Type":"application/json", 

        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"o1",
        "input":"Hello"  # Insert your question for the model here, instead of Hello   
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/responses', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'o1',
        input: 'Hello',  // Insert your question here, instead of Hello 
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "resp_686ba45ce63481a2a4b1fad55d2bea8102a1cc22f1a1bcf1",
  "object": "response",
  "created_at": 1751884892,
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 512,
  "model": "o1",
  "output": [
    {
      "id": "rs_686ba463d18481a29dde85cfd7b055bf02a1cc22f1a1bcf1",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "msg_686ba463d4e081a2b2e2aff962ab00f702a1cc22f1a1bcf1",
      "type": "message",
      "status": "in_progress",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Hello! How can I help you today?"
        }
      ],
      "role": "assistant"
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": "medium",
    "summary": null
  },
  "temperature": 1,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 294,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 2520,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 2814
  },
  "metadata": {},
  "output_text": "Hello! How can I help you today?"
}
```

{% endcode %}

</details>


# o1-mini

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>o1-mini</code></li><li><code>o1-mini-2024-09-12</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=o1-mini&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A cost-efficient reasoning model optimized for STEM tasks (science, technology, engineering, and math), particularly excelling in mathematics and coding. It offers advanced reasoning capabilities at a fraction of the cost of its larger counterpart, o1-preview.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["o1-mini","o1-mini-2024-09-12"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."}},"required":["model","messages"],"title":"o1-mini"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"o1-mini",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'o1-mini',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-BKKnquKIY17xB6bAmtOLHdcyaGX8t', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'message': {'role': 'assistant', 'content': 'Hello! üòä How can I help you today?', 'refusal': None, 'annotations': []}}], 'created': 1744186226, 'model': 'o1-mini-2024-09-12', 'usage': {'prompt_tokens': 57, 'completion_tokens': 2167, 'total_tokens': 2224, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 64, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'system_fingerprint': 'fp_e8044cf94c'}

```

{% endcode %}

</details>


# o3

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>openai/o3-2025-04-16</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=openai/o3-2025-04-16&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

OpenAI's most capable reasoning model to date, showing strong performance across programming, mathematics, science, visual understanding, and more. The model is well-suited for complex tasks that involve layered reasoning and non-obvious answers. In evaluations on challenging, real-world problems, o3 makes 20% fewer critical errors than [o1](https://docs.aimlapi.com/api-references/text-models-llm/openai/o1).

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["openai/o3-2025-04-16"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data. "},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. Currently supports JPG/JPEG, PNG, GIF, and WEBP formats."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. Only PDF format is supported.\n        - Maximum size per file: Up to 512 MB and up to 2 million tokens.\n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"openai/o3-2025-04-16"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

### Responses Endpoint

This endpoint is currently used *only* with OpenAI models. Some models support both the `/chat/completions` and `/responses` endpoints, while others support only one of them. OpenAI has announced plans to expand the capabilities of the `/responses` endpoint in the future.

## POST /v1/responses

>

```json
{"openapi":"3.0.0","info":{"title":"AI/ML Gateway","version":"1.0"},"servers":[{"url":"https://api.aimlapi.com"}],"security":[{"access-token":[]}],"components":{"securitySchemes":{"access-token":{"scheme":"bearer","bearerFormat":"<YOUR_AIMLAPI_KEY>","type":"http","description":"Bearer key"}},"schemas":{"Response.v1.CreateResponsePayload":{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o3-mini","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16","openai/o3-2025-04-16","o1","openai/o3-pro","openai/gpt-5-2025-08-07","openai/gpt-5-mini-2025-08-07","openai/gpt-5-nano-2025-08-07","openai/gpt-5-chat-latest","openai/gpt-5-pro","openai/gpt-5-1","openai/gpt-5-1-chat-latest","openai/gpt-5-1-codex","openai/gpt-5-1-codex-mini"],"description":"Model ID used to generate the response."},"background":{"type":"boolean","default":false,"description":"Whether to run the model response in the background."},"input":{"anyOf":[{"type":"string","description":"A text input to the model, equivalent to a text input with the user role."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","assistant","system","developer"],"description":"The role of the message input."},"content":{"anyOf":[{"type":"string","description":"A text input to the model."},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or audio input to the model, used to generate a response. Can also contain previous assistant responses."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role. Messages with the assistant role are presumed to have been generated by the model in previous interactions."},{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","system","developer"],"description":"The role of the message input."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of item."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the output message."},"role":{"type":"string","enum":["assistant"],"description":"The role of the output message. Always assistant."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the message input."},"type":{"type":"string","enum":["message"],"description":"The type of the output message. Always message."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"annotations":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"type":{"type":"string","enum":["url_citation"],"description":"The type of the URL citation. Always url_citation."},"url":{"type":"string","format":"uri","description":"The URL of the web resource."}},"required":["end_index","start_index","title","type","url"],"description":"A citation for a web resource used to generate a model response."}]},"description":"The annotations of the text output."},"text":{"type":"string","description":"The text output from the model."},"type":{"type":"string","enum":["output_text"],"description":"The type of the output text. Always output_text."},"logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"},"top_logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"}},"required":["bytes","logprob","token"]}}},"required":["bytes","logprob","token","top_logprobs"]}}},"required":["annotations","text","type"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal explanationfrom the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the refusal. Always refusal."}},"required":["refusal","type"]}],"description":"The content of the output message."}}},"required":["id","role","status","type","content"],"description":"An output message from the model."},{"type":"object","properties":{"action":{"oneOf":[{"type":"object","properties":{"button":{"type":"string","enum":["left","right","wheel","back","forward"],"description":"Indicates which mouse button was pressed during the click."},"type":{"type":"string","enum":["click"],"description":"Specifies the event type. For a click action, this property is always set to click."},"x":{"type":"integer","description":"The x-coordinate where the click occurred."},"y":{"type":"integer","description":"The y-coordinate where the click occurred."}},"required":["button","type","x","y"],"description":"A click action."},{"type":"object","properties":{"type":{"type":"string","enum":["double_click"],"description":"Specifies the event type. For a double click action, this property is always set to double_click."},"x":{"type":"integer","description":"The x-coordinate where the double click occurred."},"y":{"type":"integer","description":"The y-coordinate where the double click occurred."}},"required":["type","x","y"],"description":"A double click action."},{"type":"object","properties":{"path":{"type":"array","items":{"type":"object","properties":{"x":{"type":"integer","description":"The y-coordinate."},"y":{"type":"integer","description":"The y-coordinate."}},"required":["x","y"]},"description":"An array of coordinates representing the path of the drag action. Coordinates will appear as an array of objects, eg"},"type":{"type":"string","enum":["drag"],"description":"Specifies the event type. For a drag action, this property is always set to drag."}},"required":["path","type"],"description":"A drag action."},{"type":"object","properties":{"keys":{"type":"array","items":{"type":"string"},"description":"The combination of keys the model is requesting to be pressed. This is an array of strings, each representing a key."},"type":{"type":"string","enum":["keypress"],"description":"Specifies the event type. For a keypress action, this property is always set to keypress."}},"required":["keys","type"],"description":"A collection of keypresses the model would like to perform."},{"type":"object","properties":{"type":{"type":"string","enum":["move"],"description":"Specifies the event type. For a move action, this property is always set to move."},"x":{"type":"integer","description":"The x-coordinate to move to."},"y":{"type":"integer","description":"The y-coordinate to move to."}},"required":["type","x","y"],"description":"A mouse move action."},{"type":"object","properties":{"type":{"type":"string","enum":["screenshot"],"description":"Specifies the event type. For a screenshot action, this property is always set to screenshot."}},"required":["type"],"description":"A screenshot action."},{"type":"object","properties":{"type":{"type":"string","enum":["scroll"],"description":"Specifies the event type. For a scroll action, this property is always set to scroll."},"scroll_x":{"type":"integer","description":"The horizontal scroll distance."},"scroll_y":{"type":"integer","description":"The vertical scroll distance."},"x":{"type":"integer","description":"The x-coordinate where the scroll occurred."},"y":{"type":"integer","description":"The y-coordinate where the scroll occurred."}},"required":["type","scroll_x","scroll_y","x","y"],"description":"A scroll action."},{"type":"object","properties":{"type":{"type":"string","enum":["type"],"description":"Specifies the event type. For a type action, this property is always set to type."},"text":{"type":"string","description":"The text to type."}},"required":["type","text"],"description":"An action to type in text."},{"type":"object","properties":{"type":{"type":"string","enum":["wait"],"description":"Specifies the event type. For a wait action, this property is always set to wait."}},"required":["type"],"description":"A wait action."}]},"call_id":{"type":"string","description":"An identifier used when responding to the tool call with output."},"id":{"type":"string","description":"The unique ID of the computer call."},"pending_safety_checks":{"type":"array","items":{"type":"object","properties":{"code":{"type":"string","description":"The type of the pending safety check."},"id":{"type":"string","description":"The ID of the pending safety check."},"message":{"type":"string","description":"Details about the pending safety check."}},"required":["code","id","message"]},"description":"The pending safety checks for the computer call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."},"type":{"type":"string","enum":["computer_call"],"description":"The type of the computer call. Always computer_call."}},"required":["action","call_id","id","pending_safety_checks","status","type"],"description":"A tool call to a computer use tool."},{"type":"object","properties":{"call_id":{"type":"string","description":"The ID of the computer tool call that produced the output."},"output":{"type":"object","properties":{"type":{"type":"string","enum":["computer_screenshot"],"description":"Specifies the event type. For a computer screenshot, this property is always set to computer_screenshot."},"image_url":{"type":"string","format":"uri","description":"The URL of the screenshot image."}},"required":["type"],"description":"A computer screenshot image used with the computer use tool."},"type":{"type":"string","enum":["computer_call_output"],"description":"The type of the computer tool call output. Always computer_call_output."},"acknowledged_safety_checks":{"type":"array","nullable":true,"items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the pending safety check."},"code":{"type":"string","nullable":true,"description":"The type of the pending safety check."},"message":{"type":"string","nullable":true,"description":"Details about the pending safety check."}},"required":["id"]},"description":"The safety checks reported by the API that have been acknowledged by the developer."},"id":{"type":"string","nullable":true,"description":"The ID of the computer tool call output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the message input."}},"required":["call_id","output","type"],"description":"The output of a computer tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the web search tool call."},"status":{"type":"string","enum":["in_progress","completed","searching","failed"],"description":"The status of the web search tool call."},"type":{"type":"string","enum":["web_search_call"],"description":"The type of the web search tool call. Always web_search_call."}},"required":["id","status","type"],"description":"The results of a web search tool call."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments to pass to the function."},"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"name":{"type":"string","description":"The name of the function to run."},"type":{"type":"string","enum":["function_call"],"description":"The type of the function tool call. Always function_call."},"id":{"type":"string","description":"The unique ID of the function tool call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["arguments","call_id","name","type"],"description":"A tool call to run a function."},{"type":"object","properties":{"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"output":{"anyOf":[{"type":"string","description":"A JSON string of the output of the function tool call."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}}]},"type":{"type":"string","enum":["function_call_output"],"description":"The type of the function tool call output. Always function_call_output."},"id":{"type":"string","nullable":true,"description":"The unique ID of the function tool call output. Populated when this item is returned via API."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["call_id","output","type"],"description":"The output of a function tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique identifier of the reasoning content."},"summary":{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"A short summary of the reasoning used by the model when generating the response."},"type":{"type":"string","enum":["summary_text"],"description":"The type of the object. Always summary_text."}},"required":["text","type"]},"description":"Reasoning text contents."},"type":{"type":"string","enum":["reasoning"],"description":"The type of the object. Always reasoning."},"encrypted_content":{"type":"string","nullable":true,"description":"The encrypted content of the reasoning item - populated when a response is generated with reasoning.encrypted_content in the include parameter."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","summary","type"],"description":"A description of the chain of thought used by a reasoning model while generating a response."},{"type":"object","properties":{"code":{"type":"string","description":"The code to run, or null if not available."},"id":{"type":"string","description":"The unique ID of the code interpreter tool call."},"outputs":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"logs":{"type":"string","description":"The logs output from the code interpreter."},"type":{"type":"string","enum":["logs"],"description":"The type of the output. Always 'logs'."}},"required":["logs","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"url":{"type":"string"}},"required":["type","url"]}]},"description":"The outputs generated by the code interpreter, such as logs or images. Can be null if no outputs are available."},"status":{"type":"string","enum":["in_progress","completed","interpreting"],"description":"The status of the code interpreter tool call."},"type":{"type":"string","enum":["code_interpreter_call"],"description":"The type of the code interpreter tool call. Always code_interpreter_call."},"container_id":{"type":"string","description":"The ID of the container used to run the code."}},"required":["code","id","outputs","status","type","container_id"],"description":"A tool call to run code."},{"type":"object","properties":{"action":{"type":"object","properties":{"command":{"type":"array","items":{"type":"string"},"description":"The command to run."},"env":{"type":"object","additionalProperties":{"type":"string"},"description":"Environment variables to set for the command."},"type":{"type":"string","enum":["exec"],"description":"The type of the local shell action. Always exec."},"timeout_ms":{"type":"integer","nullable":true,"description":"Optional timeout in milliseconds for the command."},"user":{"type":"string","nullable":true,"description":"Optional user to run the command as."},"working_directory":{"type":"string","nullable":true,"description":"Optional working directory to run the command in."}},"required":["command","env","type"],"description":"Execute a shell command on the server."},"call_id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"id":{"type":"string","description":"The unique ID of the local shell call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the local shell call."},"type":{"type":"string","enum":["local_shell_call"],"description":"The type of the local shell call. Always local_shell_call."}},"required":["action","call_id","id","status","type"],"description":"A tool call to run a command on the local shell."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"output":{"type":"string","description":"A JSON string of the output of the local shell tool call."},"type":{"type":"string","enum":["local_shell_call_output"],"description":"The type of the local shell tool call output. Always local_shell_call_output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","output","type"],"description":"The output of a local shell tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the list."},"server_label":{"type":"string","description":"The label of the MCP server."},"tools":{"type":"array","items":{"type":"object","properties":{"input_schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The JSON schema describing the tool's input."},"name":{"type":"string","description":"The name of the tool."},"annotations":{"type":"object","nullable":true,"additionalProperties":{"nullable":true},"description":"Additional annotations about the tool."},"description":{"type":"string","nullable":true,"description":"The description of the tool."}},"required":["input_schema","name"]},"description":"The tools available on the server."},"type":{"type":"string","enum":["mcp_list_tools"],"description":"The type of the item. Always mcp_list_tools."},"error":{"type":"string","nullable":true,"description":"Error message if the server could not list tools."}},"required":["id","server_label","tools","type"],"description":"A list of tools available on an MCP server."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of arguments for the tool."},"id":{"type":"string","description":"The unique ID of the approval request."},"name":{"type":"string"},"server_label":{"type":"string","description":"The name of the tool to run."},"type":{"type":"string","enum":["mcp_approval_request"],"description":"The type of the item. Always mcp_approval_request."}},"required":["arguments","id","name","server_label","type"],"description":"A request for human approval of a tool invocation."},{"type":"object","properties":{"approval_request_id":{"type":"string","description":"The ID of the approval request being answered."},"approve":{"type":"boolean","description":"Whether the request was approved."},"type":{"type":"string","enum":["mcp_approval_response"],"description":"The type of the item. Always mcp_approval_response."},"id":{"type":"string","nullable":true,"description":"The unique ID of the approval response."},"reason":{"type":"string","nullable":true,"description":"Optional reason for the decision."}},"required":["approval_request_id","approve","type"],"description":"A response to an MCP approval request."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments passed to the tool."},"id":{"type":"string","description":"The unique ID of the tool call."},"name":{"type":"string","description":"The name of the tool that was run."},"server_label":{"type":"string","description":"The label of the MCP server running the tool."},"type":{"type":"string","enum":["mcp_call"],"description":"The type of the item. Always mcp_call."},"error":{"type":"string","nullable":true,"description":"The error from the tool call, if any."},"output":{"type":"string","nullable":true,"description":"The output from the tool call."}},"required":["arguments","id","name","server_label","type"],"description":"An invocation of a tool on an MCP server."},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the item to reference."},"type":{"type":"string","nullable":true,"enum":["item_reference"],"description":"The type of item to reference. Always item_reference."}},"required":["id"],"description":"An internal identifier for an item to reference."}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or file inputs to the model, used to generate a response."},"include":{"type":"array","nullable":true,"items":{"type":"string","enum":["message.input_image.image_url","computer_call_output.output.image_url","reasoning.encrypted_content","code_interpreter_call.outputs"]},"description":"Specify additional output data to include in the model response. Currently supported values are:\n    - code_interpreter_call.outputs: Includes the outputs of python code execution in code interpreter tool call items.\n    - computer_call_output.output.image_url: Include image urls from the computer call output.\n    - file_search_call.results: Include the search results of the file search tool call.\n    - message.output_text.logprobs: Include logprobs with assistant messages.\n    - reasoning.encrypted_content: Includes an encrypted version of reasoning tokens in reasoning item outputs. This enables reasoning items to be used in multi-turn conversations when using the Responses API statelessly (like when the store parameter is set to false, or when an organization is enrolled in the zero data retention program).\n"},"instructions":{"type":"string","nullable":true,"description":"A system (or developer) message inserted into the model's context.\n\nWhen using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses."},"max_output_tokens":{"type":"integer","default":512,"description":"An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n\nKeys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"parallel_tool_calls":{"type":"boolean","nullable":true,"description":"Whether to allow the model to run tool calls in parallel."},"previous_response_id":{"type":"string","nullable":true,"description":"The unique ID of the previous response to the model. Use this to create multi-turn conversations."},"prompt":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"The unique identifier of the prompt template to use."},"variables":{"type":"object","nullable":true,"additionalProperties":{"anyOf":[{"type":"string"},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}]},"description":"Optional map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input types like images or files."},"version":{"type":"string","nullable":true,"description":"Optional version of the prompt template."}},"required":["id"],"description":"Reference to a prompt template and its variables."},"reasoning":{"type":"object","nullable":true,"properties":{"effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"summary":{"type":"string","nullable":true,"enum":["auto","concise","detailed"],"description":"A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process."}},"description":"o-series models only\nConfiguration options for reasoning models."},"store":{"type":"boolean","nullable":true,"default":false,"description":"Whether to store the generated model response for later retrieval via API."},"stream":{"type":"boolean","nullable":true,"default":false,"description":"If set to true, the model response data will be streamed to the client as it is generated using server-sent events. "},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"text":{"type":"object","properties":{"format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"type":{"type":"string","enum":["json_schema"]},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name","schema","type"],"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["format"],"description":"Configuration options for a text response from the model. Can be plain text or structured JSON data."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"Controls which (if any) tool is called by the model.\n\nnone means the model will not call any tool and instead generates a message.\n\nauto means the model can pick between generating a message or calling one or more tools.\n\nrequired means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11","computer_use_preview","code_interpreter","mcp"]}},"required":["type"],"description":"Indicates that the model should use a built-in tool to generate a response."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"type":{"type":"string","enum":["function"],"description":"For function calling, the type is always function."}},"required":["name","type"]}],"description":"How the model should select which tool (or tools) to use when generating a response."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11"],"description":"The type of the web search tool. One of web_search_preview or web_search_preview_2025_03_11."},"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."},"city":{"type":"string","nullable":true,"description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","nullable":true,"pattern":"^[A-Z]{2}$","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","nullable":true,"description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","nullable":true,"description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"required":["type"],"description":"The user's location"}},"required":["type"],"description":"This tool searches the web for relevant results to use in a response."},{"type":"object","properties":{"display_height":{"type":"integer","description":"The height of the computer display."},"display_width":{"type":"integer","description":"The width of the computer display."},"environment":{"type":"string","enum":["windows","mac","linux","ubuntu","browser"],"description":"The type of computer environment to control."},"type":{"type":"string","enum":["computer_use_preview"],"description":"The type of the computer use tool. Always computer_use_preview."}},"required":["display_height","display_width","environment","type"],"description":"A tool that controls a virtual computer."},{"type":"object","properties":{"server_label":{"type":"string","description":"A label for this MCP server, used to identify it in tool calls."},"server_url":{"type":"string","description":"The URL for the MCP server."},"type":{"type":"string","enum":["mcp"],"description":"The type of the MCP tool. Always mcp."},"allowed_tools":{"anyOf":[{"type":"array","items":{"type":"string"},"description":"A string array of allowed tool names."},{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of allowed tool names."}},"description":"A filter object to specify which tools are allowed."},{"nullable":true}],"description":"List of allowed tool names or a filter object."},"headers":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Optional HTTP headers to send to the MCP server. Use for authentication or other purposes."},"require_approval":{"anyOf":[{"type":"string","enum":["always","never"]},{"type":"object","properties":{"always":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that require approval."}},"description":"A list of tools that always require approval."},"never":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that do not require approval."}},"description":"A list of tools that never require approval."}}},{"nullable":true}],"description":"Specify which of the MCP server's tools require approval."}},"required":["server_label","server_url","type"],"description":"Give the model access to additional tools via remote Model Context Protocol (MCP) servers."},{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of the code interpreter tool. Always code_interpreter."},"container":{"anyOf":[{"type":"string"},{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]}],"description":"The container ID."}},"required":["type","container"],"description":"A tool that runs Python code to help generate a response to a prompt."},{"type":"object","properties":{"type":{"type":"string","enum":["local_shell"],"description":"The type of the local shell tool. Always local_shell."}},"required":["type"],"description":"A tool that allows the model to execute shell commands in a local environment."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"A JSON schema object describing the parameters of the function."},"strict":{"type":"boolean","description":"Whether to enforce strict parameter validation."},"type":{"type":"string","enum":["function"],"description":"The type of the function tool. Always function."},"description":{"type":"string","description":"A description of the function. Used by the model to determine whether or not to call the function."}},"required":["name","parameters","strict","type"],"description":"Defines a function in your own code the model can choose to call."}]},"description":"An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter."},"top_p":{"type":"number","nullable":true,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both."},"truncation":{"type":"string","enum":["auto","disabled"],"default":"disabled","description":"The truncation strategy to use for the model response.\n\n    - auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.\n    - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.\n"}},"required":["model","input"]}}},"paths":{"/v1/responses":{"post":{"operationId":"ResponseApiController_createResponse_v1","summary":"","parameters":[],"requestBody":{"required":true,"content":{"application/json":{"schema":{"$ref":"#/components/schemas/Response.v1.CreateResponsePayload"}}}},"responses":{"201":{"description":""}},"tags":["Response Api"]}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"openai/o3-2025-04-16",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'openai/o3-2025-04-16',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "chatcmpl-BhaL4MrWXyha1PD3AHkJ2mmHXgEcu",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?",
        "refusal": null,
        "annotations": []
      }
    }
  ],
  "created": 1749727490,
  "model": "o3-2025-04-16",
  "usage": {
    "prompt_tokens": 34,
    "completion_tokens": 454,
    "total_tokens": 488,
    "prompt_tokens_details": {
      "cached_tokens": 0,
      "audio_tokens": 0
    },
    "completion_tokens_details": {
      "reasoning_tokens": 0,
      "audio_tokens": 0,
      "accepted_prediction_tokens": 0,
      "rejected_prediction_tokens": 0
    }
  },
  "system_fingerprint": null
}
```

{% endcode %}

</details>

## Code Example #2: Using /responses Endpoint

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json   # for getting a structured output with indentation

response = requests.post(
    "https://api.aimlapi.com/v1/responses",
    headers={
        "Content-Type":"application/json", 

        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"openai/o3-2025-04-16",
        "input":"Hello"  # Insert your question for the model here, instead of Hello   
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/responses', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'openai/o3-2025-04-16',
        input: 'Hello',  // Insert your question here, instead of Hello 
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "resp_686ba45ce63481a2a4b1fad55d2bea8102a1cc22f1a1bcf1",
  "object": "response",
  "created_at": 1751884892,
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 512,
  "model": "openai/o3-2025-04-16",
  "output": [
    {
      "id": "rs_686ba463d18481a29dde85cfd7b055bf02a1cc22f1a1bcf1",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "msg_686ba463d4e081a2b2e2aff962ab00f702a1cc22f1a1bcf1",
      "type": "message",
      "status": "in_progress",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Hello! How can I help you today?"
        }
      ],
      "role": "assistant"
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": "medium",
    "summary": null
  },
  "temperature": 1,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 294,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 2520,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 2814
  },
  "metadata": {},
  "output_text": "Hello! How can I help you today?"
}
```

{% endcode %}

</details>


# o3-mini

<table data-header-hidden data-full-width="true"><thead><tr><th width="546.4443969726562" valign="top"></th><th width="202.666748046875" valign="top"></th></tr></thead><tbody><tr><td valign="top"><div data-gb-custom-block data-tag="hint" data-style="info" class="hint hint-info"><p>This documentation is valid for the following list of our models:</p><ul><li><code>o3-mini</code></li></ul></div></td><td valign="top"><a href="https://aimlapi.com/app/?model=o3-mini&#x26;mode=chat" class="button primary">Try in Playground</a></td></tr></tbody></table>

## Model Overview

A model designed to excel in complex reasoning tasks, including mathematical problem-solving, programming challenges, and scientific inquiries. It integrates advanced reasoning capabilities.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `content` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `messages` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

## POST /v1/chat/completions

>

```json
{"openapi":"3.0.0","info":{"title":"AIML API","version":"1.0.0"},"servers":[{"url":"https://api.aimlapi.com"}],"paths":{"/v1/chat/completions":{"post":{"operationId":"_v1_chat_completions","requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"type":"string","enum":["o3-mini"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message ‚Äî in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"]},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message ‚Äî in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message ‚Äî in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the author of the message ‚Äî in this case, the tool."},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message ‚Äî in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."}},"required":["role"]}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_completion_tokens":{"type":"integer","minimum":1,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"max_tokens":{"type":"number","minimum":1,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"type":"object","additionalProperties":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name","parameters"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"],"title":"o3-mini"}}}},"responses":{"200":{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"object":{"type":"string","const":"chat.completion","description":"The object type."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"choices":{"type":"array","items":{"type":"object","properties":{"index":{"type":"number","description":"The index of the choice in the list of choices."},"message":{"type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message."},"content":{"type":"string","description":"The contents of the message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"annotations":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","const":"url_citation","description":"The type of the URL citation. Always url_citation."},"url_citation":{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"url":{"type":"string","description":"The URL of the web resource."}},"required":["end_index","start_index","title","url"],"additionalProperties":false,"description":"A URL citation when using web search."}},"required":["type","url_citation"],"additionalProperties":false}},{"type":"null"}],"description":"Annotations for the message, when applicable, as when using the web search tool."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for this audio response."},"data":{"type":"string","description":"Base64 encoded audio bytes generated by the model, in the format specified in the request."},"transcript":{"type":"string","description":"Transcript of the audio generated by the model."},"expires_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations."}},"required":["id","data","transcript","expires_at"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion message generated by the model."},"tool_calls":{"anyOf":[{"type":"array","items":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"function","description":"The type of the tool."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"],"additionalProperties":false},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","const":"custom","description":"The type of the tool."},"custom":{"type":"object","properties":{"input":{"type":"string","description":"The input for the custom tool call generated by the model."},"name":{"type":"string","description":"The name of the custom tool to call."}},"required":["input","name"],"additionalProperties":false,"description":"The custom tool that the model called."}},"required":["id","type","custom"],"additionalProperties":false}]}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["role","content"],"additionalProperties":false,"description":"A chat completion message generated by the model."},"finish_reason":{"type":"string","enum":["stop","length","content_filter","tool_calls"],"description":"The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool"},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"bytes":{"anyOf":[{"type":"array","items":{"type":"integer"}},{"type":"null"}],"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"token":{"type":"string","description":"The token."}},"required":["logprob","token"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["bytes","logprob","token"],"additionalProperties":false},"description":"A list of message content tokens with log probability information."},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"},"description":"A list of message refusal tokens with log probability information."}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["index","message","finish_reason"],"additionalProperties":false}},"model":{"type":"string","description":"The model used for the chat completion."},"usage":{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false,"description":"Usage statistics for the completion request."}},"required":["id","object","created","choices","model","usage"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},"text/event-stream":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"A unique identifier for the chat completion."},"choices":{"type":"array","items":{"type":"object","properties":{"delta":{"anyOf":[{"type":"object","properties":{"content":{"type":"string","description":"The contents of the chunk message."},"refusal":{"type":["string","null"],"description":"The refusal message generated by the model."},"role":{"type":"string","enum":["user","assistant","developer","system","tool"],"description":"The role of the author of this message."},"tool_calls":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"index":{"type":"number"},"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string"}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool."}},"required":["index","id","function","type"],"additionalProperties":false}},{"type":"null"}],"description":"The tool calls generated by the model, such as function calls."}},"required":["content","role"],"additionalProperties":false},{"type":"null"}],"description":"A chat completion delta generated by streamed model responses."},"finish_reason":{"type":"string","enum":["length","function_call","stop","tool_calls","content_filter"]},"index":{"type":"number","description":"The index of the choice in the list of choices."},"logprobs":{"anyOf":[{"type":"object","properties":{"content":{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."},"top_logprobs":{"anyOf":[{"type":"array","items":{"type":"object","properties":{"token":{"type":"string","description":"The token."},"bytes":{"type":"array","items":{"type":"number"},"description":"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token."},"logprob":{"type":"number","description":"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value -9999.0 is used to signify that the token is very unlikely."}},"required":["token","bytes","logprob"],"additionalProperties":false}},{"type":"null"}],"description":"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned."}},"required":["token","bytes","logprob"],"additionalProperties":false}},"refusal":{"type":"array","items":{"$ref":"#/properties/choices/items/properties/logprobs/anyOf/0/properties/content/items"}}},"required":["content","refusal"],"additionalProperties":false},{"type":"null"}],"description":"Log probability information for the choice."}},"required":["finish_reason","index"],"additionalProperties":false},"description":"A list of chat completion choices. Can be more than one if n is greater than 1."},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created."},"model":{"type":"string","description":"The model used for the chat completion."},"object":{"type":"string","const":"chat.completion.chunk","description":"The object type."},"service_tier":{"anyOf":[{"type":"string","enum":["auto","default","flex","scale","priority"]},{"type":"null"}],"description":"Specifies the processing type used for serving the request."},"usage":{"anyOf":[{"anyOf":[{"type":"object","properties":{"prompt_tokens":{"type":"number","description":"Number of tokens in the prompt."},"completion_tokens":{"type":"number","description":"Number of tokens in the generated completion."},"total_tokens":{"type":"number","description":"Total number of tokens used in the request (prompt + completion)."},"completion_tokens_details":{"anyOf":[{"type":"object","properties":{"accepted_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion."},"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens generated by the model."},"reasoning_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Tokens generated by the model for reasoning."},"rejected_prediction_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in a completion."},"prompt_tokens_details":{"anyOf":[{"type":"object","properties":{"audio_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Audio input tokens present in the prompt."},"cached_tokens":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Cached tokens present in the prompt."}},"additionalProperties":false},{"type":"null"}],"description":"Breakdown of tokens used in the prompt."}},"required":["prompt_tokens","completion_tokens","total_tokens"],"additionalProperties":false},{"type":"null"}]},{"type":"null"}],"description":"Usage statistics for the completion request."}},"required":["id","choices","created","model","object"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}}}}}}}
```

### Responses Endpoint

This endpoint is currently used *only* with OpenAI models. Some models support both the `/chat/completions` and `/responses` endpoints, while others support only one of them. OpenAI has announced plans to expand the capabilities of the `/responses` endpoint in the future.

## POST /v1/responses

>

```json
{"openapi":"3.0.0","info":{"title":"AI/ML Gateway","version":"1.0"},"servers":[{"url":"https://api.aimlapi.com"}],"security":[{"access-token":[]}],"components":{"securitySchemes":{"access-token":{"scheme":"bearer","bearerFormat":"<YOUR_AIMLAPI_KEY>","type":"http","description":"Bearer key"}},"schemas":{"Response.v1.CreateResponsePayload":{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o3-mini","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16","openai/o3-2025-04-16","o1","openai/o3-pro","openai/gpt-5-2025-08-07","openai/gpt-5-mini-2025-08-07","openai/gpt-5-nano-2025-08-07","openai/gpt-5-chat-latest","openai/gpt-5-pro","openai/gpt-5-1","openai/gpt-5-1-chat-latest","openai/gpt-5-1-codex","openai/gpt-5-1-codex-mini"],"description":"Model ID used to generate the response."},"background":{"type":"boolean","default":false,"description":"Whether to run the model response in the background."},"input":{"anyOf":[{"type":"string","description":"A text input to the model, equivalent to a text input with the user role."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","assistant","system","developer"],"description":"The role of the message input."},"content":{"anyOf":[{"type":"string","description":"A text input to the model."},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or audio input to the model, used to generate a response. Can also contain previous assistant responses."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role. Messages with the assistant role are presumed to have been generated by the model in previous interactions."},{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","system","developer"],"description":"The role of the message input."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of item."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the output message."},"role":{"type":"string","enum":["assistant"],"description":"The role of the output message. Always assistant."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the message input."},"type":{"type":"string","enum":["message"],"description":"The type of the output message. Always message."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"annotations":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"type":{"type":"string","enum":["url_citation"],"description":"The type of the URL citation. Always url_citation."},"url":{"type":"string","format":"uri","description":"The URL of the web resource."}},"required":["end_index","start_index","title","type","url"],"description":"A citation for a web resource used to generate a model response."}]},"description":"The annotations of the text output."},"text":{"type":"string","description":"The text output from the model."},"type":{"type":"string","enum":["output_text"],"description":"The type of the output text. Always output_text."},"logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"},"top_logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"}},"required":["bytes","logprob","token"]}}},"required":["bytes","logprob","token","top_logprobs"]}}},"required":["annotations","text","type"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal explanationfrom the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the refusal. Always refusal."}},"required":["refusal","type"]}],"description":"The content of the output message."}}},"required":["id","role","status","type","content"],"description":"An output message from the model."},{"type":"object","properties":{"action":{"oneOf":[{"type":"object","properties":{"button":{"type":"string","enum":["left","right","wheel","back","forward"],"description":"Indicates which mouse button was pressed during the click."},"type":{"type":"string","enum":["click"],"description":"Specifies the event type. For a click action, this property is always set to click."},"x":{"type":"integer","description":"The x-coordinate where the click occurred."},"y":{"type":"integer","description":"The y-coordinate where the click occurred."}},"required":["button","type","x","y"],"description":"A click action."},{"type":"object","properties":{"type":{"type":"string","enum":["double_click"],"description":"Specifies the event type. For a double click action, this property is always set to double_click."},"x":{"type":"integer","description":"The x-coordinate where the double click occurred."},"y":{"type":"integer","description":"The y-coordinate where the double click occurred."}},"required":["type","x","y"],"description":"A double click action."},{"type":"object","properties":{"path":{"type":"array","items":{"type":"object","properties":{"x":{"type":"integer","description":"The y-coordinate."},"y":{"type":"integer","description":"The y-coordinate."}},"required":["x","y"]},"description":"An array of coordinates representing the path of the drag action. Coordinates will appear as an array of objects, eg"},"type":{"type":"string","enum":["drag"],"description":"Specifies the event type. For a drag action, this property is always set to drag."}},"required":["path","type"],"description":"A drag action."},{"type":"object","properties":{"keys":{"type":"array","items":{"type":"string"},"description":"The combination of keys the model is requesting to be pressed. This is an array of strings, each representing a key."},"type":{"type":"string","enum":["keypress"],"description":"Specifies the event type. For a keypress action, this property is always set to keypress."}},"required":["keys","type"],"description":"A collection of keypresses the model would like to perform."},{"type":"object","properties":{"type":{"type":"string","enum":["move"],"description":"Specifies the event type. For a move action, this property is always set to move."},"x":{"type":"integer","description":"The x-coordinate to move to."},"y":{"type":"integer","description":"The y-coordinate to move to."}},"required":["type","x","y"],"description":"A mouse move action."},{"type":"object","properties":{"type":{"type":"string","enum":["screenshot"],"description":"Specifies the event type. For a screenshot action, this property is always set to screenshot."}},"required":["type"],"description":"A screenshot action."},{"type":"object","properties":{"type":{"type":"string","enum":["scroll"],"description":"Specifies the event type. For a scroll action, this property is always set to scroll."},"scroll_x":{"type":"integer","description":"The horizontal scroll distance."},"scroll_y":{"type":"integer","description":"The vertical scroll distance."},"x":{"type":"integer","description":"The x-coordinate where the scroll occurred."},"y":{"type":"integer","description":"The y-coordinate where the scroll occurred."}},"required":["type","scroll_x","scroll_y","x","y"],"description":"A scroll action."},{"type":"object","properties":{"type":{"type":"string","enum":["type"],"description":"Specifies the event type. For a type action, this property is always set to type."},"text":{"type":"string","description":"The text to type."}},"required":["type","text"],"description":"An action to type in text."},{"type":"object","properties":{"type":{"type":"string","enum":["wait"],"description":"Specifies the event type. For a wait action, this property is always set to wait."}},"required":["type"],"description":"A wait action."}]},"call_id":{"type":"string","description":"An identifier used when responding to the tool call with output."},"id":{"type":"string","description":"The unique ID of the computer call."},"pending_safety_checks":{"type":"array","items":{"type":"object","properties":{"code":{"type":"string","description":"The type of the pending safety check."},"id":{"type":"string","description":"The ID of the pending safety check."},"message":{"type":"string","description":"Details about the pending safety check."}},"required":["code","id","message"]},"description":"The pending safety checks for the computer call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."},"type":{"type":"string","enum":["computer_call"],"description":"The type of the computer call. Always computer_call."}},"required":["action","call_id","id","pending_safety_checks","status","type"],"description":"A tool call to a computer use tool."},{"type":"object","properties":{"call_id":{"type":"string","description":"The ID of the computer tool call that produced the output."},"output":{"type":"object","properties":{"type":{"type":"string","enum":["computer_screenshot"],"description":"Specifies the event type. For a computer screenshot, this property is always set to computer_screenshot."},"image_url":{"type":"string","format":"uri","description":"The URL of the screenshot image."}},"required":["type"],"description":"A computer screenshot image used with the computer use tool."},"type":{"type":"string","enum":["computer_call_output"],"description":"The type of the computer tool call output. Always computer_call_output."},"acknowledged_safety_checks":{"type":"array","nullable":true,"items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the pending safety check."},"code":{"type":"string","nullable":true,"description":"The type of the pending safety check."},"message":{"type":"string","nullable":true,"description":"Details about the pending safety check."}},"required":["id"]},"description":"The safety checks reported by the API that have been acknowledged by the developer."},"id":{"type":"string","nullable":true,"description":"The ID of the computer tool call output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the message input."}},"required":["call_id","output","type"],"description":"The output of a computer tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the web search tool call."},"status":{"type":"string","enum":["in_progress","completed","searching","failed"],"description":"The status of the web search tool call."},"type":{"type":"string","enum":["web_search_call"],"description":"The type of the web search tool call. Always web_search_call."}},"required":["id","status","type"],"description":"The results of a web search tool call."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments to pass to the function."},"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"name":{"type":"string","description":"The name of the function to run."},"type":{"type":"string","enum":["function_call"],"description":"The type of the function tool call. Always function_call."},"id":{"type":"string","description":"The unique ID of the function tool call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["arguments","call_id","name","type"],"description":"A tool call to run a function."},{"type":"object","properties":{"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"output":{"anyOf":[{"type":"string","description":"A JSON string of the output of the function tool call."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}}]},"type":{"type":"string","enum":["function_call_output"],"description":"The type of the function tool call output. Always function_call_output."},"id":{"type":"string","nullable":true,"description":"The unique ID of the function tool call output. Populated when this item is returned via API."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["call_id","output","type"],"description":"The output of a function tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique identifier of the reasoning content."},"summary":{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"A short summary of the reasoning used by the model when generating the response."},"type":{"type":"string","enum":["summary_text"],"description":"The type of the object. Always summary_text."}},"required":["text","type"]},"description":"Reasoning text contents."},"type":{"type":"string","enum":["reasoning"],"description":"The type of the object. Always reasoning."},"encrypted_content":{"type":"string","nullable":true,"description":"The encrypted content of the reasoning item - populated when a response is generated with reasoning.encrypted_content in the include parameter."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","summary","type"],"description":"A description of the chain of thought used by a reasoning model while generating a response."},{"type":"object","properties":{"code":{"type":"string","description":"The code to run, or null if not available."},"id":{"type":"string","description":"The unique ID of the code interpreter tool call."},"outputs":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"logs":{"type":"string","description":"The logs output from the code interpreter."},"type":{"type":"string","enum":["logs"],"description":"The type of the output. Always 'logs'."}},"required":["logs","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"url":{"type":"string"}},"required":["type","url"]}]},"description":"The outputs generated by the code interpreter, such as logs or images. Can be null if no outputs are available."},"status":{"type":"string","enum":["in_progress","completed","interpreting"],"description":"The status of the code interpreter tool call."},"type":{"type":"string","enum":["code_interpreter_call"],"description":"The type of the code interpreter tool call. Always code_interpreter_call."},"container_id":{"type":"string","description":"The ID of the container used to run the code."}},"required":["code","id","outputs","status","type","container_id"],"description":"A tool call to run code."},{"type":"object","properties":{"action":{"type":"object","properties":{"command":{"type":"array","items":{"type":"string"},"description":"The command to run."},"env":{"type":"object","additionalProperties":{"type":"string"},"description":"Environment variables to set for the command."},"type":{"type":"string","enum":["exec"],"description":"The type of the local shell action. Always exec."},"timeout_ms":{"type":"integer","nullable":true,"description":"Optional timeout in milliseconds for the command."},"user":{"type":"string","nullable":true,"description":"Optional user to run the command as."},"working_directory":{"type":"string","nullable":true,"description":"Optional working directory to run the command in."}},"required":["command","env","type"],"description":"Execute a shell command on the server."},"call_id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"id":{"type":"string","description":"The unique ID of the local shell call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the local shell call."},"type":{"type":"string","enum":["local_shell_call"],"description":"The type of the local shell call. Always local_shell_call."}},"required":["action","call_id","id","status","type"],"description":"A tool call to run a command on the local shell."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"output":{"type":"string","description":"A JSON string of the output of the local shell tool call."},"type":{"type":"string","enum":["local_shell_call_output"],"description":"The type of the local shell tool call output. Always local_shell_call_output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","output","type"],"description":"The output of a local shell tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the list."},"server_label":{"type":"string","description":"The label of the MCP server."},"tools":{"type":"array","items":{"type":"object","properties":{"input_schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The JSON schema describing the tool's input."},"name":{"type":"string","description":"The name of the tool."},"annotations":{"type":"object","nullable":true,"additionalProperties":{"nullable":true},"description":"Additional annotations about the tool."},"description":{"type":"string","nullable":true,"description":"The description of the tool."}},"required":["input_schema","name"]},"description":"The tools available on the server."},"type":{"type":"string","enum":["mcp_list_tools"],"description":"The type of the item. Always mcp_list_tools."},"error":{"type":"string","nullable":true,"description":"Error message if the server could not list tools."}},"required":["id","server_label","tools","type"],"description":"A list of tools available on an MCP server."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of arguments for the tool."},"id":{"type":"string","description":"The unique ID of the approval request."},"name":{"type":"string"},"server_label":{"type":"string","description":"The name of the tool to run."},"type":{"type":"string","enum":["mcp_approval_request"],"description":"The type of the item. Always mcp_approval_request."}},"required":["arguments","id","name","server_label","type"],"description":"A request for human approval of a tool invocation."},{"type":"object","properties":{"approval_request_id":{"type":"string","description":"The ID of the approval request being answered."},"approve":{"type":"boolean","description":"Whether the request was approved."},"type":{"type":"string","enum":["mcp_approval_response"],"description":"The type of the item. Always mcp_approval_response."},"id":{"type":"string","nullable":true,"description":"The unique ID of the approval response."},"reason":{"type":"string","nullable":true,"description":"Optional reason for the decision."}},"required":["approval_request_id","approve","type"],"description":"A response to an MCP approval request."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments passed to the tool."},"id":{"type":"string","description":"The unique ID of the tool call."},"name":{"type":"string","description":"The name of the tool that was run."},"server_label":{"type":"string","description":"The label of the MCP server running the tool."},"type":{"type":"string","enum":["mcp_call"],"description":"The type of the item. Always mcp_call."},"error":{"type":"string","nullable":true,"description":"The error from the tool call, if any."},"output":{"type":"string","nullable":true,"description":"The output from the tool call."}},"required":["arguments","id","name","server_label","type"],"description":"An invocation of a tool on an MCP server."},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the item to reference."},"type":{"type":"string","nullable":true,"enum":["item_reference"],"description":"The type of item to reference. Always item_reference."}},"required":["id"],"description":"An internal identifier for an item to reference."}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or file inputs to the model, used to generate a response."},"include":{"type":"array","nullable":true,"items":{"type":"string","enum":["message.input_image.image_url","computer_call_output.output.image_url","reasoning.encrypted_content","code_interpreter_call.outputs"]},"description":"Specify additional output data to include in the model response. Currently supported values are:\n    - code_interpreter_call.outputs: Includes the outputs of python code execution in code interpreter tool call items.\n    - computer_call_output.output.image_url: Include image urls from the computer call output.\n    - file_search_call.results: Include the search results of the file search tool call.\n    - message.output_text.logprobs: Include logprobs with assistant messages.\n    - reasoning.encrypted_content: Includes an encrypted version of reasoning tokens in reasoning item outputs. This enables reasoning items to be used in multi-turn conversations when using the Responses API statelessly (like when the store parameter is set to false, or when an organization is enrolled in the zero data retention program).\n"},"instructions":{"type":"string","nullable":true,"description":"A system (or developer) message inserted into the model's context.\n\nWhen using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses."},"max_output_tokens":{"type":"integer","default":512,"description":"An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n\nKeys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"parallel_tool_calls":{"type":"boolean","nullable":true,"description":"Whether to allow the model to run tool calls in parallel."},"previous_response_id":{"type":"string","nullable":true,"description":"The unique ID of the previous response to the model. Use this to create multi-turn conversations."},"prompt":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"The unique identifier of the prompt template to use."},"variables":{"type":"object","nullable":true,"additionalProperties":{"anyOf":[{"type":"string"},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}]},"description":"Optional map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input types like images or files."},"version":{"type":"string","nullable":true,"description":"Optional version of the prompt template."}},"required":["id"],"description":"Reference to a prompt template and its variables."},"reasoning":{"type":"object","nullable":true,"properties":{"effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"summary":{"type":"string","nullable":true,"enum":["auto","concise","detailed"],"description":"A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process."}},"description":"o-series models only\nConfiguration options for reasoning models."},"store":{"type":"boolean","nullable":true,"default":false,"description":"Whether to store the generated model response for later retrieval via API."},"stream":{"type":"boolean","nullable":true,"default":false,"description":"If set to true, the model response data will be streamed to the client as it is generated using server-sent events. "},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"text":{"type":"object","properties":{"format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"type":{"type":"string","enum":["json_schema"]},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name","schema","type"],"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["format"],"description":"Configuration options for a text response from the model. Can be plain text or structured JSON data."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"Controls which (if any) tool is called by the model.\n\nnone means the model will not call any tool and instead generates a message.\n\nauto means the model can pick between generating a message or calling one or more tools.\n\nrequired means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11","computer_use_preview","code_interpreter","mcp"]}},"required":["type"],"description":"Indicates that the model should use a built-in tool to generate a response."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"type":{"type":"string","enum":["function"],"description":"For function calling, the type is always function."}},"required":["name","type"]}],"description":"How the model should select which tool (or tools) to use when generating a response."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11"],"description":"The type of the web search tool. One of web_search_preview or web_search_preview_2025_03_11."},"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."},"city":{"type":"string","nullable":true,"description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","nullable":true,"pattern":"^[A-Z]{2}$","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","nullable":true,"description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","nullable":true,"description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"required":["type"],"description":"The user's location"}},"required":["type"],"description":"This tool searches the web for relevant results to use in a response."},{"type":"object","properties":{"display_height":{"type":"integer","description":"The height of the computer display."},"display_width":{"type":"integer","description":"The width of the computer display."},"environment":{"type":"string","enum":["windows","mac","linux","ubuntu","browser"],"description":"The type of computer environment to control."},"type":{"type":"string","enum":["computer_use_preview"],"description":"The type of the computer use tool. Always computer_use_preview."}},"required":["display_height","display_width","environment","type"],"description":"A tool that controls a virtual computer."},{"type":"object","properties":{"server_label":{"type":"string","description":"A label for this MCP server, used to identify it in tool calls."},"server_url":{"type":"string","description":"The URL for the MCP server."},"type":{"type":"string","enum":["mcp"],"description":"The type of the MCP tool. Always mcp."},"allowed_tools":{"anyOf":[{"type":"array","items":{"type":"string"},"description":"A string array of allowed tool names."},{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of allowed tool names."}},"description":"A filter object to specify which tools are allowed."},{"nullable":true}],"description":"List of allowed tool names or a filter object."},"headers":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Optional HTTP headers to send to the MCP server. Use for authentication or other purposes."},"require_approval":{"anyOf":[{"type":"string","enum":["always","never"]},{"type":"object","properties":{"always":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that require approval."}},"description":"A list of tools that always require approval."},"never":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that do not require approval."}},"description":"A list of tools that never require approval."}}},{"nullable":true}],"description":"Specify which of the MCP server's tools require approval."}},"required":["server_label","server_url","type"],"description":"Give the model access to additional tools via remote Model Context Protocol (MCP) servers."},{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of the code interpreter tool. Always code_interpreter."},"container":{"anyOf":[{"type":"string"},{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]}],"description":"The container ID."}},"required":["type","container"],"description":"A tool that runs Python code to help generate a response to a prompt."},{"type":"object","properties":{"type":{"type":"string","enum":["local_shell"],"description":"The type of the local shell tool. Always local_shell."}},"required":["type"],"description":"A tool that allows the model to execute shell commands in a local environment."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"A JSON schema object describing the parameters of the function."},"strict":{"type":"boolean","description":"Whether to enforce strict parameter validation."},"type":{"type":"string","enum":["function"],"description":"The type of the function tool. Always function."},"description":{"type":"string","description":"A description of the function. Used by the model to determine whether or not to call the function."}},"required":["name","parameters","strict","type"],"description":"Defines a function in your own code the model can choose to call."}]},"description":"An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter."},"top_p":{"type":"number","nullable":true,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both."},"truncation":{"type":"string","enum":["auto","disabled"],"default":"disabled","description":"The truncation strategy to use for the model response.\n\n    - auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.\n    - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.\n"}},"required":["model","input"]}}},"paths":{"/v1/responses":{"post":{"operationId":"ResponseApiController_createResponse_v1","summary":"","parameters":[],"requestBody":{"required":true,"content":{"application/json":{"schema":{"$ref":"#/components/schemas/Response.v1.CreateResponsePayload"}}}},"responses":{"201":{"description":""}},"tags":["Response Api"]}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json  # for getting a structured output with indentation 

response = requests.post(
    "https://api.aimlapi.com/v1/chat/completions",
    headers={
        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"o3-mini",
        "messages":[
            {
                "role":"user",
                "content":"Hello"  # insert your prompt here, instead of Hello
            }
        ]
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  const response = await fetch('https://api.aimlapi.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      // insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
      'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'o3-mini',
      messages:[
          {
              role:'user',
              content: 'Hello'  // insert your prompt here, instead of Hello
          }
      ],
    }),
  });

  const data = await response.json();
  console.log(JSON.stringify(data, null, 2));
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{'id': 'chatcmpl-BKKqDz4BBMnR8lWHTwwUiInJtdup0', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'message': {'role': 'assistant', 'content': 'Hello there! How can I help you today?', 'refusal': None, 'annotations': []}}], 'created': 1744186373, 'model': 'o3-mini-2025-01-31', 'usage': {'prompt_tokens': 16, 'completion_tokens': 2559, 'total_tokens': 2575, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 256, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'system_fingerprint': 'fp_617f206dd9'}
```

{% endcode %}

</details>

## Code Example #2: Using /responses Endpoint

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json   # for getting a structured output with indentation

response = requests.post(
    "https://api.aimlapi.com/v1/responses",
    headers={
        "Content-Type":"application/json", 

        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"o3-mini",
        "input":"Hello"  # Insert your question for the model here, instead of Hello   
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/responses', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'o3-mini',
        input: 'Hello',  // Insert your question here, instead of Hello 
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "resp_686ba45ce63481a2a4b1fad55d2bea8102a1cc22f1a1bcf1",
  "object": "response",
  "created_at": 1751884892,
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 512,
  "model": "o3-mini",
  "output": [
    {
      "id": "rs_686ba463d18481a29dde85cfd7b055bf02a1cc22f1a1bcf1",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "msg_686ba463d4e081a2b2e2aff962ab00f702a1cc22f1a1bcf1",
      "type": "message",
      "status": "in_progress",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Hello! How can I help you today?"
        }
      ],
      "role": "assistant"
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": "medium",
    "summary": null
  },
  "temperature": 1,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 294,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 2520,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 2814
  },
  "metadata": {},
  "output_text": "Hello! How can I help you today?"
}
```

{% endcode %}

</details>


# o3-pro

{% hint style="info" %}
This documentation is valid for the following list of our models:

* `openai/o3-pro`
  {% endhint %}

## Model Overview

Designed for deeper reasoning and tougher questions, o3-pro uses more compute to deliver higher-quality answers. It‚Äôs only available in the `/responses` API, which supports multi-turn model interactions and will enable more advanced features in the future. Some complex requests may take a few minutes.

## How to Make a Call

<details>

<summary>Step-by-Step Instructions</summary>

:digit\_one: **Setup You Can‚Äôt Skip**

:black\_small\_square: [**Create an Account**](https://aimlapi.com/app/sign-up): Visit the AI/ML API website and create an account (if you don‚Äôt have one yet).\
:black\_small\_square: [**Generate an API Key**](https://aimlapi.com/app/keys): After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

:digit\_two: **Copy the code example**

At the bottom of this page, you'll find [a code example](#code-example) that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.

:digit\_three: **Modify the code example**

:black\_small\_square: Replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.\
:black\_small\_square: Insert your question or request into the `input` field‚Äîthis is what the model will respond to.

:digit\_four: <sup><sub><mark style="background-color:yellow;">**(Optional)**<mark style="background-color:yellow;"><sub></sup>**&#x20;Adjust other optional parameters if needed**

Only `model` and `input` are required parameters for this model (and we‚Äôve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model‚Äôs behavior. Below, you can find the corresponding [API schema](#api-schema), which lists all available parameters along with notes on how to use them.

:digit\_five: **Run your modified code**

Run your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.

{% hint style="success" %}
If you need a more detailed walkthrough for setting up your development environment and making a request step by step ‚Äî feel free to use our [Quickstart guide](https://docs.aimlapi.com/quickstart/setting-up).
{% endhint %}

</details>

## API Schema

{% hint style="warning" %}
Note: This model can ONLY be called via the `/responses` endpoint!
{% endhint %}

This endpoint is currently used *only* with OpenAI models. Some models support both the `/chat/completions` and `/responses` endpoints, while others (like `openai/o3-pro`) support only one of them. OpenAI has announced plans to expand the capabilities of the `/responses` endpoint in the future.

## POST /v1/responses

>

```json
{"openapi":"3.0.0","info":{"title":"AI/ML Gateway","version":"1.0"},"servers":[{"url":"https://api.aimlapi.com"}],"security":[{"access-token":[]}],"components":{"securitySchemes":{"access-token":{"scheme":"bearer","bearerFormat":"<YOUR_AIMLAPI_KEY>","type":"http","description":"Bearer key"}},"schemas":{"Response.v1.CreateResponsePayload":{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o3-mini","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16","openai/o3-2025-04-16","o1","openai/o3-pro","openai/gpt-5-2025-08-07","openai/gpt-5-mini-2025-08-07","openai/gpt-5-nano-2025-08-07","openai/gpt-5-chat-latest","openai/gpt-5-pro","openai/gpt-5-1","openai/gpt-5-1-chat-latest","openai/gpt-5-1-codex","openai/gpt-5-1-codex-mini"],"description":"Model ID used to generate the response."},"background":{"type":"boolean","default":false,"description":"Whether to run the model response in the background."},"input":{"anyOf":[{"type":"string","description":"A text input to the model, equivalent to a text input with the user role."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","assistant","system","developer"],"description":"The role of the message input."},"content":{"anyOf":[{"type":"string","description":"A text input to the model."},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or audio input to the model, used to generate a response. Can also contain previous assistant responses."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role. Messages with the assistant role are presumed to have been generated by the model in previous interactions."},{"type":"object","properties":{"type":{"type":"string","enum":["message"],"description":"The type of the message input. Always message."},"role":{"type":"string","enum":["user","system","developer"],"description":"The role of the message input."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of item."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]},"description":"A list of one or many input items to the model, containing different content types."}},"required":["role","content"],"description":"A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the output message."},"role":{"type":"string","enum":["assistant"],"description":"The role of the output message. Always assistant."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the message input."},"type":{"type":"string","enum":["message"],"description":"The type of the output message. Always message."},"content":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"annotations":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"end_index":{"type":"integer","description":"The index of the last character of the URL citation in the message."},"start_index":{"type":"integer","description":"The index of the first character of the URL citation in the message."},"title":{"type":"string","description":"The title of the web resource."},"type":{"type":"string","enum":["url_citation"],"description":"The type of the URL citation. Always url_citation."},"url":{"type":"string","format":"uri","description":"The URL of the web resource."}},"required":["end_index","start_index","title","type","url"],"description":"A citation for a web resource used to generate a model response."}]},"description":"The annotations of the text output."},"text":{"type":"string","description":"The text output from the model."},"type":{"type":"string","enum":["output_text"],"description":"The type of the output text. Always output_text."},"logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"},"top_logprobs":{"type":"array","items":{"type":"object","properties":{"bytes":{"type":"array","items":{"type":"integer"}},"logprob":{"type":"number"},"token":{"type":"string"}},"required":["bytes","logprob","token"]}}},"required":["bytes","logprob","token","top_logprobs"]}}},"required":["annotations","text","type"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal explanationfrom the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the refusal. Always refusal."}},"required":["refusal","type"]}],"description":"The content of the output message."}}},"required":["id","role","status","type","content"],"description":"An output message from the model."},{"type":"object","properties":{"action":{"oneOf":[{"type":"object","properties":{"button":{"type":"string","enum":["left","right","wheel","back","forward"],"description":"Indicates which mouse button was pressed during the click."},"type":{"type":"string","enum":["click"],"description":"Specifies the event type. For a click action, this property is always set to click."},"x":{"type":"integer","description":"The x-coordinate where the click occurred."},"y":{"type":"integer","description":"The y-coordinate where the click occurred."}},"required":["button","type","x","y"],"description":"A click action."},{"type":"object","properties":{"type":{"type":"string","enum":["double_click"],"description":"Specifies the event type. For a double click action, this property is always set to double_click."},"x":{"type":"integer","description":"The x-coordinate where the double click occurred."},"y":{"type":"integer","description":"The y-coordinate where the double click occurred."}},"required":["type","x","y"],"description":"A double click action."},{"type":"object","properties":{"path":{"type":"array","items":{"type":"object","properties":{"x":{"type":"integer","description":"The y-coordinate."},"y":{"type":"integer","description":"The y-coordinate."}},"required":["x","y"]},"description":"An array of coordinates representing the path of the drag action. Coordinates will appear as an array of objects, eg"},"type":{"type":"string","enum":["drag"],"description":"Specifies the event type. For a drag action, this property is always set to drag."}},"required":["path","type"],"description":"A drag action."},{"type":"object","properties":{"keys":{"type":"array","items":{"type":"string"},"description":"The combination of keys the model is requesting to be pressed. This is an array of strings, each representing a key."},"type":{"type":"string","enum":["keypress"],"description":"Specifies the event type. For a keypress action, this property is always set to keypress."}},"required":["keys","type"],"description":"A collection of keypresses the model would like to perform."},{"type":"object","properties":{"type":{"type":"string","enum":["move"],"description":"Specifies the event type. For a move action, this property is always set to move."},"x":{"type":"integer","description":"The x-coordinate to move to."},"y":{"type":"integer","description":"The y-coordinate to move to."}},"required":["type","x","y"],"description":"A mouse move action."},{"type":"object","properties":{"type":{"type":"string","enum":["screenshot"],"description":"Specifies the event type. For a screenshot action, this property is always set to screenshot."}},"required":["type"],"description":"A screenshot action."},{"type":"object","properties":{"type":{"type":"string","enum":["scroll"],"description":"Specifies the event type. For a scroll action, this property is always set to scroll."},"scroll_x":{"type":"integer","description":"The horizontal scroll distance."},"scroll_y":{"type":"integer","description":"The vertical scroll distance."},"x":{"type":"integer","description":"The x-coordinate where the scroll occurred."},"y":{"type":"integer","description":"The y-coordinate where the scroll occurred."}},"required":["type","scroll_x","scroll_y","x","y"],"description":"A scroll action."},{"type":"object","properties":{"type":{"type":"string","enum":["type"],"description":"Specifies the event type. For a type action, this property is always set to type."},"text":{"type":"string","description":"The text to type."}},"required":["type","text"],"description":"An action to type in text."},{"type":"object","properties":{"type":{"type":"string","enum":["wait"],"description":"Specifies the event type. For a wait action, this property is always set to wait."}},"required":["type"],"description":"A wait action."}]},"call_id":{"type":"string","description":"An identifier used when responding to the tool call with output."},"id":{"type":"string","description":"The unique ID of the computer call."},"pending_safety_checks":{"type":"array","items":{"type":"object","properties":{"code":{"type":"string","description":"The type of the pending safety check."},"id":{"type":"string","description":"The ID of the pending safety check."},"message":{"type":"string","description":"Details about the pending safety check."}},"required":["code","id","message"]},"description":"The pending safety checks for the computer call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."},"type":{"type":"string","enum":["computer_call"],"description":"The type of the computer call. Always computer_call."}},"required":["action","call_id","id","pending_safety_checks","status","type"],"description":"A tool call to a computer use tool."},{"type":"object","properties":{"call_id":{"type":"string","description":"The ID of the computer tool call that produced the output."},"output":{"type":"object","properties":{"type":{"type":"string","enum":["computer_screenshot"],"description":"Specifies the event type. For a computer screenshot, this property is always set to computer_screenshot."},"image_url":{"type":"string","format":"uri","description":"The URL of the screenshot image."}},"required":["type"],"description":"A computer screenshot image used with the computer use tool."},"type":{"type":"string","enum":["computer_call_output"],"description":"The type of the computer tool call output. Always computer_call_output."},"acknowledged_safety_checks":{"type":"array","nullable":true,"items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the pending safety check."},"code":{"type":"string","nullable":true,"description":"The type of the pending safety check."},"message":{"type":"string","nullable":true,"description":"Details about the pending safety check."}},"required":["id"]},"description":"The safety checks reported by the API that have been acknowledged by the developer."},"id":{"type":"string","nullable":true,"description":"The ID of the computer tool call output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the message input."}},"required":["call_id","output","type"],"description":"The output of a computer tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the web search tool call."},"status":{"type":"string","enum":["in_progress","completed","searching","failed"],"description":"The status of the web search tool call."},"type":{"type":"string","enum":["web_search_call"],"description":"The type of the web search tool call. Always web_search_call."}},"required":["id","status","type"],"description":"The results of a web search tool call."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments to pass to the function."},"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"name":{"type":"string","description":"The name of the function to run."},"type":{"type":"string","enum":["function_call"],"description":"The type of the function tool call. Always function_call."},"id":{"type":"string","description":"The unique ID of the function tool call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["arguments","call_id","name","type"],"description":"A tool call to run a function."},{"type":"object","properties":{"call_id":{"type":"string","description":"The unique ID of the function tool call generated by the model."},"output":{"anyOf":[{"type":"string","description":"A JSON string of the output of the function tool call."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}}]},"type":{"type":"string","enum":["function_call_output"],"description":"The type of the function tool call output. Always function_call_output."},"id":{"type":"string","nullable":true,"description":"The unique ID of the function tool call output. Populated when this item is returned via API."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["call_id","output","type"],"description":"The output of a function tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique identifier of the reasoning content."},"summary":{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"A short summary of the reasoning used by the model when generating the response."},"type":{"type":"string","enum":["summary_text"],"description":"The type of the object. Always summary_text."}},"required":["text","type"]},"description":"Reasoning text contents."},"type":{"type":"string","enum":["reasoning"],"description":"The type of the object. Always reasoning."},"encrypted_content":{"type":"string","nullable":true,"description":"The encrypted content of the reasoning item - populated when a response is generated with reasoning.encrypted_content in the include parameter."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","summary","type"],"description":"A description of the chain of thought used by a reasoning model while generating a response."},{"type":"object","properties":{"code":{"type":"string","description":"The code to run, or null if not available."},"id":{"type":"string","description":"The unique ID of the code interpreter tool call."},"outputs":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"logs":{"type":"string","description":"The logs output from the code interpreter."},"type":{"type":"string","enum":["logs"],"description":"The type of the output. Always 'logs'."}},"required":["logs","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"url":{"type":"string"}},"required":["type","url"]}]},"description":"The outputs generated by the code interpreter, such as logs or images. Can be null if no outputs are available."},"status":{"type":"string","enum":["in_progress","completed","interpreting"],"description":"The status of the code interpreter tool call."},"type":{"type":"string","enum":["code_interpreter_call"],"description":"The type of the code interpreter tool call. Always code_interpreter_call."},"container_id":{"type":"string","description":"The ID of the container used to run the code."}},"required":["code","id","outputs","status","type","container_id"],"description":"A tool call to run code."},{"type":"object","properties":{"action":{"type":"object","properties":{"command":{"type":"array","items":{"type":"string"},"description":"The command to run."},"env":{"type":"object","additionalProperties":{"type":"string"},"description":"Environment variables to set for the command."},"type":{"type":"string","enum":["exec"],"description":"The type of the local shell action. Always exec."},"timeout_ms":{"type":"integer","nullable":true,"description":"Optional timeout in milliseconds for the command."},"user":{"type":"string","nullable":true,"description":"Optional user to run the command as."},"working_directory":{"type":"string","nullable":true,"description":"Optional working directory to run the command in."}},"required":["command","env","type"],"description":"Execute a shell command on the server."},"call_id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"id":{"type":"string","description":"The unique ID of the local shell call."},"status":{"type":"string","enum":["in_progress","completed","incomplete"],"description":"The status of the local shell call."},"type":{"type":"string","enum":["local_shell_call"],"description":"The type of the local shell call. Always local_shell_call."}},"required":["action","call_id","id","status","type"],"description":"A tool call to run a command on the local shell."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the local shell tool call generated by the model."},"output":{"type":"string","description":"A JSON string of the output of the local shell tool call."},"type":{"type":"string","enum":["local_shell_call_output"],"description":"The type of the local shell tool call output. Always local_shell_call_output."},"status":{"type":"string","nullable":true,"enum":["in_progress","completed","incomplete"],"description":"The status of the item."}},"required":["id","output","type"],"description":"The output of a local shell tool call."},{"type":"object","properties":{"id":{"type":"string","description":"The unique ID of the list."},"server_label":{"type":"string","description":"The label of the MCP server."},"tools":{"type":"array","items":{"type":"object","properties":{"input_schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The JSON schema describing the tool's input."},"name":{"type":"string","description":"The name of the tool."},"annotations":{"type":"object","nullable":true,"additionalProperties":{"nullable":true},"description":"Additional annotations about the tool."},"description":{"type":"string","nullable":true,"description":"The description of the tool."}},"required":["input_schema","name"]},"description":"The tools available on the server."},"type":{"type":"string","enum":["mcp_list_tools"],"description":"The type of the item. Always mcp_list_tools."},"error":{"type":"string","nullable":true,"description":"Error message if the server could not list tools."}},"required":["id","server_label","tools","type"],"description":"A list of tools available on an MCP server."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of arguments for the tool."},"id":{"type":"string","description":"The unique ID of the approval request."},"name":{"type":"string"},"server_label":{"type":"string","description":"The name of the tool to run."},"type":{"type":"string","enum":["mcp_approval_request"],"description":"The type of the item. Always mcp_approval_request."}},"required":["arguments","id","name","server_label","type"],"description":"A request for human approval of a tool invocation."},{"type":"object","properties":{"approval_request_id":{"type":"string","description":"The ID of the approval request being answered."},"approve":{"type":"boolean","description":"Whether the request was approved."},"type":{"type":"string","enum":["mcp_approval_response"],"description":"The type of the item. Always mcp_approval_response."},"id":{"type":"string","nullable":true,"description":"The unique ID of the approval response."},"reason":{"type":"string","nullable":true,"description":"Optional reason for the decision."}},"required":["approval_request_id","approve","type"],"description":"A response to an MCP approval request."},{"type":"object","properties":{"arguments":{"type":"string","description":"A JSON string of the arguments passed to the tool."},"id":{"type":"string","description":"The unique ID of the tool call."},"name":{"type":"string","description":"The name of the tool that was run."},"server_label":{"type":"string","description":"The label of the MCP server running the tool."},"type":{"type":"string","enum":["mcp_call"],"description":"The type of the item. Always mcp_call."},"error":{"type":"string","nullable":true,"description":"The error from the tool call, if any."},"output":{"type":"string","nullable":true,"description":"The output from the tool call."}},"required":["arguments","id","name","server_label","type"],"description":"An invocation of a tool on an MCP server."},{"type":"object","properties":{"id":{"type":"string","description":"The ID of the item to reference."},"type":{"type":"string","nullable":true,"enum":["item_reference"],"description":"The type of item to reference. Always item_reference."}},"required":["id"],"description":"An internal identifier for an item to reference."}]},"description":"A list of one or many input items to the model, containing different content types."}],"description":"Text, image, or file inputs to the model, used to generate a response."},"include":{"type":"array","nullable":true,"items":{"type":"string","enum":["message.input_image.image_url","computer_call_output.output.image_url","reasoning.encrypted_content","code_interpreter_call.outputs"]},"description":"Specify additional output data to include in the model response. Currently supported values are:\n    - code_interpreter_call.outputs: Includes the outputs of python code execution in code interpreter tool call items.\n    - computer_call_output.output.image_url: Include image urls from the computer call output.\n    - file_search_call.results: Include the search results of the file search tool call.\n    - message.output_text.logprobs: Include logprobs with assistant messages.\n    - reasoning.encrypted_content: Includes an encrypted version of reasoning tokens in reasoning item outputs. This enables reasoning items to be used in multi-turn conversations when using the Responses API statelessly (like when the store parameter is set to false, or when an organization is enrolled in the zero data retention program).\n"},"instructions":{"type":"string","nullable":true,"description":"A system (or developer) message inserted into the model's context.\n\nWhen using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses."},"max_output_tokens":{"type":"integer","default":512,"description":"An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n\nKeys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"parallel_tool_calls":{"type":"boolean","nullable":true,"description":"Whether to allow the model to run tool calls in parallel."},"previous_response_id":{"type":"string","nullable":true,"description":"The unique ID of the previous response to the model. Use this to create multi-turn conversations."},"prompt":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"The unique identifier of the prompt template to use."},"variables":{"type":"object","nullable":true,"additionalProperties":{"anyOf":[{"type":"string"},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["input_text"],"description":"The type of the input item. Always input_text."},"text":{"type":"string","description":"The text input to the model."}},"required":["type","text"],"description":"A text input to the model."},{"type":"object","properties":{"type":{"type":"string","enum":["input_image"],"description":"The type of the input item. Always input_image."},"detail":{"type":"string","enum":["high","low","auto"],"default":"auto","description":"The detail level of the image to be sent to the model. One of high, low, or auto."},"image_url":{"type":"string","nullable":true,"description":"The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_file"],"description":"The type of the input item. Always input_file."},"file_data":{"type":"string","description":"The content of the file to be sent to the model."},"filename":{"type":"string","description":"The name of the file to be sent to the model."}},"required":["type"]}]}]},"description":"Optional map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input types like images or files."},"version":{"type":"string","nullable":true,"description":"Optional version of the prompt template."}},"required":["id"],"description":"Reference to a prompt template and its variables."},"reasoning":{"type":"object","nullable":true,"properties":{"effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"summary":{"type":"string","nullable":true,"enum":["auto","concise","detailed"],"description":"A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process."}},"description":"o-series models only\nConfiguration options for reasoning models."},"store":{"type":"boolean","nullable":true,"default":false,"description":"Whether to store the generated model response for later retrieval via API."},"stream":{"type":"boolean","nullable":true,"default":false,"description":"If set to true, the model response data will be streamed to the client as it is generated using server-sent events. "},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"text":{"type":"object","properties":{"format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"type":{"type":"string","enum":["json_schema"]},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name","schema","type"],"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["format"],"description":"Configuration options for a text response from the model. Can be plain text or structured JSON data."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"Controls which (if any) tool is called by the model.\n\nnone means the model will not call any tool and instead generates a message.\n\nauto means the model can pick between generating a message or calling one or more tools.\n\nrequired means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11","computer_use_preview","code_interpreter","mcp"]}},"required":["type"],"description":"Indicates that the model should use a built-in tool to generate a response."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"type":{"type":"string","enum":["function"],"description":"For function calling, the type is always function."}},"required":["name","type"]}],"description":"How the model should select which tool (or tools) to use when generating a response."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["web_search_preview","web_search_preview_2025_03_11"],"description":"The type of the web search tool. One of web_search_preview or web_search_preview_2025_03_11."},"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."},"city":{"type":"string","nullable":true,"description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","nullable":true,"pattern":"^[A-Z]{2}$","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","nullable":true,"description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","nullable":true,"description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"required":["type"],"description":"The user's location"}},"required":["type"],"description":"This tool searches the web for relevant results to use in a response."},{"type":"object","properties":{"display_height":{"type":"integer","description":"The height of the computer display."},"display_width":{"type":"integer","description":"The width of the computer display."},"environment":{"type":"string","enum":["windows","mac","linux","ubuntu","browser"],"description":"The type of computer environment to control."},"type":{"type":"string","enum":["computer_use_preview"],"description":"The type of the computer use tool. Always computer_use_preview."}},"required":["display_height","display_width","environment","type"],"description":"A tool that controls a virtual computer."},{"type":"object","properties":{"server_label":{"type":"string","description":"A label for this MCP server, used to identify it in tool calls."},"server_url":{"type":"string","description":"The URL for the MCP server."},"type":{"type":"string","enum":["mcp"],"description":"The type of the MCP tool. Always mcp."},"allowed_tools":{"anyOf":[{"type":"array","items":{"type":"string"},"description":"A string array of allowed tool names."},{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of allowed tool names."}},"description":"A filter object to specify which tools are allowed."},{"nullable":true}],"description":"List of allowed tool names or a filter object."},"headers":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Optional HTTP headers to send to the MCP server. Use for authentication or other purposes."},"require_approval":{"anyOf":[{"type":"string","enum":["always","never"]},{"type":"object","properties":{"always":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that require approval."}},"description":"A list of tools that always require approval."},"never":{"type":"object","properties":{"tool_names":{"type":"array","items":{"type":"string"},"description":"List of tools that do not require approval."}},"description":"A list of tools that never require approval."}}},{"nullable":true}],"description":"Specify which of the MCP server's tools require approval."}},"required":["server_label","server_url","type"],"description":"Give the model access to additional tools via remote Model Context Protocol (MCP) servers."},{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of the code interpreter tool. Always code_interpreter."},"container":{"anyOf":[{"type":"string"},{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]}],"description":"The container ID."}},"required":["type","container"],"description":"A tool that runs Python code to help generate a response to a prompt."},{"type":"object","properties":{"type":{"type":"string","enum":["local_shell"],"description":"The type of the local shell tool. Always local_shell."}},"required":["type"],"description":"A tool that allows the model to execute shell commands in a local environment."},{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"A JSON schema object describing the parameters of the function."},"strict":{"type":"boolean","description":"Whether to enforce strict parameter validation."},"type":{"type":"string","enum":["function"],"description":"The type of the function tool. Always function."},"description":{"type":"string","description":"A description of the function. Used by the model to determine whether or not to call the function."}},"required":["name","parameters","strict","type"],"description":"Defines a function in your own code the model can choose to call."}]},"description":"An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter."},"top_p":{"type":"number","nullable":true,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both."},"truncation":{"type":"string","enum":["auto","disabled"],"default":"disabled","description":"The truncation strategy to use for the model response.\n\n    - auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.\n    - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.\n"}},"required":["model","input"]}}},"paths":{"/v1/responses":{"post":{"operationId":"ResponseApiController_createResponse_v1","summary":"","parameters":[],"requestBody":{"required":true,"content":{"application/json":{"schema":{"$ref":"#/components/schemas/Response.v1.CreateResponsePayload"}}}},"responses":{"201":{"description":""}},"tags":["Response Api"]}}}}
```

## Code Example

{% tabs %}
{% tab title="Python" %}
{% code overflow="wrap" %}

```python
import requests
import json   # for getting a structured output with indentation

response = requests.post(
    "https://api.aimlapi.com/v1/responses",
    headers={
        "Content-Type":"application/json", 

        # Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>:
        "Authorization":"Bearer <YOUR_AIMLAPI_KEY>",
        "Content-Type":"application/json"
    },
    json={
        "model":"openai/o3-pro",
        "input":"Hello"  # Insert your question for the model here, instead of Hello   
    }
)

data = response.json()
print(json.dumps(data, indent=2, ensure_ascii=False))
```

{% endcode %}
{% endtab %}

{% tab title="JavaScript" %}
{% code overflow="wrap" %}

```javascript
async function main() {
  try {
    const response = await fetch('https://api.aimlapi.com/v1/responses', {
      method: 'POST',
      headers: {
        // Insert your AIML API Key instead of <YOUR_AIMLAPI_KEY>
        'Authorization': 'Bearer <YOUR_AIMLAPI_KEY>',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'openai/o3-pro',
        input: 'Hello',  // Insert your question here, instead of Hello 
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status ${response.status}`);
    }

    const data = await response.json();
    console.log(JSON.stringify(data, null, 2));

  } catch (error) {
    console.error('Error', error);
  }
}

main();
```

{% endcode %}
{% endtab %}
{% endtabs %}

<details>

<summary>Response</summary>

{% code overflow="wrap" %}

```json5
{
  "id": "resp_686ba45ce63481a2a4b1fad55d2bea8102a1cc22f1a1bcf1",
  "object": "response",
  "created_at": 1751884892,
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": 512,
  "model": "o3-pro-2025-06-10",
  "output": [
    {
      "id": "rs_686ba463d18481a29dde85cfd7b055bf02a1cc22f1a1bcf1",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "msg_686ba463d4e081a2b2e2aff962ab00f702a1cc22f1a1bcf1",
      "type": "message",
      "status": "in_progress",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Hello! How can I help you today?"
        }
      ],
      "role": "assistant"
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": "medium",
    "summary": null
  },
  "temperature": 1,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 294,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 2520,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 2814
  },
  "metadata": {},
  "output_text": "Hello! How can I help you today?"
}
```

{% endcode %}

</details>




---

[Next Page](/llms-full.txt/1)

