# Source: https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-evaluate.md

# Overview of Galileo Evaluate

> Stop experimenting in spreadsheets and notebooks. Use Evaluateâ€™s powerful insights to build GenAI systems that just work.

<img src="https://mintlify.s3.us-west-1.amazonaws.com/galileo/images/evaluate-slide.png" width="100%" height="480px" />

*Galileo Evaluate* is a powerful bench for rapid, collaborative experimentation and evaluation of your LLM applications.

## Core features

* **Tracing and Visualizations** - Track the end-to-end execution of your queries. See what happened along the way and where things went wrong.

* **State-of-the-art Metrics -** Combine our research-backed Guardrail Metrics with your own Custom Metrics to evaluate your system.

* **Experiment Management** - Track all your experiments in one place. Find the best configuration for your system.

<Frame caption="An Evaluation Run of a RAG Workflow">
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/galileo/images/evaluate.webp" />
</Frame>

### The Workflow

<Steps>
  <Step title="Log your runs">Integrate promptquality into your system or test a template model combination through the Playground. Choose and register your metrics to define what success means for your use case.</Step>
  <Step title="Analyze results">Identify poor perfomance, trace it to the broken step, form hypothesis on what could be behind it.</Step>
  <Step title="Debug, Fix & Run another Eval">Tweak your system and try again until your quality bar is met.</Step>
</Steps>

### Getting Started

<CardGroup cols={1}>
  <Card title="Quickstart" icon="chevron-right" href="/galileo/gen-ai-studio-products/galileo-evaluate/quickstart" horizontal />
</CardGroup>
