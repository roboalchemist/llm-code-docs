# Source: https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-evaluate/how-to/collaborate-with-other-personas.md

# Collaborate with other personas

> Galileo Evaluate is geared for cross-functional collaboration. Most of the teams using Galileo consist of a mix of the following personas

![collaborate](https://mintlify.s3.us-west-1.amazonaws.com/galileo/images/collab.png)

* The AI Engineer: Responsible for building and productionizing an AI-powered feature or product.

* The PM or Subject Matter Expert: Often, a non-technical persona. Responsible for evaluating the quality and production-readiness of a feature or application.

* The Annotator: Often, the same as the Subject Matter Expert. Tasked with going through individual LLM requests and responses, performing qualitative evaluations and annotating the runs with findings.

To collaborate with other users, you need to [share your project](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/share-a-project).

## How-to Guides for different personas

If you're an **AI Engineer,** check out the following sections:

* [Quickstart](/galileo/gen-ai-studio-products/galileo-evaluate/quickstart)

* Evaluate and Optimize [Prompts](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/evaluate-and-optimize-prompts), [RAG Applications](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/evaluate-and-optimize-rag-applications), [Agents or Chains](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/evaluate-and-optimize-agents--chains-or-multi-step-workflows)

* [Register Custom Metrics](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/register-custom-metrics)

* [Log Pre-generated Responses](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/log-pre-generated-responses-in-python)

* [Prompt Management and Storage](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/prompt-management-storage)

* Experiment with [Multiple Prompts](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/experiment-with-multiple-prompts) or [Chain Workflows](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/experiment-with-multiple-chain-workflows)

If you're a **PM or SME**, check out the following sections:

* [Choose your Guardrail Metrics](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/choose-your-guardrail-metrics)

* Evaluate and Optimize [Prompts](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/evaluate-and-optimize-prompts), [RAG Applications](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/evaluate-and-optimize-rag-applications), [Agents or Chains](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/evaluate-and-optimize-agents--chains-or-multi-step-workflows)

* [A/B Compare Prompts](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/a-b-compare-prompts)

* [Evaluate with Human Feedback](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/evaluate-with-human-feedback)

If you're an **Annotator**, check out:

* [Evaluate with Human Feedback](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/evaluate-with-human-feedback)
