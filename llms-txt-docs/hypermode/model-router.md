# Source: https://docs.hypermode.com/model-router.md

# Model Router

> Iterate quickly with seamless access to the most popular models

With Model Router, you can access the most popular models with a single endpoint
and bill. Experiment with new models and scale your app without worrying about
the underlying infrastructure.

## Setup

Getting started with Model Router is simple. Generate an API key and drop it
into your favorite framework.

### Generate API key

API keys for Model Router are generated within your workspace. Generate a key by
[logging into the console](https://hypermode.com/login) and navigating to
**Model router** â†’ **API keys**.

### Connect via framework

Model Router integrates easily into the most popular frameworks.

<Tabs>
  <Tab title="OpenAI SDK">
    Model Router is a drop-in replacement for OpenAI's API.

    <CodeGroup>
      ```python Python
      import openai

      # Configure with your Hypermode Workspace API key and Hypermode Model Router base url
      client = openai.OpenAI(
          api_key="<YOUR_HYP_WKS_KEY>",
          base_url="https://models.hypermode.host/v1",
      )

      # Set up the request
      response = client.chat.completions.create(
          model="meta-llama/llama-4-scout-17b-16e-instruct",
          messages=[
              {"role": "system", "content": "You are a helpful assistant."},
              {"role": "user", "content": "What is Modus?"},
          ],
          max_tokens=150,
          temperature=0.7,
      )

      # Print the response
      print(response.choices[0].message.content)

      ```

      ```typescript TypeScript
      import OpenAI from "openai"

      // Configure with your Hypermode Workspace API key and Hypermode Model Router base url
      const client = new OpenAI({
        apiKey: "<YOUR_HYP_WKS_KEY>",
        baseURL: "https://models.hypermode.host/v1",
      })

      async function generateCompletion() {
        try {
          // Set up the request
          const response = await client.chat.completions.create({
            model: "meta-llama/llama-4-scout-17b-16e-instruct",
            messages: [
              { role: "system", content: "You are a helpful assistant." },
              { role: "user", content: "What is Modus?" },
            ],
            max_tokens: 150,
            temperature: 0.7,
          })

          // Print the response
          console.log(response.choices[0].message.content)
        } catch (error) {
          console.error("Error:", error)
        }
      }

      // Call the function
      generateCompletion()
      ```
    </CodeGroup>
  </Tab>

  <Tab title="Vercel AI SDK">
    ```typescript
    import { createOpenAI } from "@ai-sdk/openai"
    import { generateText } from "ai"

    // Configure with your Hypermode Workspace API key and Hypermode Model Router base url
    const hypermode = createOpenAI({
      name: "hypermode",
      apiKey: "<YOUR_HYP_WKS_KEY>",
      baseURL: "https://models.hypermode.host/v1",
    })

    async function generateHoliday() {
      try {
        const { text, usage, providerMetadata } = await generateText({
          model: hypermode("o3-mini"),
          prompt: "Invent a new holiday and describe its traditions.",
          providerOptions: { hypermode: { reasoningEffort: "low" } },
        })

        // Print the response
        console.log(text)
        return { text, usage, providerMetadata }
      } catch (error) {
        console.error("Error generating text:", error)
        throw error
      }
    }

    // Call the function
    generateHoliday()
    ```
  </Tab>

  <Tab title="Modus">
    To use the Model Router in a Modus app, in your
    [app manifest](/modus/app-manifest) create a connection to the Model Router and
    add a model that references that connection.

    ```json modus.json {11-16}
    {
        ...
      "models": {
        "meta-llama-llama-4-scout-17b-16e-instruct": {
          "sourceModel": "meta-llama/llama-4-scout-17b-16e-instruct",
          "connection": "model-router",
          "path": "v1/chat/completions"
        },
      },
      "connections": {
        "model-router": {
          "type": "http",
          "baseUrl": "https://models.hypermode.host/",
          "headers": {
            "Authorization": "Bearer {{YOUR_HYP_WKS_KEY}}"
          }
        }
      },
      ...
    }
    ```
  </Tab>
</Tabs>

### Connect directly via API

You can also access the API directly.

<Tabs>
  <Tab title="Generation">
    <CodeGroup>
      ```bash curl
      curl -X POST \
        https://models.hypermode.host/v1/chat/completions \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer $YOUR_HYP_WKS_KEY" \
        -d '{
          "model": "meta-llama/llama-4-scout-17b-16e-instruct",
          "messages": [
              {"role": "system", "content": "You are a helpful assistant."},
              {"role": "user", "content": "What is Dgraph?"}
          ],
          "max_tokens": 150,
          "temperature": 0.7
        }'
      ```

      ```python Python
      import json
      import requests

      # Your Hypermode Workspace API key
      api_key = "<YOUR_HYP_WKS_KEY>"

      # Use the Hypermode Model Router base url
      base_url = "https://models.hypermode.host/v1"

      # API endpoint
      endpoint = f"{base_url}/chat/completions"

      # Headers
      headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}

      # Request payload
      payload = {
          "model": "meta-llama/llama-4-scout-17b-16e-instruct",
          "messages": [
              {"role": "system", "content": "You are a helpful assistant."},
              {"role": "user", "content": "What is Dgraph?"},
          ],
          "max_tokens": 150,
          "temperature": 0.7,
      }

      # Make the API request
      with requests.post(endpoint, headers=headers, data=json.dumps(payload)) as response:
          response.raise_for_status()
          print(response.json()["choices"][0]["message"]["content"])
      ```

      ```typescript TypeScript
      // Define types for API responses
      interface ChatCompletionChoice {
        message: {
          role: string
          content: string
        }
        index: number
        finish_reason: string
      }

      interface ChatCompletionResponse {
        id: string
        object: string
        created: number
        model: string
        choices: ChatCompletionChoice[]
        usage: {
          prompt_tokens: number
          completion_tokens: number
          total_tokens: number
        }
      }

      async function callOpenAI(): Promise<void> {
        // Your Hypermode Workspace API key
        const apiKey = "<YOUR_HYP_WKS_KEY>"

        // Use the Hypermode Model Router base url
        const baseUrl = "https://models.hypermode.host/v1"

        // API endpoint
        const endpoint = `${baseUrl}/chat/completions`

        // Request payload
        const payload = {
          model: "meta-llama/llama-4-scout-17b-16e-instruct",
          messages: [
            { role: "system", content: "You are a helpful assistant." },
            { role: "user", content: "What is Dgraph?" },
          ],
          max_tokens: 150,
          temperature: 0.7,
        }

        try {
          // Make the API request
          const response = await fetch(endpoint, {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              Authorization: `Bearer ${apiKey}`,
            },
            body: JSON.stringify(payload),
          })

          // Check if the request was successful
          if (response.ok) {
            // Parse and print the response
            const responseData: ChatCompletionResponse = await response.json()
            console.log(responseData.choices[0].message.content)
          } else {
            // Print error information
            console.error(`Error: ${response.status}`)
            console.error(await response.text())
          }
        } catch (error) {
          console.error("Request failed:", error)
        }
      }

      // Call the function
      callOpenAI()
      ```

      ```go Go
      package main

      import (
        "bytes"
        "encoding/json"
        "fmt"
        "io"
        "net/http"
        "os"
      )

      // Define structs for the request and response
      type Message struct {
        Role    string `json:"role"`
        Content string `json:"content"`
      }

      type ChatCompletionRequest struct {
        Model       string    `json:"model"`
        Messages    []Message `json:"messages"`
        MaxTokens   int       `json:"max_tokens"`
        Temperature float64   `json:"temperature"`
      }

      type ChatCompletionChoice struct {
        Message      Message `json:"message"`
        Index        int     `json:"index"`
        FinishReason string  `json:"finish_reason"`
      }

      type Usage struct {
        PromptTokens     int `json:"prompt_tokens"`
        CompletionTokens int `json:"completion_tokens"`
        TotalTokens      int `json:"total_tokens"`
      }

      type ChatCompletionResponse struct {
        ID      string                 `json:"id"`
        Object  string                 `json:"object"`
        Created int64                  `json:"created"`
        Model   string                 `json:"model"`
        Choices []ChatCompletionChoice `json:"choices"`
        Usage   Usage                  `json:"usage"`
      }

      func main() {
        // Your Hypermode Workspace API key
        apiKey := "<YOUR_HYP_WKS_KEY>"

        // Use the Hypermode Model Router base url
        baseURL := "https://models.hypermode.host/v1"

        // API endpoint
        endpoint := baseURL + "/chat/completions"

        // Create the request payload
        requestBody := ChatCompletionRequest{
          Model: "meta-llama/llama-4-scout-17b-16e-instruct",
          Messages: []Message{
            {
              Role:    "system",
              Content: "You are a helpful assistant.",
            },
            {
              Role:    "user",
              Content: "What is Dgraph?",
            },
          },
          MaxTokens:   150,
          Temperature: 0.7,
        }

        // Convert the request to JSON
        jsonData, err := json.Marshal(requestBody)
        if err != nil {
          fmt.Printf("Error marshaling JSON: %v\n", err)
          os.Exit(1)
        }

        // Create an HTTP request
        req, err := http.NewRequest("POST", endpoint, bytes.NewBuffer(jsonData))
        if err != nil {
          fmt.Printf("Error creating request: %v\n", err)
          os.Exit(1)
        }

        // Set headers
        req.Header.Set("Content-Type", "application/json")
        req.Header.Set("Authorization", "Bearer "+apiKey)

        // Create an HTTP client and send the request
        client := &http.Client{}
        resp, err := client.Do(req)
        if err != nil {
          fmt.Printf("Error sending request: %v\n", err)
          os.Exit(1)
        }
        defer resp.Body.Close()

        // Read the response body
        body, err := io.ReadAll(resp.Body)
        if err != nil {
          fmt.Printf("Error reading response: %v\n", err)
          os.Exit(1)
        }

        // Check if the request was successful
        if resp.StatusCode == http.StatusOK {
          // Parse and print the response
          var response ChatCompletionResponse
          err = json.Unmarshal(body, &response)
          if err != nil {
            fmt.Printf("Error parsing response: %v\n", err)
            os.Exit(1)
          }

          fmt.Println(response.Choices[0].Message.Content)
        } else {
          // Print error information
          fmt.Printf("Error: %d\n", resp.StatusCode)
          fmt.Println(string(body))
        }
      }

      ```
    </CodeGroup>
  </Tab>

  <Tab title="Embedding">
    <CodeGroup>
      ```bash curl
      # Note: verify that your model supports the dimensions parameter
      # this is the case for OpenAI 'text-embedding-3' and later models.

      curl -X POST \
        https://models.hypermode.host/v1/embeddings \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer $YOUR_HYP_WKS_KEY" \
        -d '{
          "model": "text-embedding-3-large",
          "dimensions": 256,
          "input": [
              "Hypermode is an AI development platform that provides hosting and management capabilities for agents and knowledge graphs",
              "Modus is an open source, serverless framework for building functions and APIs, powered by WebAssembly."
          ],
          "encoding_format": "float"
        }'

      ```

      ```python Python
      import requests
      import json

      # Your Hypermode Workspace API key
      api_key = "<YOUR_HYP_WKS_KEY>"

      # Use the Hypermode Model Router base url
      base_url = "https://models.hypermode.host/v1"

      # API endpoint
      endpoint = f"{base_url}/embeddings"

      # Headers
      headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}

      # Request payload
      payload = {
          "model": "text-embedding-3-large",
          "input": [
              "Hypermode is an AI development platform that provides hosting and management capabilities for agents and knowledge graphs",
              "Modus is an open source, serverless framework for building functions and APIs, powered by WebAssembly.",
          ],
          "dimensions": 256,
      }

      # Make the API request
      with requests.post(endpoint, headers=headers, data=json.dumps(payload)) as response:
          response.raise_for_status()
          print(response.json()["data"])
      ```

      ```typescript TypeScript
      // Define types for API responses
      interface EmbeddingResult {
        object: string
        index: number
        embedding: number[]
      }

      interface EmbeddingResponse {
        object: string
        data: EmbeddingResult[]
        model: string
        usage: {
          prompt_tokens: number
          total_tokens: number
        }
      }

      async function testEmbeddings(): Promise<void> {
        // Your Hypermode Workspace API key
        const apiKey = "<YOUR_HYP_WKS_KEY>"

        // Use the Hypermode Model Router base url
        const baseUrl = "https://models.hypermode.host/v1"

        // API endpoint
        const endpoint = `${baseUrl}/embeddings`

        // Request payload
        const payload = {
          model: "text-embedding-3-large",
          input: [
            "Hypermode is an AI development platform that provides hosting and management capabilities for agents and knowledge graphs",
            "Modus is an open source, serverless framework for building functions and APIs, powered by WebAssembly.",
          ],
          dimensions: 256,
        }

        try {
          // Make the API request
          const response = await fetch(endpoint, {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              Authorization: `Bearer ${apiKey}`,
            },
            body: JSON.stringify(payload),
          })

          // Check if the request was successful
          if (response.ok) {
            // Parse and print the response
            const responseData: EmbeddingResponse = await response.json()
            console.log(responseData.data)
          } else {
            // Print error information
            console.error(`Error: ${response.status}`)
            console.error(await response.text())
          }
        } catch (error) {
          console.error("Request failed:", error)
        }
      }

      // Call the function
      testEmbeddings()
      ```

      ```go Go
      package main

      import (
        "bytes"
        "encoding/json"
        "fmt"
        "io"
        "net/http"
        "os"
      )

      // Define structs for the request and response
      type Message struct {
        Role    string `json:"role"`
        Content string `json:"content"`
      }

      type EmbeddingRequest struct {
        Model       string    `json:"model"`
        Input    []string `json:"input"`
        Dimensions   int       `json:"dimensions"`
      }

      type EmbeddingResult struct {
        Object    string       `json:"object"`
        Index     int           `json:"index"`
        Embedding []float       `json:"embedding"`
      }

      type Usage struct {
        PromptTokens     int `json:"prompt_tokens"`
        TotalTokens      int `json:"total_tokens"`
      }

      type EmbeddingsResponse struct {
        Object  string                 `json:"object"`
        Model   string                 `json:"model"`
        Data []EmbeddingResult `json:"data"`
        Usage   Usage                  `json:"usage"`
      }

      func main() {
        // Your Hypermode Workspace API key
        apiKey := "<YOUR_HYP_WKS_KEY>"

        // Use the Hypermode Model Router base url
        baseURL := "https://models.hypermode.host/v1"

        // API endpoint
        endpoint := baseURL + "/embeddings"

        // Create the request payload
        requestBody := EmbeddingRequest{
          Model: "text-embedding-3-large",
          Input: []string{
            "Hypermode is an AI development platform that provides hosting and management capabilities for agents and knowledge graphs",
            "Modus is an open source, serverless framework for building functions and APIs, powered by WebAssembly.",
          },
          Dimensions:   256,
        }

        // Convert the request to JSON
        jsonData, err := json.Marshal(requestBody)
        if err != nil {
          fmt.Printf("Error marshaling JSON: %v\n", err)
          os.Exit(1)
        }

        // Create an HTTP request
        req, err := http.NewRequest("POST", endpoint, bytes.NewBuffer(jsonData))
        if err != nil {
          fmt.Printf("Error creating request: %v\n", err)
          os.Exit(1)
        }

        // Set headers
        req.Header.Set("Content-Type", "application/json")
        req.Header.Set("Authorization", "Bearer "+apiKey)

        // Create an HTTP client and send the request
        client := &http.Client{}
        resp, err := client.Do(req)
        if err != nil {
          fmt.Printf("Error sending request: %v\n", err)
          os.Exit(1)
        }
        defer resp.Body.Close()

        // Read the response body
        body, err := io.ReadAll(resp.Body)
        if err != nil {
          fmt.Printf("Error reading response: %v\n", err)
          os.Exit(1)
        }

        // Check if the request was successful
        if resp.StatusCode == http.StatusOK {
          // Parse and print the response
          var response EmbeddingsResponse
          err = json.Unmarshal(body, &response)
          if err != nil {
            fmt.Printf("Error parsing response: %v\n", err)
            os.Exit(1)
          }

          fmt.Println(response.Data)
        } else {
          // Print error information
          fmt.Printf("Error: %d\n", resp.StatusCode)
          fmt.Println(string(body))
        }
      }

      ```
    </CodeGroup>
  </Tab>
</Tabs>

## Available models

Hypermode provides a variety of the most popular open source and commercial
models.

<Note>
  We're constantly evaluating model usage in determining new models to add to
  our catalog. Interested in using a model not listed here? Let us know at
  [help@hypermode.com](mailto:help@hypermode.com).
</Note>

### Model Introspection

The full list of available models is available via the API.

```bash curl
curl -X POST \
  https://models.hypermode.host/v1/models \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer <YOUR_HYP_WKS_KEY>"
```

The most popular models are included below for your convenience.

### Generation

Large language models provide text generation and reasoning capabilities.

| Provider  | Model                               | Slug                                        |
| --------- | ----------------------------------- | ------------------------------------------- |
| Anthropic | Claude 4 Sonnet                     | `claude-sonnet-4-20250514`                  |
| Anthropic | Claude 4 Opus                       | `claude-opus-4-20250514`                    |
| Anthropic | Claude 3.7 Sonnet (latest)          | `claude-3-7-sonnet-latest`                  |
| Anthropic | Claude 3.7 Sonnet                   | `claude-3-7-sonnet-20250219`                |
| Anthropic | Claude 3.5 Sonnet (latest)          | `claude-3-5-sonnet-latest`                  |
| Anthropic | Claude 3.5 Sonnet                   | `claude-3-5-sonnet-20241022`                |
| Anthropic | Claude 3.5 Sonnet                   | `claude-3-5-sonnet-20240620`                |
| Anthropic | Claude 3.5 Haiku (latest)           | `claude-3-5-haiku-latest`                   |
| Anthropic | Claude 3.5 Haiku                    | `claude-3-5-haiku-20241022`                 |
| Anthropic | Claude 3 Opus (latest)              | `claude-3-opus-latest`                      |
| Anthropic | Claude 3 Opus                       | `claude-3-opus-20240229`                    |
| Anthropic | Claude 3 Sonnet                     | `claude-3-sonnet-20240229`                  |
| Anthropic | Claude 3 Haiku                      | `claude-3-haiku-20240307`                   |
| DeepSeek  | DeepSeek-R1-Distill-Llama           | `deepseek-ai/deepseek-r1-distill-llama-70b` |
| Google    | Gemini 2.5 Pro                      | `gemini-2.5-pro-exp-03-25`                  |
| Google    | Gemini 2.5 Pro Preview              | `gemini-2.5-pro-preview-05-06`              |
| Google    | Gemini 2.5 Flash Preview            | `gemini-2.5-flash-preview-04-17`            |
| Google    | Gemini 2.0 Flash Lite               | `gemini-2.0-flash-lite`                     |
| Google    | Gemini 2.0 Flash Image Generation   | `gemini-2.0-flash-exp-image-generation`     |
| Google    | Gemini 2.0 Flash Live               | `gemini-2.0-flash-live-001`                 |
| Google    | Gemini 2.0 Flash (latest)           | `gemini-2.0-flash`                          |
| Google    | Gemini 2.0 Flash                    | `gemini-2.0-flash-001`                      |
| Google    | Gemini 1.5 Pro (latest)             | `gemini-1.5-pro-latest`                     |
| Google    | Gemini 1.5 Pro                      | `gemini-1.5-pro`                            |
| Google    | Gemini 1.5 Pro                      | `gemini-1.5-pro-002`                        |
| Google    | Gemini 1.5 Pro                      | `gemini-1.5-pro-001`                        |
| Google    | Gemini 1.5 Flash (latest)           | `gemini-1.5-flash-latest`                   |
| Google    | Gemini 1.5 Flash                    | `gemini-1.5-flash`                          |
| Google    | Gemini 1.5 Flash                    | `gemini-1.5-flash-002`                      |
| Google    | Gemini 1.5 Flash                    | `gemini-1.5-flash-001`                      |
| Google    | Gemini 1.5 Flash 8B (latest)        | `gemini-1.5-flash-8b-latest`                |
| Google    | Gemini 1.5 Flash 8B                 | `gemini-1.5-flash-8b`                       |
| Google    | Gemini 1.5 Flash 8B                 | `gemini-1.5-flash-8b-exp-0924`              |
| Google    | Gemini 1.5 Flash 8B                 | `gemini-1.5-flash-8b-exp-0827`              |
| Google    | Gemini 1.5 Flash 8B                 | `gemini-1.5-flash-8b-001`                   |
| Meta      | Llama 4 Scout                       | `meta-llama/llama-4-scout-17b-16e-instruct` |
| Meta      | Llama 3.3                           | `meta-llama/llama-3.3-70b-versatile`        |
| OpenAI    | O3 (latest)                         | `o3`                                        |
| OpenAI    | O3                                  | `o3-2025-04-16`                             |
| OpenAI    | O4 Mini (latest)                    | `o4-mini`                                   |
| OpenAI    | O4 Mini                             | `o4-mini-2025-04-16`                        |
| OpenAI    | GPT 4.5 Preview (latest)            | `gpt-4.5-preview`                           |
| OpenAI    | GPT 4.5 Preview                     | `gpt-4.5-preview-2025-02-27`                |
| OpenAI    | O3 Mini (latest)                    | `o3-mini`                                   |
| OpenAI    | O3 Mini                             | `o3-mini-2025-01-31`                        |
| OpenAI    | O1 (latest)                         | `o1`                                        |
| OpenAI    | O1                                  | `o1-2024-12-17`                             |
| OpenAI    | O1 Preview (latest)                 | `o1-preview`                                |
| OpenAI    | O1 Preview                          | `o1-preview-2024-09-12`                     |
| OpenAI    | O1 Mini (latest)                    | `o1-mini`                                   |
| OpenAI    | O1 Mini                             | `o1-mini-2024-09-12`                        |
| OpenAI    | GPT 4.1 (latest)                    | `gpt-4.1`                                   |
| OpenAI    | GPT 4.1                             | `gpt-4.1-2025-04-14`                        |
| OpenAI    | GPT 4.1 Mini (latest)               | `gpt-4.1-mini`                              |
| OpenAI    | GPT 4.1 Mini                        | `gpt-4.1-mini-2025-04-14`                   |
| OpenAI    | GPT 4.1 Nano (latest)               | `gpt-4.1-nano`                              |
| OpenAI    | GPT 4.1 Nano                        | `gpt-4.1-nano-2025-04-14`                   |
| OpenAI    | GPT-4o Mini Search Preview (latest) | `gpt-4o-mini-search-preview`                |
| OpenAI    | GPT-4o Mini Search Preview          | `gpt-4o-mini-search-preview-2025-03-11`     |
| OpenAI    | GPT 4o (latest)                     | `gpt-4o`                                    |
| OpenAI    | GPT 4o                              | `gpt-4o-2024-11-20`                         |
| OpenAI    | GPT 4o                              | `gpt-4o-2024-08-06`                         |
| OpenAI    | GPT 4o                              | `gpt-4o-2024-05-13`                         |
| OpenAI    | GPT 4o Mini (latest)                | `gpt-4o-mini`                               |
| OpenAI    | GPT 4o Mini                         | `gpt-4o-mini-2024-07-18`                    |
| OpenAI    | GPT 4o Audio Preview (latest)       | `gpt-4o-audio-preview`                      |
| OpenAI    | GPT 4o Audio Preview                | `gpt-4o-audio-preview-2024-12-17`           |
| OpenAI    | GPT 4o Audio Preview                | `gpt-4o-audio-preview-2024-10-01`           |
| OpenAI    | GPT 4o Search Preview (latest)      | `gpt-4o-search-preview`                     |
| OpenAI    | GPT 4o Search Preview               | `gpt-4o-search-preview-2025-03-11`          |
| OpenAI    | GPT 4o Search Preview               | `gpt-4o-search-preview-2025-03-11`          |
| OpenAI    | ChatGPT 4o                          | `chatgpt-4o-latest`                         |
| OpenAI    | GPT 4 (latest)                      | `gpt-4`                                     |
| OpenAI    | GPT 4                               | `gpt-4-0613`                                |
| OpenAI    | GPT 4 Turbo                         | `gpt-4-turbo-2024-04-09`                    |
| OpenAI    | GPT 4 Turbo Preview                 | `gpt-4-turbo-preview`                       |
| OpenAI    | GPT 4 Preview (latest)              | `gpt-4-1106-preview`                        |
| OpenAI    | GPT 4 Preview                       | `gpt-4-0125-preview`                        |
| OpenAI    | GPT 3.5 Turbo (latest)              | `gpt-3.5-turbo`                             |
| OpenAI    | GPT 3.5 Turbo                       | `gpt-3.5-turbo-1106`                        |
| OpenAI    | GPT 3.5 Turbo                       | `gpt-3.5-turbo-0125`                        |
| Mistral   | Mistral Large (coming soon)         | `mistral-large-latest`                      |
| Mistral   | Pixtral Large (coming soon)         | `pixtral-large-latest`                      |
| Mistral   | Mistral Medium (coming soon)        | `mistral-medium-latest`                     |
| Mistral   | Mistral Moderation (coming soon)    | `mistral-moderation-latest`                 |
| Mistral   | Ministral 3B (coming soon)          | `ministral-3b-latest`                       |
| Mistral   | Ministral 8B (coming soon)          | `ministral-8b-latest`                       |
| Mistral   | Open Mistral Nemo (coming soon)     | `open-mistral-nemo`                         |
| Mistral   | Mistral Small (coming soon)         | `mistral-small-latest`                      |
| Mistral   | Mistral Saba (coming soon)          | `mistral-saba-latest`                       |
| Mistral   | Codestral (coming soon)             | `codestral-latest`                          |
| xAI       | Grok 3 Beta (coming soon)           | `grok-3-beta`                               |
| xAI       | Grok 3 Fast Beta (coming soon)      | `grok-3-fast-beta`                          |
| xAI       | Grok 3 Mini Beta (coming soon)      | `grok-3-mini-beta`                          |
| xAI       | Grok 3 Mini Fast Beta (coming soon) | `grok-3-mini-fast-beta`                     |

### Embedding

Embedding models provide vector representations of text for similarity matching
and other applications.

| Provider     | Model                      | Slug                                     |
| ------------ | -------------------------- | ---------------------------------------- |
| Nomic AI     | Embed Text V1.5            | `nomic-ai/nomic-embed-text-v1.5`         |
| OpenAI       | Embedding 3 Large          | `text-embedding-3-large`                 |
| OpenAI       | Embedding 3 Small          | `text-embedding-3-small`                 |
| OpenAI       | ADA Embedding              | `text-embedding-ada-002`                 |
| Hugging Face | MiniLM-L6-v2 (coming soon) | `sentence-transformers/all-MiniLM-L6-v2` |

## Logging

By default, all model invocations are logged for future display in the console.
If you'd like to opt out of model logging, please
[contact us](mailto:help@hypermode.com).
