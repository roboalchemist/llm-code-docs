# Source: https://docs.hypermode.com/modus/deepseek-model.md

# Using DeepSeek

> Use the DeepSeek-R1 Model with your Modus app

`DeepSeek-R1` is an open source AI reasoning model that rivals the performance
of frontier models such as OpenAI's o1 in complex reasoning tasks like math and
coding. Benefits of DeepSeek include:

* **Performance**: `DeepSeek-R1` achieves comparable results to OpenAI's o1
  model on several benchmarks.
* **Efficiency**: The model uses significantly fewer parameters and therefore
  operates at a lower cost relative to competing frontier models.
* **Open Source**: The open source license allows both commercial and
  non-commercial usage of the model weights and associated code.
* **Novel training approach**: The research team developed DeepSeek-R1 through a
  multi-stage approach that combines reinforcement learning, fine-tuning, and
  data distillation.
* **Distilled versions**: The DeepSeek team released smaller, distilled models
  based on DeepSeek-R1 that offer high reasoning capabilities with fewer
  parameters.

In this guide we review how to use the `DeepSeek-R1` model in your Modus app.

## Options for using DeepSeek with Modus

There are two options for using `DeepSeek-R1` in your Modus app:

1. [Use the distilled `DeepSeek-R1` model hosted by Hypermode](#using-the-distilled-deepseek-model-hosted-by-hypermode)
   Hypermode hosts and makes available the distilled DeepSeek model based on
   `Llama-3.1-8B` enabling Modus apps to use it in both local development
   environments and deployed apps.

2. [Use the DeepSeek Platform API with your Modus app](#using-the-deepseek-platform-api-with-modus)
   Access DeepSeek models hosted on the DeepSeek platform by configuring a
   DeepSeek connection in your Modus app and using your DeepSeek API key

## Using the distilled DeepSeek model hosted by Hypermode

The open source `DeepSeek-R1-Distill-Llama-8B` DeepSeek model is available on
Hypermode's [Model Router](/model-router). This means that we can invoke this
model in a Modus app in both a local development environment and also in an app
deployed on Hypermode.

The `DeepSeek-R1-Distill-Llama-8B` model is a distilled version of the
DeepSeek-R1 model which has been fine-tuned using the `Llama-3.1-8B` model as a
base model, using samples generated by DeepSeek-R1.

Distilled models offer similar high reasoning capabilities with fewer
parameters.

<iframe
  src="https://www.youtube.com/embed/ICRwZ8ywR9Q"
  title="DeepSeek + Modus"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
  style={{ aspectRatio: "16 / 9", width: "100%" }}
/>

<Steps>
  <Step title="Create a Modus app">
    If you haven't already, create a new modus app. Skip this step if you already
    have a Modus app.

    ```sh
    modus new
    ```

    <Tip>
      See [the Modus Quickstart](quickstart) for more information about creating
      Modus projects.
    </Tip>
  </Step>

  <Step title="Add the DeepSeek model to your app manifest">
    Update your Modus app manifest `modus.json` file to specify the
    `DeepSeek-R1-Distill-Llama-8B` model hosted on Hypermode.

    ```json modus.json
      "models": {
        "deepseek-reasoner": {
          "sourceModel": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
          "provider": "hugging-face",
          "connection": "hypermode"
        }
      },
    ```

    <Note>
      Note that we named the model `deepseek-reasoner` in our app manifest, which we
      use to access the model in our Modus function.
    </Note>
  </Step>

  <Step title="Use the Hyp CLI to sign in to Hypermode">
    To use Hypermode hosted models in our local development environment we use the
    `hyp` CLI to log in to Hypermode.

    Install the `hyp` CLI if not previously installed.

    ```sh
    npm i -g @hypermode/hyp-cli
    ```

    Log into Hypermode using the command:

    ```sh
    hyp login
    ```

    If signing in for the first time, Hypermode prompts to create an account and
    specify an organization.
  </Step>

  <Step title="Write a function to invoke the model">
    You can now invoke the model in your Modus app's functions using the Modus
    models interface.

    Here we write a function that takes a prompt as input, invokes the DeepSeek
    model, and returns the generated text as output. At runtime this function
    becomes a GraphQL Query field in the GraphQL API generated by Modus.

    <CodeGroup>
      ```go Go
      package main

      import (
         "strings"

         "github.com/hypermodeinc/modus/sdk/go/pkg/models"
         "github.com/hypermodeinc/modus/sdk/go/pkg/models/openai"
      )

      func GenerateText(prompt string) (string, error) {

         model, err := models.GetModel[openai.ChatModel]("deepseek-reasoner")
         if err != nil {
             return "", err
         }

         // DeepSeek recommends not using a system prompt and including all instructions in the user prompt
         input, err := model.CreateInput(
             openai.NewUserMessage(prompt),
         )
         if err != nil {
             return "", err
         }

         // DeepSeek recommends setting temperature within the range of 0.5-0.7 with 0.6 the recommended default
         input.Temperature = 0.6

         // Here we invoke the model with the input we created.
         output, err := model.Invoke(input)
         if err != nil {
             return "", err
         }

         // The output is also specific to the ChatModel interface.
         // Here we return the trimmed content of the first choice.
         return strings.TrimSpace(output.Choices[0].Message.Content), nil
      }
      ```

      ```ts AssemblyScript
      import { models } from "@hypermode/modus-sdk-as"

      import {
        OpenAIChatModel,
        SystemMessage,
        UserMessage,
      } from "@hypermode/modus-sdk-as/models/openai/chat"

      export function generateText(prompt: string): string {
        const model = models.getModel<OpenAIChatModel>("deepseek-reasoner")

        // DeepSeek recommends not using a system prompt and including all instructions in the user prompt
        const input = model.createInput([new UserMessage(prompt)])

        // DeepSeek recommends setting temperature within the range of 0.5-0.7 with 0.6 the recommended default
        input.temperature = 0.6

        // Here we invoke the model with the input we created.
        const output = model.invoke(input)

        // The output is also specific to the OpenAIChatModel interface.
        // Here we return the trimmed content of the first choice.
        return output.choices[0].message.content.trim()
      }
      ```
    </CodeGroup>

    <Info>
      DeepSeek recommends not using a system prompt with DeepSeek-R1 and setting the
      `temperature` parameter in the range of 0.5-0.7
    </Info>
  </Step>

  <Step title="Run your Modus app">
    Run your Modus app locally:

    ```sh
    modus dev
    ```

    This command compiles your Modus app and starts a local GraphQL API endpoint.
  </Step>

  <Step title="Query your function in the Modus API Explorer">
    Open the Modus API Explorer in your web browser at
    `http://localhost:8686/explorer`.

    Add your prompt as an input argument for the `generateText` query field and
    select "Run" to invoke the DeepSeek model.

        <img src="https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-distilled-model.png?fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=9e75bdceef92e88904785394513be04a" alt="Querying the DeepSeek model using the Modus API Explorer" width="2510" height="1724" data-path="images/how-to-guides/deepseek/api-explorer-distilled-model.png" srcset="https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-distilled-model.png?w=280&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=3e5845d9dd5b37a17023ca675a2c0e82 280w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-distilled-model.png?w=560&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=357d67eec678a8f7cf89dbf4d3c89456 560w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-distilled-model.png?w=840&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=791d048c846884f1bdf04198943d8464 840w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-distilled-model.png?w=1100&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=878609ca7e27f6485ee3bbb1919038fd 1100w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-distilled-model.png?w=1650&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=fac1721b35470d35c82f8ced27f88627 1650w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-distilled-model.png?w=2500&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=061552c1a0e70e2d66a0ed5c573c35dc 2500w" data-optimize="true" data-opv="2" />

    <Tip>
      For mathematical problems,use a directive in your prompt such as: "Please
      reason step by step, and include your final answer within `\boxed{}.`"
    </Tip>
  </Step>
</Steps>

This example demonstrated how to use the DeepSeek-R1 Distilled Model hosted by
Hypermode in a Modus app to create an endpoint that returns text generated by
the DeepSeek model. More advanced use cases for leveraging the DeepSeek
reasoning models in your Modus app include workflows like tool use / function
calling, generating structured outputs, problem solving, code generation, and
much more. [Let us know](mailto:hello@hypermode.com?subject=DeepSeek+Modus) how
you're leveraging DeepSeek with Modus.

## Using the DeepSeek platform API with Modus

This option involves using the DeepSeek models hosted by DeepSeek Platform with
your Modus app. You'll need to create an account with DeepSeek Platform and pay
for your model usage.

<Steps>
  <Step title="Create a DeepSeek API key">
    Create an account with [DeepSeek Platform](https://platform.deepseek.com).

    Once you've signed in select the "API keys" tab and "Create new API token" to
    generate your DeepSeek Platform API token.

        <img src="https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/deepseek-platform-api-token.png?fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=ee425859ed4d718004554b0cd471bbde" alt="DeepSeek Platform API tokens" width="2526" height="770" data-path="images/how-to-guides/deepseek/deepseek-platform-api-token.png" srcset="https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/deepseek-platform-api-token.png?w=280&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=fffb572875277eecac478930a0fe6900 280w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/deepseek-platform-api-token.png?w=560&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=4ca78975b55093528bdcb12087afc3b9 560w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/deepseek-platform-api-token.png?w=840&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=ed42339496aa8a4f3d212db75d7af1e0 840w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/deepseek-platform-api-token.png?w=1100&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=676f34bb5e99bf26dd33729da94a9466 1100w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/deepseek-platform-api-token.png?w=1650&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=b3f211af6416494895c90c7f8e62ebe8 1650w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/deepseek-platform-api-token.png?w=2500&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=e88a78d70709e4fe60a5f360f731e841 2500w" data-optimize="true" data-opv="2" />
  </Step>

  <Step title="Create a Modus app">
    If you haven't already, create a new modus app. Skip this step if you already have a Modus app.

    ```sh
    modus new
    ```

    <Tip>
      See [the Modus Quickstart](quickstart) for more information about creating
      Modus projects.
    </Tip>
  </Step>

  <Step title="Define the model and connection in your app manifest">
    Update your Modus app's `modus.json` app manifest file to include the DeepSeek
    model and a connection for the DeepSeek Platform API. Use `deepseek-reasoner` as
    the value for `sourceModel` for the DeepSeek-R1 reasoning model and
    `deepseek-chat` for the DeepSeek-V3 model.

    ```json modus.json
    {
      "models": {
        "deepseek-reasoner": {
          "sourceModel": "deepseek-reasoner",
          "connection": "deepseek",
          "path": "v1/chat/completions"
        }
      },
      "connections": {
        "deepseek": {
          "type": "http",
          "baseUrl": "https://api.deepseek.com/",
          "headers": {
            "Authorization": "Bearer {{API_TOKEN}}"
          }
        }
      }
    }
    ```

    <Note>
      At query time, Modus replaces the `{{API_TOKEN}}`secret placeholder with your `MODUS_DEEPSEEK_API_TOKEN` environment variable value.

      Set this environment variable value in the next step.
    </Note>
  </Step>

  <Step title="Create environment variable for your API token">
    Edit the `.env.dev.local` file to declare an environment variable for your DeepSeek Platform API key.

    ```env
    MODUS_DEEPSEEK_API_TOKEN=<YOUR_TOKEN_VALUE_HERE>
    ```

    <Note>
      Modus namespaces environment variables for secrets placeholders with `MODUS` and
      your connection's name from the app manifest. For more details on using secrets
      in Modus, refer to
      [working locally with secrets](/modus/app-manifest#working-locally-with-secrets).
    </Note>
  </Step>

  <Step title="Write a function to invoke the DeepSeek model">
    You can now invoke the model in your Modus app's functions using the Modus
    models interface.

    Here we write a function that takes a prompt as input, invokes the DeepSeek
    model, and returns the generated text as output. At runtime this function
    becomes a GraphQL Query field in the GraphQL API generated by Modus.

    <CodeGroup>
      ```go Go
          package main

          import (
              "strings"

              "github.com/hypermodeinc/modus/sdk/go/pkg/models"
              "github.com/hypermodeinc/modus/sdk/go/pkg/models/openai"
          )

          func GenerateText(prompt string) (string, error) {

              model, err := models.GetModel[openai.ChatModel]("deepseek-reasoner")
              if err != nil {
                  return "", err
              }

              // DeepSeek recommends not using a system prompt and including all instructions in the user prompt
              input, err := model.CreateInput(
                  openai.NewUserMessage(prompt),
              )
              if err != nil {
                  return "", err
              }

              // DeepSeek recommends setting temperature within the range of 0.5-0.7 with 0.6 the recommended default
              input.Temperature = 0.6

              // Here we invoke the model with the input we created.
              output, err := model.Invoke(input)
              if err != nil {
                  return "", err
              }

              // The output is also specific to the ChatModel interface.
              // Here we return the trimmed content of the first choice.
              return strings.TrimSpace(output.Choices[0].Message.Content), nil
          }
      ```

      ```ts AssemblyScript
      import { models } from "@hypermode/modus-sdk-as"

      import {
        OpenAIChatModel,
        SystemMessage,
        UserMessage,
      } from "@hypermode/modus-sdk-as/models/openai/chat"

      export function generateText(prompt: string): string {
        const model = models.getModel<OpenAIChatModel>("deepseek-reasoner")

        // DeepSeek recommends not using a system prompt and including all instructions in the user prompt
        const input = model.createInput([new UserMessage(prompt)])

        // DeepSeek recommends setting temperature within the range of 0.5-0.7 with 0.6 the recommended default
        input.temperature = 0.6

        // Here we invoke the model with the input we created.
        const output = model.invoke(input)

        // The output is also specific to the OpenAIChatModel interface.
        // Here we return the trimmed content of the first choice.
        return output.choices[0].message.content.trim()
      }
      ```
    </CodeGroup>

    <Info>
      DeepSeek recommends not using a system prompt with DeepSeek-R1 and setting the
      `temperature` parameter in the range of 0.5-0.7
    </Info>
  </Step>

  <Step title="Run your Modus app">
    Run your Modus app locally:

    ```sh
    modus dev
    ```

    This command compiles your Modus app and starts a local GraphQL API endpoint.
  </Step>

  <Step title="Query using the Modus API Explorer">
    Open the Modus API Explorer in your web browser at `http://localhost:8686/explorer`.

    Add your prompt as an input argument for the `generateText` query field and
    select "Run" to invoke the DeepSeek model.

        <img src="https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-deepseek-api.png?fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=7101a1f3480ebb65832f119615693744" alt="Querying the DeepSeek model using the Modus API Explorer" width="2366" height="1676" data-path="images/how-to-guides/deepseek/api-explorer-deepseek-api.png" srcset="https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-deepseek-api.png?w=280&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=5a49fd24c51eb7b3ed7160a207b0bdc2 280w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-deepseek-api.png?w=560&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=decbc92199ca5fad27ad6deba2e58e1c 560w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-deepseek-api.png?w=840&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=18756a518df700e5726185a141ea6936 840w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-deepseek-api.png?w=1100&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=ea43f85ac7cadefa83c46fc2c9f557c7 1100w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-deepseek-api.png?w=1650&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=4c93cfa45e23d59653589d04a5ad6a09 1650w, https://mintcdn.com/hypermode/KxRLwu8b3AQQ03et/images/how-to-guides/deepseek/api-explorer-deepseek-api.png?w=2500&fit=max&auto=format&n=KxRLwu8b3AQQ03et&q=85&s=c8843f9a68c10e7beb37a8e0fd697c5f 2500w" data-optimize="true" data-opv="2" />

    <Tip>
      For mathematical problems,use a directive in your prompt such as: "Please
      reason step by step, and include your final answer within `\boxed{}.`"
    </Tip>
  </Step>
</Steps>

This example demonstrated how to use the DeepSeek Platform API in a Modus app to
create an endpoint that returns text generated by the DeepSeek-R1 model. More
advanced use cases for leveraging the DeepSeek reasoning models in your Modus
app include workflows like tool use / function calling, generating structured
outputs, problem solving, code generation, and much more.
[Let us know](mailto:hello@hypermode.com?subject=DeepSeek+Modus) how you're
leveraging DeepSeek with Modus.

## Resources

* [DeepSeek on Hugging Face](https://huggingface.co/deepseek-ai)
* [`DeepSeek-R1-Distill-Llama-8B` model card](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)
* [DeepSeek-R1 paper](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)
* [DeepSeek Platform API docs](https://api-docs.deepseek.com/)
